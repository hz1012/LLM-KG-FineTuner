# coding:utf-8
"""
ä¸»æ¨¡å— - åè°ƒå„ä¸ªåŠŸèƒ½æ¨¡å—å®ŒæˆPDFåˆ°çŸ¥è¯†å›¾è°±çš„å®Œæ•´æµç¨‹
"""
import os
import logging
import random
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from langchain.docstore.document import Document
# å¯¼å…¥å„ä¸ªåŠŸèƒ½æ¨¡å—
from document_converter import DocumentConverter
from markdown_processor import MarkdownProcessor

from chunk_splitter import ChunkSplitter
from knowledge_graph_extractor import KnowledgeGraphExtractor
from utils import FileManager, StatisticsReporter, ContentAnalyzer, ConfigManager, ProgressTracker, OpenAIAPIManager
from graph_data_processor import EnhancedGraphDataProcessor
from quality_filter import QualityFilter
from qa_generator import QAGenerator
from graph_enhancer import GraphEnhancer


# è®¾ç½®æ—¥å¿—
# å…ˆè¯»é…ç½®ï¼Œå†é…æ—¥å¿—
config = ConfigManager.load_config()
log_cfg = config.get("logging", {})

log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

logging.basicConfig(
    level=getattr(logging, log_cfg.get("level", "INFO").upper(), logging.INFO),
    format=log_cfg.get(
        "format", "%(asctime)s - %(name)s - %(levelname)s - %(message)s"),
    handlers=[
        logging.StreamHandler(),
        *([logging.FileHandler(log_dir/log_cfg["filename"])] if log_cfg.get("file_output") and log_cfg.get("filename") else [])
    ]
)
logger = logging.getLogger(__name__)


class Document2KnowledgeGraphPipeline:  #
    """æ–‡æ¡£åˆ°çŸ¥è¯†å›¾è°±çš„å®Œæ•´å¤„ç†æµæ°´çº¿ï¼ˆæ”¯æŒPDFå’ŒHTMLï¼‰"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–å¤„ç†æµæ°´çº¿

        Args:
            config: é…ç½®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        # ğŸ”¥ ä¿®æ”¹ï¼šä»config.jsonåŠ è½½é…ç½®
        if config is None:
            self.config = ConfigManager.load_config()
            # æ‰“å°é…ç½®æ‘˜è¦
            ConfigManager.print_config_summary()
        else:
            self.config = config

        self.document_converter = DocumentConverter(self.config)

        self.markdown_processor = MarkdownProcessor()

        chunk_config = self.config.get('chunk_splitter', {})
        # å¤„ç†document_titleï¼Œå¦‚æœæ˜¯"None"å­—ç¬¦ä¸²åˆ™è½¬æ¢ä¸ºNone
        document_title = chunk_config.get('document_title', None)
        if document_title == "None":
            document_title = None
        self.chunk_splitter = ChunkSplitter(
            max_chunk_size=chunk_config.get('max_chunk_size', 2000),
            chunk_overlap=chunk_config.get('chunk_overlap', 200),
            document_title=document_title
        )

        openai_config = self.config.get('openai', {})
        api_manager = OpenAIAPIManager(openai_config)

        # ğŸ”¥ æ–°å¢ï¼šQAç”Ÿæˆå™¨åˆå§‹åŒ–
        qa_config = self.config.get('qa_generator', {})
        self.qa_generator = QAGenerator(qa_config, api_manager)

        # ğŸ”¥ ä¼ å…¥å®Œæ•´é…ç½®ç»™çŸ¥è¯†å›¾è°±æå–å™¨
        kg_config = self.config.get('knowledge_extractor', {})
        self.kg_extractor = KnowledgeGraphExtractor(
            kg_config=kg_config,
            api_manager=api_manager
        )

        # ğŸ”¥ æ–°å¢ï¼šå›¾è°±å¢å¼ºå™¨åˆå§‹åŒ–
        enhancer_config = self.config.get('graph_enhancer', {})
        self.graph_enhancer = GraphEnhancer(enhancer_config, api_manager)

        # å·¥å…·ç±»
        self.file_manager = FileManager()
        self.stats_reporter = StatisticsReporter()
        self.content_analyzer = ContentAnalyzer()
        self.quality_filter = QualityFilter(self.config)

        # ä½¿ç”¨å¢å¼ºç‰ˆå›¾æ•°æ®å¤„ç†å™¨
        self.graph_processor = EnhancedGraphDataProcessor(config=self.config)

        # ğŸ”¥ æ·»åŠ ï¼šæ˜¾ç¤ºåŠ è½½çš„é…ç½®æ–‡ä»¶ä½ç½®
        logger.info(f"ğŸ“ é…ç½®æ–‡ä»¶ä½ç½®: {Path(__file__).parent / 'config.json'}")

    def process_document_file(
        self,
        file_path: str,
        output_dir: str = "./output",
        save_intermediate: bool = True,
        max_chunks: int = None,
        chunk_selection_strategy: str = "quality",
        enable_qa_generation: bool = False  # æ˜¯å¦ç”Ÿæˆqaå¯¹
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£æ–‡ä»¶çš„å®Œæ•´æµç¨‹ï¼ˆæ”¯æŒPDFå’ŒHTMLï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒæœ¬åœ°PDF/HTMLæ–‡ä»¶å’Œè¿œç¨‹URLï¼‰
            output_dir: è¾“å‡ºç›®å½•
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
            max_chunks: æœ€å¤§å¤„ç†chunkæ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰chunk
            chunk_selection_strategy: chunké€‰æ‹©ç­–ç•¥
            enable_qa_generationï¼š æ˜¯å¦ç”Ÿæˆqaå¯¹

        Returns:
            åŒ…å«æœ€ç»ˆç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """

        # ğŸ”¥ åŠ¨æ€è®¡ç®—æ€»æ­¥éª¤æ•°
        total_steps = 8  # åŸºç¡€æ­¥éª¤
        if enable_qa_generation:
            total_steps += 1

        progress = ProgressTracker(total_steps)

        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£æ–‡ä»¶: {file_path}")

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)

            # ğŸ”¥ ç®€åŒ–ï¼šä¸€æ­¥å®Œæˆæ£€æµ‹å’Œè½¬æ¢
            progress.update("æ–‡æ¡£ç±»å‹æ£€æµ‹å’Œè½¬æ¢")
            file_type, raw_markdown, extracted_images = self.document_converter.detect_and_convert(
                file_path)

            if save_intermediate:
                raw_md_path = os.path.join(output_dir, "01_raw_markdown.md")
                self.file_manager.save_text(raw_markdown, raw_md_path)

                # ä¿å­˜æå–çš„å›¾ç‰‡ä¿¡æ¯
                if extracted_images:
                    images_info_path = os.path.join(
                        output_dir, "00_extracted_images.json")
                    self.file_manager.save_json(
                        extracted_images, images_info_path)
                    logger.info(f"æå–äº†{len(extracted_images)}å¼ å›¾ç‰‡ä¿¡æ¯")

            # 2. Markdownåå¤„ç†
            progress.update("Markdownåå¤„ç†")
            processed_markdown = self.markdown_processor.post_process_markdown(
                raw_markdown)

            if save_intermediate:
                processed_md_path = os.path.join(
                    output_dir, "02_processed_markdown.md")
                self.file_manager.save_text(
                    processed_markdown, processed_md_path)

            # 3. æ–‡æ¡£åˆ†å— chunké€‰æ‹©é€»è¾‘
            progress.update("æ–‡æ¡£åˆ†å—")
            docs = self.chunk_splitter.process_document(processed_markdown)
            if save_intermediate:
                total_chunks_path = os.path.join(
                    output_dir, "02.5_total_chunks.json")
                # å°†Documentå¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸æ ¼å¼
                docs_data = [
                    {
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'length': doc.metadata.get('token_length', len(doc.page_content))
                    }
                    for doc in docs
                ]
                self.file_manager.save_json(
                    docs_data, total_chunks_path)

            selected_docs, selection_info = self._select_chunks_for_processing(
                docs, max_chunks, chunk_selection_strategy
            )

            # 4. chunkå†…å®¹åˆ†æ
            progress.update("chunkå†…å®¹åˆ†æ")

            # ğŸ”¥ ç»Ÿä¸€ä½¿ç”¨ContentAnalyzerï¼Œä¼ å…¥ä¿å­˜é€‰é¡¹
            content_analysis = self.content_analyzer.analyze_and_optionally_save(
                docs=docs,
                selected_docs=selected_docs,
                selection_info=selection_info,
                output_dir=output_dir if save_intermediate else None
            )
            # 5. QAå¯¹ç”Ÿæˆ
            qa_results = None
            qa_summary = None
            if enable_qa_generation:
                progress.update("QAå¯¹ç”Ÿæˆ")
                qa_results = self.qa_generator.generate_qa_for_chunks(
                    selected_docs,
                    max_chunks=len(selected_docs)
                )

                qa_summary = self.qa_generator.generate_qa_summary(qa_results)

                # ä¿å­˜QAç»“æœ
                if save_intermediate:
                    qa_path = os.path.join(output_dir, "03.5_qa_pairs.json")
                    self.file_manager.save_json({
                        'qa_results': qa_results,
                        'qa_summary': qa_summary
                    }, qa_path)

                    logger.info(
                        f"âœ… QAç”Ÿæˆå®Œæˆ: {qa_summary['total_qa_pairs']}ä¸ªQAå¯¹")

            # 6. çŸ¥è¯†å›¾è°±æŠ½å–
            progress.update("çŸ¥è¯†å›¾è°±æŠ½å–")
            kg_results = self.kg_extractor.extract_from_chunks(
                selected_docs)

            # ä¿å­˜å¯¹é½å‰çš„åŸå§‹ç‚¹è¾¹æ•°æ®
            if save_intermediate:
                raw_graph_data = self.graph_processor.extract_raw_graph_data(
                    kg_results)
                raw_graph_path = os.path.join(
                    output_dir, "04_raw_graph_data.json")
                self.file_manager.save_json(raw_graph_data, raw_graph_path)

            # 7. ç”Ÿæˆå¯¹é½åçš„çŸ¥è¯†å›¾è°±æ•°æ®
            full_kg_data, simple_kg_data = self.graph_processor.extract_pure_graph_data(
                kg_results)

            # ğŸ”¥ ç´§è·Ÿåœ¨æ­¥éª¤7åä¿å­˜å®Œæ•´è®°å½•æ•°æ®
            if save_intermediate:
                full_kg_path = os.path.join(
                    output_dir, "05_knowledge_graph_full.json")
                self.file_manager.save_json(full_kg_data, full_kg_path)

                # ä¿å­˜ç®€åŒ–å›¾æ•°æ®
                simple_kg_path = os.path.join(
                    output_dir, "06_knowledge_graph_simple.json")
                self.file_manager.save_json(simple_kg_data, simple_kg_path)

            # 8. å›¾è°±å¢å¼º
            # ğŸ”¥ æ·»åŠ å¯æ§å‚æ•°enable_graph_enhancement
            enable_graph_enhancement = self.config.get(
                'graph_enhancer', {}).get('enable', True)
            enhanced_kg_data = None
            enhancement_stats = None

            if enable_graph_enhancement:
                progress.update("å›¾è°±ESå¢å¼º")
                enhanced_kg_data, enhancement_stats = self.graph_enhancer.enhance_knowledge_graph(
                    simple_kg_data)

                # ğŸ”¥ ç´§è·Ÿåœ¨æ­¥éª¤8åä¿å­˜å¢å¼ºåçš„å›¾æ•°æ®
                if save_intermediate:
                    enhanced_kg_path = os.path.join(
                        output_dir, "07_enhanced_knowledge_graph.json")
                    self.file_manager.save_json(
                        enhanced_kg_data, enhanced_kg_path)

                    # ä¿å­˜å¢å¼ºç»Ÿè®¡ä¿¡æ¯
                    enhancement_stats_path = os.path.join(
                        output_dir, "08_enhancement_stats.json")
                    self.file_manager.save_json(
                        enhancement_stats, enhancement_stats_path)
            else:
                logger.info("â­ï¸  è·³è¿‡å›¾è°±å¢å¼ºæ­¥éª¤")
                enhanced_kg_data = simple_kg_data
                enhancement_stats = {}

            # 9. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            progress.update("ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š")

            # ğŸ”¥ ç²¾ç®€ï¼šç›´æ¥ä½¿ç”¨StatisticsReporterå¤„ç†èšåˆæ ¼å¼æ•°æ®
            if enhanced_kg_data and len(enhanced_kg_data) > 0:
                self.stats_reporter.print_graph_summary(
                    enhanced_kg_data, len(selected_docs), "å¢å¼ºå")
            elif simple_kg_data and len(simple_kg_data) > 0:
                self.stats_reporter.print_graph_summary(
                    simple_kg_data, len(selected_docs), "ç®€åŒ–")
            else:
                logger.info("=" * 60)
                logger.info("ğŸ¯ çŸ¥è¯†å›¾è°±å¤„ç†å®Œæˆ")
                logger.info("=" * 60)
                logger.info("ğŸ“Š å›¾è°±ç»Ÿè®¡: 0ä¸ªå®ä½“, 0ä¸ªå…³ç³»")
                logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£å—: {len(selected_docs)}ä¸ª")
                logger.info("=" * 60)

            # 10. ä¿å­˜æœ€ç»ˆç»“æœ
            progress.update("ä¿å­˜ç»“æœ")
            final_results = {
                'source_file': file_path,
                'file_type': file_type,
                'extracted_images_count': len(extracted_images) if extracted_images else 0,
                'processing_config': self.config,
                'content_analysis': content_analysis,
                'chunk_selection_info': selection_info,
                'qa_generation_results': qa_results,
                'qa_summary': qa_summary,
                'knowledge_graph_results': kg_results,
                'aligned_knowledge_graph': full_kg_data,
                'enhanced_knowledge_graph': enhanced_kg_data,
                'enhancement_stats': enhancement_stats
            }

            results_path = os.path.join(output_dir, "09_final_results.json")
            self.file_manager.save_json(final_results, results_path)

            return final_results

        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}", exc_info=True)
            raise

    def _select_chunks_for_processing(
        self,
        docs: List[Document],
        max_chunks: int = None,
        strategy: str = "quality"
    ) -> Tuple[List[Document], Dict[str, Any]]:
        """
        é€‰æ‹©è¦å¤„ç†çš„chunksï¼Œæ”¯æŒå¤šç§é€‰æ‹©ç­–ç•¥

        Args:
            docs: æ‰€æœ‰æ–‡æ¡£å—
            max_chunks: æœ€å¤§å¤„ç†æ•°é‡
            strategy: é€‰æ‹©ç­–ç•¥ - 'quality'(è´¨é‡ä¼˜å…ˆ)ã€'first'(å‰Nä¸ª)ã€'random'(éšæœºé€‰æ‹©)

        Returns:
            (é€‰ä¸­çš„æ–‡æ¡£å—, é€‰æ‹©ä¿¡æ¯)
        """

        logger.info(f"ğŸ“‹ å¼€å§‹chunké€‰æ‹©ï¼Œæ€»è®¡{len(docs)}ä¸ªchunkï¼Œç­–ç•¥: {strategy}")

        # æ­¥éª¤1: è´¨é‡è¿‡æ»¤ï¼ˆæ‰€æœ‰ç­–ç•¥éƒ½éœ€è¦ï¼‰
        filtered_docs_result = self.quality_filter.filter_chunks(docs)
        # æ­£ç¡®è§£åŒ…filter_chunksçš„è¿”å›å€¼ï¼ˆå…ƒç»„ï¼‰
        if isinstance(filtered_docs_result, tuple):
            filtered_docs, filtered_chunks_info = filtered_docs_result
        else:
            filtered_docs = filtered_docs_result
            filtered_chunks_info = None

        if not filtered_docs:
            logger.warning("âš ï¸ è´¨é‡è¿‡æ»¤åæ— å¯ç”¨chunk")
            return [], {
                'strategy': strategy,
                'original_count': len(docs),
                'filtered_count': 0,
                'selected_count': 0,
                'quality_threshold': self.quality_filter.min_quality_score,
                'message': 'è´¨é‡è¿‡æ»¤åæ— å¯ç”¨chunk'
            }

        # æ­¥éª¤2: æ ¹æ®ç­–ç•¥é€‰æ‹©chunk
        if strategy == "first":
            # ç­–ç•¥1: é€‰æ‹©å‰Nä¸ªï¼ˆæŒ‰åŸé¡ºåºï¼‰
            selected_docs = filtered_docs[:max_chunks] if max_chunks else filtered_docs

        elif strategy == "quality":
            # ç­–ç•¥2: æŒ‰è´¨é‡åˆ†æ•°æ’åºï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜çš„Nä¸ª
            scored_docs = []
            for doc in filtered_docs:
                score, _ = self.quality_filter.calculate_quality_score(
                    doc.page_content,
                    doc.metadata.get('type', 'text')
                )
                scored_docs.append((doc, score))

            # æŒ‰è´¨é‡åˆ†æ•°é™åºæ’åº
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            selected_docs = [doc for doc, score in scored_docs[:max_chunks]] if max_chunks else [
                doc for doc, score in scored_docs]

        elif strategy == "random":
            # ç­–ç•¥3: éšæœºé€‰æ‹©Nä¸ª
            if max_chunks and max_chunks < len(filtered_docs):
                selected_docs = random.sample(filtered_docs, max_chunks)
            else:
                selected_docs = filtered_docs

        else:
            logger.warning(f"âš ï¸ æœªçŸ¥ç­–ç•¥'{strategy}'ï¼Œä½¿ç”¨é»˜è®¤'quality'ç­–ç•¥")
            # é»˜è®¤ä½¿ç”¨qualityç­–ç•¥
            return self._select_chunks_for_processing(docs, max_chunks, "quality")

        # è®°å½•é€‰æ‹©ç»“æœ
        selection_info = {
            'strategy': strategy,
            'original_count': len(docs),
            'filtered_count': len(filtered_docs),
            'selected_count': len(selected_docs),
            'quality_threshold': self.quality_filter.min_quality_score
        }

        # ä¸ºqualityç­–ç•¥æ·»åŠ é¢å¤–ç»Ÿè®¡ä¿¡æ¯
        if strategy == "quality" and selected_docs:
            quality_scores = []
            for doc in selected_docs:
                score, _ = self.quality_filter.calculate_quality_score(
                    doc.page_content,
                    doc.metadata.get('type', 'text')
                )
                quality_scores.append(score)

            selection_info.update({
                'avg_quality_score': round(sum(quality_scores) / len(quality_scores), 1),
                'min_quality_score': min(quality_scores),
                'max_quality_score': max(quality_scores)
            })

        logger.info(
            f"âœ… ç­–ç•¥'{strategy}'é€‰æ‹©å®Œæˆ: {len(filtered_docs)} -> {len(selected_docs)}ä¸ªchunk")

        return selected_docs, selection_info


def main():
    """ä¸»å‡½æ•° - å¤„ç†æ–‡æ¡£å¹¶ç”ŸæˆçŸ¥è¯†å›¾è°±"""

    # æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    test_files = [
        {
            'path': "test_samples/sample_1.html",
            'type': 'HTML',
            'output_dir': "html_output"
        },
        {
            'path': "https://www.example.com/research/sample-report.html",
            'type': 'HTML',
            'output_dir': "html_output_2"
        },
        {
            'path': "test_samples/sample_document.pdf",
            'type': 'PDF',
            'output_dir': "pdf_output"
        }
    ]

    # é€‰æ‹©è¦æµ‹è¯•çš„æ–‡ä»¶ï¼ˆé»˜è®¤æµ‹è¯•HTMLæ–‡ä»¶ï¼‰
    test_file = test_files[1]  # ä¿®æ”¹ç´¢å¼•æ¥é€‰æ‹©ä¸åŒçš„æµ‹è¯•æ–‡ä»¶

    logger.info(f"ğŸš€ å¼€å§‹æµ‹è¯•{test_file['type']}æ–‡ä»¶å¤„ç†æµç¨‹")
    logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {test_file['path']}")
    logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {test_file['output_dir']}")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(test_file['output_dir'], exist_ok=True)

    # åˆå§‹åŒ–å¤„ç†æµæ°´çº¿
    pipeline = Document2KnowledgeGraphPipeline()

    try:
        # å¤„ç†æ–‡æ¡£
        results = pipeline.process_document_file(
            file_path=test_file['path'],
            output_dir=test_file['output_dir'],
            save_intermediate=True,
            max_chunks=None,
            chunk_selection_strategy="quality",
            enable_qa_generation=False  # ç¦ç”¨QAç”Ÿæˆ
        )

        # æ‰“å°QAç”Ÿæˆæ‘˜è¦
        qa_summary = results.get('qa_summary')
        if qa_summary:
            logger.info("ğŸ“Š QAç”Ÿæˆæ‘˜è¦:")
            logger.info(f"   - æ€»QAå¯¹æ•°: {qa_summary['total_qa_pairs']}")
            logger.info(f"   - æˆåŠŸç‡: {qa_summary['success_rate']}%")
            logger.info(f"   - å¹³å‡ç½®ä¿¡åº¦: {qa_summary['average_confidence']}")
            logger.info(f"   - é—®é¢˜ç±»å‹åˆ†å¸ƒ: {qa_summary['qa_types_distribution']}")
            logger.info("âœ… QAç”Ÿæˆç¤ºä¾‹å®Œæˆ")

        # æ‰“å°å¤„ç†ç»“æœæ‘˜è¦
        logger.info(f"ğŸ‰ å¤„ç†å®Œæˆï¼")
        logger.info(f"   - æºæ–‡ä»¶: {results['source_file']}")
        logger.info(f"   - æ–‡ä»¶ç±»å‹: {results['file_type']}")
        logger.info(
            f"   - æ€»chunkæ•°: {results['chunk_selection_info']['original_count']}")
        logger.info(
            f"   - è¿‡æ»¤åchunkæ•°: {results['chunk_selection_info']['filtered_count']}")
        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
        # æˆ– 'selected_chunks'
        logger.info(
            f"   - é€‰ä¸­chunkæ•°: {results['chunk_selection_info']['selected_count']}")
        logger.info(
            f"   - é€‰æ‹©ç­–ç•¥: {results['chunk_selection_info']['strategy']}")

        # ç‰¹æ®Šå¤„ç†HTMLæ–‡ä»¶çš„å›¾ç‰‡ä¿¡æ¯
        if results['file_type'] == 'html' and results.get('extracted_images_count', 0) > 0:
            logger.info(f"ğŸ–¼ï¸  å·²æå–{results['extracted_images_count']}å¼ å›¾ç‰‡ä¿¡æ¯")

            # ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°ï¼Œä¼ å…¥æ­£ç¡®çš„base_url
            try:
                image_output_dir = os.path.join(
                    test_file['output_dir'], "images")

                # ç¡®ä¿base_urlæ­£ç¡®
                if test_file['path'].startswith('http'):
                    base_url = test_file['path']
                else:
                    # æœ¬åœ°æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶æ‰€åœ¨ç›®å½•ä½œä¸ºbase_url
                    base_url = f"file://{os.path.dirname(os.path.abspath(test_file['path']))}/"

                downloaded_images = pipeline.document_converter.html_converter.download_images(
                    image_output_dir,
                    base_url=base_url
                )
                logger.info(
                    f"ğŸ“¥ æˆåŠŸä¸‹è½½{len(downloaded_images)}å¼ å›¾ç‰‡åˆ°: {image_output_dir}")

            except Exception as e:
                logger.warning(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {e}")

        logger.info(f"ğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: {test_file['output_dir']}")

    except Exception as e:
        logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
