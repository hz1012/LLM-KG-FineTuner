import json
import logging
import torch
from pathlib import Path
from peft import PeftModel
import os
from modelscope import AutoModelForCausalLM, AutoTokenizer

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class InferenceDemo:
    """ç®€åŒ–ç‰ˆæ¨ç†æ¼”ç¤º"""

    def __init__(self, base_model_path=None, lora_model_path=None):
        """åˆå§‹åŒ–æ¨ç†å™¨"""
        self.model = None
        self.tokenizer = None
        # æ›¿æ¢æˆè‡ªå·±çš„åŸºç¡€æ¨¡å‹è·¯å¾„
        self.base_model_path = base_model_path or "/root/.cache/modelscope/hub/models/qwen/Qwen2___5-7B-Instruct"
        # æ›¿æ¢æˆè‡ªå·±çš„LoRAæ¨¡å‹è·¯å¾„
        self.lora_model_path = lora_model_path or "./fine_tune_output/final_model"

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            logger.info("æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")

            # æ£€æŸ¥åŸºç¡€æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.base_model_path):
                logger.warning(
                    f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.base_model_path}ï¼Œå°†ä»ModelScopeä¸‹è½½")
                self.base_model_path = "qwen/Qwen2.5-7B-Instruct"

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_path,
                trust_remote_code=True,
                padding_side="right",
                truncation_side="right"
            )

            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            if self.tokenizer.pad_token_id is None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            # åŠ è½½åŸºç¡€æ¨¡å‹
            logger.info(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {self.base_model_path}")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_path,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            )

            # æ£€æŸ¥LoRAæ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨å¹¶åŠ è½½
            if os.path.exists(self.lora_model_path):
                logger.info(f"æ­£åœ¨åŠ è½½LoRAæƒé‡: {self.lora_model_path}")
                try:
                    # åŠ è½½LoRAé€‚é…å™¨
                    self.model = PeftModel.from_pretrained(
                        self.model,
                        self.lora_model_path,
                        device_map="auto",
                    )
                    logger.info("âœ… LoRAæƒé‡åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ LoRAæƒé‡åŠ è½½å¤±è´¥: {e}")
                    logger.info("ğŸ’¡ å°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæ¨ç†")
            else:
                logger.warning(f"LoRAæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.lora_model_path}")
                logger.info("ğŸ’¡ å°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œæ¨ç†")

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()

            logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def inference(self, input_text):
        """æ‰§è¡Œæ¨ç†"""
        if self.model is None or self.tokenizer is None:
            logger.error("âŒ æ¨¡å‹æœªåŠ è½½")
            return None

        try:
            # æ„å»ºæç¤ºè¯
            system_prompt = '''ä½ æ˜¯ç½‘ç»œå®‰å…¨çŸ¥è¯†å›¾è°±æå–ä¸“å®¶ã€‚ä»å¨èƒæƒ…æŠ¥æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚

å®ä½“ç±»å‹åŒ…æ‹¬ï¼šThreatOrganization(å¨èƒç»„ç»‡), AttackEvent(æ”»å‡»äº‹ä»¶), Tool(å·¥å…·), Technique(æŠ€æœ¯), Target(ç›®æ ‡), Report(æŠ¥å‘Š), Tactic(æˆ˜æœ¯), Procedure(ç¨‹åº), Asset(èµ„äº§)
å…³ç³»ç±»å‹åŒ…æ‹¬ï¼šLAUNCH(å‘èµ·), ATTACK(æ”»å‡»), USE(ä½¿ç”¨), BELONG(å±äº), HAS(æ‹¥æœ‰), IMPLEMENT(å®ç°)

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
{"entities":[{"labels":"ç±»å‹","id":"å”¯ä¸€ID","name":"åç§°","description":"æè¿°"}],"relationships":[{"type":"å…³ç³»ç±»å‹","source":"æºå®ä½“ID","target":"ç›®æ ‡å®ä½“ID","confidence":0.95,"evidence":"è¯æ®"}]}

æ³¨æ„äº‹é¡¹ï¼š
1. åªæå–ä¸ç½‘ç»œå®‰å…¨ç›¸å…³çš„ä¿¡æ¯
2. å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰å®‰å…¨ç›¸å…³å†…å®¹ï¼Œå¯ä»¥è¿”å›ç©ºæ•°ç»„
3. å…³ç³»ä¸­çš„sourceå’Œtargetå¿…é¡»æ˜¯å®ä½“ä¸­çš„id
4. å®ä½“IDåº”è¯¥æœ‰æ„ä¹‰ä¸”å”¯ä¸€
5. è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—'''

            prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n"

            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt, return_tensors="pt", truncation=True, max_length=2048)

            # ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            # ç”Ÿæˆè¾“å‡º
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(
                outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)

            return response

        except Exception as e:
            logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
            return None


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨æ¨ç†æ¼”ç¤º")

    # åˆå§‹åŒ–æ¨ç†å™¨
    inference_demo = InferenceDemo()

    # åŠ è½½æ¨¡å‹
    if not inference_demo.load_model():
        return

    # æµ‹è¯•è¾“å…¥
    test_inputs = [
        "APT41 World Tour 2021 on a tight schedule\n=========================================  \nShare this article  \nFound it interesting? Don't hesitate to share it to wow your friends or colleagues  \n[â† Blog](/blog/)  \nShare this article  \nFound it interesting? Don't hesitate to share it to wow your friends or colleagues  \n[Nikita Rostovcev  \nCyber Intelligence Researcher](https://www.group-ib.com/author/nikita-rostovtsev/)  \nAPT41 World Tour 2021 on a tight schedule\n=========================================  \n4 malicious campaigns, 13 confirmed victims, and a new wave of Cobalt Strike infections  \nAugust 18, 2022 Â·  min to read Â· Advanced Persistent Threats  \nAPT41  \nThreat Intelligence",
        "# APT41 World Tour 2021 on a tight schedule\n\n4 malicious campaigns, 13 confirmed victims, and a new wave of Cobalt Strike infections  \nAugust 18, 2022 Â·  min to read Â· Advanced Persistent Threats  \nAPT41  \nThreat Intelligence  \nIn March 2022 one of the oldest state-sponsored hacker groups, **APT41**, breached government networks in six US states, including by exploiting a vulnerability in a livestock management system, Mandiant investigators [have reported](https://www.mandiant.com/resources/apt41-initiates-global-intrusion-campaign-using-multiple-exploits).",
        "# APT41 World Tour 2021 on a tight schedule\n\nInterestingly, according to sqlmap logs, the threat actors breached only half of the websites they were interested in. This suggests that even hackers like APT41 do not always go out of their way to ensure that a breach is successful.  \nThis blog post also uncovers subnets from which the threat actors connected to their C&C servers, which is further evidence confirming the threatâ€™s country of origin.  \nFor the first time, we were able to identify the groupâ€™s working hours in 2021, which are similar to regular office business hours.  \nIT directors, heads of cybersecurity teams, SOC analysts and incident response specialists are likely to find this material useful. Our goal is to reduce financial losses and infrastructure downtime as well as to help take preventive measures to fend off APT41 attacks.",
        "# APT41 World Tour 2021 on a tight schedule\n\nKey findings\n------------  \n* We estimate that in 2021 APT41 compromised and gained various levels of access to at least 13 organizations worldwide.\n* The groupâ€™s targets include government and private organizations based in the US, Taiwan, India, Thailand, China, Hong Kong, Mongolia, Indonesia, Vietnam, Bangladesh, Ireland, Brunei, and the UK.\n* In the campaigns that we analyzed, APT41 targeted the following industries: the government sector, manufacturing, healthcare, logistics, hospitality, finance, education, telecommunications, consulting, sports, media, and travel. The targets also included a political group, military organizations, and airlines.\n* To conduct reconnaissance, the threat actors use tools such as Acunetix, Nmap, Sqlmap, OneForAll, subdomain3, subDomainsBrute, and Sublist3r.\n* As an initial vector, the group uses web applications vulnerable to SQL injection attacks."
    ]

    # æ‰§è¡Œæ¨ç†
    for i, input_text in enumerate(test_inputs, 1):
        logger.info(f"\nğŸ” æµ‹è¯• {i}: {input_text}")
        result = inference_demo.inference(input_text)
        if result:
            logger.info(f"âœ… è¾“å‡º: {result}")
        else:
            logger.error("âŒ æ¨ç†å¤±è´¥")


if __name__ == "__main__":
    main()
