# coding:utf-8
"""
Fine-tune Demo - çŸ¥è¯†å›¾è°±æå–æ¨¡å‹å¾®è°ƒæ¡†æ¶
ä¸“é—¨è§£å†³JSONæ ¼å¼ä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡è§£ææˆåŠŸç‡

ä¸»è¦åŠŸèƒ½ï¼š
1. æ•°æ®å‡†å¤‡ï¼šä»ç°æœ‰few-shotç¤ºä¾‹æå–è®­ç»ƒæ•°æ®
2. æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨unslothè¿›è¡Œé«˜æ•ˆfine-tune
3. æ•ˆæœéªŒè¯ï¼šå¯¹æ¯”è®­ç»ƒå‰åçš„JSONè§£ææˆåŠŸç‡
4. æ¨ç†æµ‹è¯•ï¼šå®é™…æµ‹è¯•fine-tunedæ¨¡å‹æ•ˆæœ
"""

import json
import os
import time
import logging
import platform
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import torch
from datasets import Dataset
import pandas as pd
import re
import random

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KnowledgeGraphFineTuner:
    """çŸ¥è¯†å›¾è°±æå–æ¨¡å‹å¾®è°ƒå™¨"""

    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–å¾®è°ƒå™¨

        Args:
            config: é…ç½®å‚æ•°
        """
        # ä¿®å¤ï¼šæ­£ç¡®åˆå¹¶é…ç½®ï¼Œè€Œä¸æ˜¯ç›´æ¥æ›¿æ¢
        default_config = self._get_default_config()
        if config:
            default_config.update(config)  # å°†ä¼ å…¥é…ç½®åˆå¹¶åˆ°é»˜è®¤é…ç½®ä¸­
        self.config = default_config
        self.model = None
        self.tokenizer = None
        self.trainer = None
        self.use_alternative = False  # æ˜¯å¦ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(self.config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ£€æµ‹è¿è¡Œç¯å¢ƒ
        self._detect_environment()

        logger.info(f"ğŸš€ Fine-tuneæ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            # æ¨¡å‹é…ç½® base_modelä¸ºunslothå‚æ•° model_idä¸ºmodelscopeå‚æ•°
            'base_model': "unsloth/Qwen2.5-7B-Instruct-bnb-4bit",
            "model_id": "qwen/Qwen2.5-7B-Instruct",
            'max_seq_length': 2048,  # è¿›ä¸€æ­¥é™ä½åºåˆ—é•¿åº¦
            'load_in_4bit': True,

            # LoRAé…ç½®
            'lora_r': 4,  # è¿›ä¸€æ­¥é™ä½LoRA rank
            'lora_alpha': 4,
            'lora_dropout': 0.1,
            'target_modules': ["q_proj", "k_proj", "v_proj", "o_proj",
                               "gate_proj", "up_proj", "down_proj"],

            # è®­ç»ƒé…ç½®
            'per_device_train_batch_size': 1,
            'gradient_accumulation_steps': 32,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            'warmup_steps': 10,
            'max_steps': 100,
            'learning_rate': 2e-4,
            'fp16': not torch.cuda.is_bf16_supported(),
            'bf16': torch.cuda.is_bf16_supported(),
            'logging_steps': 1,
            'optim': "adamw_8bit",
            'weight_decay': 0.01,
            'lr_scheduler_type': "linear",
            'seed': 42,

            # è¾“å‡ºé…ç½®
            'output_dir': "./fine_tune_output",
            'save_steps': 50,
            'save_total_limit': 2,
        }

    def _detect_environment(self):
        """æ£€æµ‹è¿è¡Œç¯å¢ƒï¼Œå†³å®šä½¿ç”¨å“ªç§æ–¹æ¡ˆ"""
        system = platform.system()
        machine = platform.machine()

        # æ£€æµ‹æ˜¯å¦ä¸º Apple Silicon Mac
        if system == "Darwin" and machine in ["arm64", "aarch64"]:
            logger.info("ğŸ æ£€æµ‹åˆ° Apple Silicon Macï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ")
            self.use_alternative = True
        else:
            # å¼ºåˆ¶ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆï¼ˆè·³è¿‡unslothæ£€æµ‹ï¼‰
            logger.info("âš ï¸ å¼ºåˆ¶ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆï¼ˆè·³è¿‡unslothæ£€æµ‹ï¼‰")
            self.use_alternative = True
            # å°è¯•å¯¼å…¥ unsloth æ¥æ£€æµ‹æ˜¯å¦å¯ç”¨
            # try:
            #     import unsloth
            #     logger.info("âœ… unsloth å¯ç”¨ï¼Œå°†ä½¿ç”¨ unsloth æ–¹æ¡ˆ")
            #     self.use_alternative = False
            # except (ImportError, NotImplementedError) as e:
            #     logger.info(f"âš ï¸ unsloth ä¸å¯ç”¨ ({e})ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ")
            #     self.use_alternative = True

    def prepare_training_data(self) -> Dataset:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®

        ä»ç°æœ‰çš„few-shotç¤ºä¾‹ä¸­æå–é«˜è´¨é‡è®­ç»ƒæ ·æœ¬
        ä¸“é—¨é’ˆå¯¹JSONæ ¼å¼ä¸€è‡´æ€§è¿›è¡Œä¼˜åŒ–

        Returns:
            Dataset: æ ¼å¼åŒ–çš„è®­ç»ƒæ•°æ®é›†
        """
        logger.info("ğŸ“Š å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®...")

        # 1. ä»training_data.jsonæ–‡ä»¶è¯»å–è®­ç»ƒæ•°æ®
        training_file_path = Path("fine_tune_input/training_data.json")
        if not training_file_path.exists():
            logger.error(f"âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {training_file_path}")
            raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {training_file_path}")

        with open(training_file_path, 'r', encoding='utf-8') as f:
            training_samples = json.load(f)

        logger.info(f"ğŸ“‹ ä»æ–‡ä»¶åŠ è½½äº† {len(training_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")

        # 1. ä»few-shotç¤ºä¾‹æå–åŸºç¡€æ•°æ®
        # training_samples = self._extract_few_shot_samples()

        # 2. æ•°æ®å¢å¼º
        augmented_samples = self._augment_training_data(training_samples)

        # 3. æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ ¼å¼
        formatted_samples = self._format_for_training(augmented_samples)

        # 4. åˆ›å»ºDataset
        dataset = Dataset.from_pandas(pd.DataFrame(formatted_samples))

        logger.info(f"âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: {len(formatted_samples)}ä¸ªæ ·æœ¬")

        # ä¿å­˜è®­ç»ƒæ•°æ®æ ·æœ¬
        sample_path = self.output_dir / "training_samples.json"
        with open(sample_path, 'w', encoding='utf-8') as f:
            json.dump(formatted_samples[:200], f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“ æ ·æœ¬æ•°æ®å·²ä¿å­˜: {sample_path}")

        return dataset

    def _extract_few_shot_samples(self) -> List[Dict[str, str]]:
        """ä»few-shotç¤ºä¾‹æå–è®­ç»ƒæ ·æœ¬"""

        # è‹±æ–‡ç¤ºä¾‹
        english_samples = [
            {
                "input": "Behind the Great Wall: Void Arachne Targets Chinese-Speaking Users. Void Arachne group launched a campaign targeting Chinese users using SEO poisoning techniques.",
                "output": '{"entities":[{"labels":"Report","id":"report--great-wall","name":"Behind the Great Wall: Void Arachne Targets Chinese-Speaking Users","description":"å¨èƒæƒ…æŠ¥æŠ¥å‘Š"},{"labels":"ThreatOrganization","id":"threat-org--void-arachne","name":"Void Arachne","description":"å¨èƒç»„ç»‡"},{"labels":"AttackEvent","id":"attack-event--seo-campaign","name":"SEO Poisoning Campaign","description":"é’ˆå¯¹ä¸­æ–‡ç”¨æˆ·çš„SEOæŠ•æ¯’æ”»å‡»æ´»åŠ¨"},{"labels":"Target","id":"target--chinese-users","name":"Chinese-Speaking Users","description":"ä¸­æ–‡ç”¨æˆ·"},{"labels":"Technique","id":"technique--seo-poisoning","name":"SEO Poisoning","description":"æœç´¢å¼•æ“ä¼˜åŒ–æŠ•æ¯’æŠ€æœ¯"}],"relationships":[{"type":"BELONG","source":"report--great-wall","target":"attack-event--seo-campaign","confidence":0.95,"evidence":"æŠ¥å‘Šè®°å½•äº†SEOæŠ•æ¯’æ”»å‡»æ´»åŠ¨"},{"type":"LAUNCH","source":"threat-org--void-arachne","target":"attack-event--seo-campaign","confidence":0.95,"evidence":"Void Arachneç»„ç»‡å‘èµ·äº†æ”»å‡»æ´»åŠ¨"},{"type":"ATTACK","source":"attack-event--seo-campaign","target":"target--chinese-users","confidence":0.95,"evidence":"æ”»å‡»æ´»åŠ¨é’ˆå¯¹ä¸­æ–‡ç”¨æˆ·"},{"type":"ATTACK","source":"attack-event--seo-campaign","target":"technique--seo-poisoning","confidence":0.9,"evidence":"æ”»å‡»æ´»åŠ¨ä½¿ç”¨SEOæŠ•æ¯’æŠ€æœ¯"}]}'
            },
            {
                "input": "The malicious MSI file uses Dynamic Link Libraries during the installation process. The MSI installer deploys backdoor components to the system.",
                "output": '{"entities":[{"labels":"Tool","id":"tool--msi-file","name":"Malicious MSI File","description":"æ¶æ„MSIå®‰è£…æ–‡ä»¶"},{"labels":"Procedure","id":"procedure--dll-installation","name":"DLL Installation Process","description":"ä½¿ç”¨åŠ¨æ€é“¾æ¥åº“çš„å®‰è£…è¿‡ç¨‹"},{"labels":"Tool","id":"tool--dll","name":"Dynamic Link Libraries","description":"åŠ¨æ€é“¾æ¥åº“"}],"relationships":[{"type":"USE","source":"procedure--dll-installation","target":"tool--msi-file","confidence":0.9,"evidence":"å®‰è£…è¿‡ç¨‹ä½¿ç”¨MSIæ–‡ä»¶"},{"type":"USE","source":"procedure--dll-installation","target":"tool--dll","confidence":0.95,"evidence":"å®‰è£…è¿‡ç¨‹ä½¿ç”¨åŠ¨æ€é“¾æ¥åº“"}]}'
            },
            {
                "input": "The system was running normally without any suspicious activities detected during the monitoring period.",
                "output": '{"entities":[],"relationships":[]}'
            }
        ]

        # ä¸­æ–‡ç¤ºä¾‹
        chinese_samples = [
            {
                "input": "æµ·è²èŠ±ç»„ç»‡æ˜¯ç”±å¥‡å®‰ä¿¡å¨èƒæƒ…æŠ¥ä¸­å¿ƒæœ€æ—©æŠ«éœ²å¹¶å‘½åçš„ä¸€ä¸ªAPTç»„ç»‡ï¼Œè¯¥ç»„ç»‡é’ˆå¯¹ä¸­å›½æ”¿åºœã€ç§‘ç ”é™¢æ‰€ã€æµ·äº‹æœºæ„å±•å¼€äº†æœ‰ç»„ç»‡ã€æœ‰è®¡åˆ’ã€æœ‰é’ˆå¯¹æ€§çš„é•¿æ—¶é—´ä¸é—´æ–­æ”»å‡»ã€‚",
                "output": '{"entities":[{"labels":"Report","id":"report--qianxin-apt","name":"å¥‡å®‰ä¿¡å¨èƒæƒ…æŠ¥æŠ¥å‘Š","description":"å¨èƒæƒ…æŠ¥æŠ¥å‘Š"},{"labels":"ThreatOrganization","id":"threat-org--ocean-lotus","name":"æµ·è²èŠ±ç»„ç»‡","description":"APTå¨èƒç»„ç»‡"},{"labels":"AttackEvent","id":"attack-event--targeted-campaign","name":"é’ˆå¯¹æ€§æ”»å‡»æ´»åŠ¨","description":"æœ‰ç»„ç»‡æœ‰è®¡åˆ’çš„æ”»å‡»æ´»åŠ¨"},{"labels":"Target","id":"target--cn-gov","name":"ä¸­å›½æ”¿åºœæœºæ„","description":"æ”»å‡»ç›®æ ‡"},{"labels":"Target","id":"target--research-inst","name":"ç§‘ç ”é™¢æ‰€","description":"æ”»å‡»ç›®æ ‡"},{"labels":"Target","id":"target--maritime","name":"æµ·äº‹æœºæ„","description":"æ”»å‡»ç›®æ ‡"}],"relationships":[{"type":"BELONG","source":"report--qianxin-apt","target":"attack-event--targeted-campaign","confidence":0.95,"evidence":"æŠ¥å‘ŠæŠ«éœ²äº†é’ˆå¯¹æ€§æ”»å‡»æ´»åŠ¨"},{"type":"LAUNCH","source":"threat-org--ocean-lotus","target":"attack-event--targeted-campaign","confidence":0.95,"evidence":"æµ·è²èŠ±ç»„ç»‡å‘èµ·æ”»å‡»æ´»åŠ¨"},{"type":"ATTACK","source":"attack-event--targeted-campaign","target":"target--cn-gov","confidence":0.9,"evidence":"æ”»å‡»æ´»åŠ¨é’ˆå¯¹ä¸­å›½æ”¿åºœ"},{"type":"ATTACK","source":"attack-event--targeted-campaign","target":"target--research-inst","confidence":0.9,"evidence":"æ”»å‡»æ´»åŠ¨é’ˆå¯¹ç§‘ç ”é™¢æ‰€"},{"type":"ATTACK","source":"attack-event--targeted-campaign","target":"target--maritime","confidence":0.9,"evidence":"æ”»å‡»æ´»åŠ¨é’ˆå¯¹æµ·äº‹æœºæ„"}]}'
            },
            {
                "input": "æ”»å‡»è€…ä½¿ç”¨é±¼å‰å¼é’“é±¼é‚®ä»¶ä½œä¸ºåˆå§‹è®¿é—®æ‰‹æ®µï¼Œé‚®ä»¶ä¸­åŒ…å«æ¶æ„é™„ä»¶ï¼Œåˆ©ç”¨0dayæ¼æ´æ‰§è¡Œæ¶æ„ä»£ç ã€‚",
                "output": '{"entities":[{"labels":"Technique","id":"technique--spearphishing","name":"é±¼å‰å¼é’“é±¼é‚®ä»¶","description":"é’“é±¼æ”»å‡»æŠ€æœ¯"},{"labels":"Tool","id":"tool--malicious-attachment","name":"æ¶æ„é™„ä»¶","description":"æ”»å‡»å·¥å…·"},{"labels":"Tool","id":"tool--zero-day","name":"0dayæ¼æ´","description":"é›¶æ—¥æ¼æ´åˆ©ç”¨å·¥å…·"},{"labels":"Procedure","id":"procedure--code-execution","name":"æ¶æ„ä»£ç æ‰§è¡Œ","description":"ä»£ç æ‰§è¡Œç¨‹åº"}],"relationships":[{"type":"USE","source":"technique--spearphishing","target":"tool--malicious-attachment","confidence":0.95,"evidence":"é’“é±¼é‚®ä»¶ä½¿ç”¨æ¶æ„é™„ä»¶"},{"type":"USE","source":"procedure--code-execution","target":"tool--zero-day","confidence":0.9,"evidence":"ä»£ç æ‰§è¡Œåˆ©ç”¨0dayæ¼æ´"},{"type":"LAUNCH","source":"technique--spearphishing","target":"procedure--code-execution","confidence":0.85,"evidence":"é’“é±¼æŠ€æœ¯å¯åŠ¨ä»£ç æ‰§è¡Œ"}]}'
            }
        ]

        all_samples = english_samples + chinese_samples
        logger.info(f"ğŸ“‹ æå–äº†{len(all_samples)}ä¸ªfew-shotæ ·æœ¬")
        return all_samples

    def _augment_training_data(self, samples: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        æ•°æ®å¢å¼º
        é€šè¿‡å˜æ¢ç”Ÿæˆæ›´å¤šè®­ç»ƒæ ·æœ¬ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
        """
        augmented = samples.copy()

        # 1. åŒä¹‰è¯æ›¿æ¢

        synonyms = {
            # ä¸­æ–‡åŒä¹‰è¯
            "æ”»å‡»": ["å…¥ä¾µ", "ä¾µçŠ¯", "æ‰“å‡»", "è¿›çŠ¯", "è¢­å‡»"],
            "ç»„ç»‡": ["å›¢ä¼™", "é›†å›¢", "æœºæ„", "å›¢ä½“", "åŠ¿åŠ›"],
            "æ¼æ´": ["ç¼ºé™·", "å¼±ç‚¹", "ç ´ç»½", "ç‘•ç–µ", "éšæ‚£"],
            "æ¶æ„": ["æœ‰å®³", "ä¸è‰¯", "å±é™©", "ç ´åæ€§"],
            "è½¯ä»¶": ["ç¨‹åº", "åº”ç”¨", "ç³»ç»Ÿ", "å·¥å…·"],
            "ç½‘ç»œ": ["äº’è”ç½‘", "å› ç‰¹ç½‘", "åœ¨çº¿", "è”ç½‘"],

            # è‹±æ–‡åŒä¹‰è¯
            "attack": ["infiltration", "assault", "strike", "breach"],
            "organization": ["group", "team", "entity", "faction"],
            "vulnerability": ["weakness", "flaw", "gap", "loophole"],
            "malicious": ["harmful", "dangerous", "hostile", "destructive"],
            "software": ["program", "application", "tool", "system"],
            "network": ["internet", "web", "online", "cyber"]
        }

        synonym_samples = []
        # éšæœºé€‰æ‹©æ ·æœ¬è¿›è¡ŒåŒä¹‰è¯æ›¿æ¢ï¼Œçº¦å æ€»æ ·æœ¬çš„30%
        num_synonym_samples = max(3, int(len(samples) * 0.3))
        selected_samples = random.sample(
            samples, min(num_synonym_samples, len(samples)))

        for sample in selected_samples:
            new_input = sample["input"]
            # è¿›è¡Œå¤šæ¬¡æ›¿æ¢ä»¥å¢åŠ æ‰°åŠ¨
            for _ in range(random.randint(1, 3)):  # æ¯ä¸ªæ ·æœ¬æ›¿æ¢1-3æ¬¡
                word_to_replace = random.choice(list(synonyms.keys()))
                replacement = random.choice(synonyms[word_to_replace])
                new_input = new_input.replace(
                    word_to_replace, replacement, 1)  # åªæ›¿æ¢ä¸€æ¬¡

            synonym_samples.append({
                "input": new_input,
                "output": sample["output"]  # ä¿æŒæ ‡ç­¾ä¸å˜
            })

        augmented.extend(synonym_samples)

        # 2. å¥å­é¡ºåºè°ƒæ•´ï¼ˆå¯¹äºåŒ…å«å¤šä¸ªå¥å­çš„è¾“å…¥ï¼‰
        shuffle_samples = []
        # éšæœºé€‰æ‹©æ ·æœ¬è¿›è¡Œå¥å­é¡ºåºè°ƒæ•´ï¼Œçº¦å æ€»æ ·æœ¬çš„15%
        num_shuffle_samples = max(2, int(len(samples) * 0.15))
        shuffle_candidates = [s for s in samples if len(
            re.split(r'[.ã€‚!?ï¼ï¼Ÿ;ï¼›]', s["input"])) > 2]
        selected_shuffle_samples = random.sample(
            shuffle_candidates, min(num_shuffle_samples, len(shuffle_candidates)))

        for sample in selected_shuffle_samples:
            # æ›´æ™ºèƒ½åœ°åˆ†å‰²å¥å­ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
            # æ”¯æŒä¸­è‹±æ–‡å¥å­åˆ†å‰²
            sentences = re.split(r'[.ã€‚!?ï¼ï¼Ÿ;ï¼›]', sample["input"])
            sentences = [s.strip() for s in sentences if s.strip()]  # æ¸…ç†ç©ºå¥å­

            if len(sentences) > 2:
                random.shuffle(sentences)
                # æ ¹æ®æœ€åä¸€ä¸ªå¥å­æ˜¯å¦ä»¥æ ‡ç‚¹ç»“å°¾æ¥å†³å®šæ˜¯å¦æ·»åŠ æ ‡ç‚¹
                shuffled_input = "".join(
                    sentences) if sample["input"][-1] not in '.ã€‚!?ï¼ï¼Ÿ;ï¼›' else "".join(sentences) + "ã€‚"
                shuffle_samples.append({
                    "input": shuffled_input,
                    "output": sample["output"]
                })

        augmented.extend(shuffle_samples)

        # 3. æ·»åŠ å‰ç¼€/åç¼€æ‰°åŠ¨
        prefix_suffix_samples = []
        prefixes = [
            "æ ¹æ®æŠ¥å‘Šï¼Œ", "èµ„æ–™æ˜¾ç¤ºï¼Œ", "æ®åˆ†æï¼Œ", "ç ”ç©¶è¡¨æ˜ï¼Œ",
            "Based on the report, ", "According to the analysis, ",
            "As shown in the data, ", "Research indicates that "
        ]

        suffixes = [
            "è¿™æ˜¯é‡è¦çš„å®‰å…¨ä¿¡æ¯ã€‚", "éœ€è¦å¼•èµ·å…³æ³¨ã€‚", "å…·æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚",
            "This is important security information.",
            "This requires attention.",
            "It has important reference value."
        ]

        # éšæœºé€‰æ‹©æ ·æœ¬æ·»åŠ å‰ç¼€/åç¼€æ‰°åŠ¨ï¼Œçº¦å æ€»æ ·æœ¬çš„15%
        num_prefix_suffix_samples = max(2, int(len(samples) * 0.15))
        selected_prefix_suffix_samples = random.sample(
            samples, min(num_prefix_suffix_samples, len(samples)))

        for sample in selected_prefix_suffix_samples:
            # æ·»åŠ å‰ç¼€
            prefix_samples = [{
                "input": random.choice(prefixes) + sample["input"],
                "output": sample["output"]
            } for _ in range(2)]

            # æ·»åŠ åç¼€
            suffix_samples = [{
                "input": sample["input"] + random.choice(suffixes),
                "output": sample["output"]
            } for _ in range(2)]

            prefix_suffix_samples.extend(prefix_samples)
            prefix_suffix_samples.extend(suffix_samples)

        augmented.extend(prefix_suffix_samples)

        # 4. æ·»åŠ ç©ºç»“æœæ ·æœ¬
        empty_samples = [
            {
                "input": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšã€‚",
                "output": '{"entities":[],"relationships":[]}'
            },
            {
                "input": "The weather is nice today.",
                "output": '{"entities":[],"relationships":[]}'
            },
            {
                "input": "ç”¨æˆ·ç™»å½•ç³»ç»Ÿï¼ŒæŸ¥çœ‹äº†ä¸ªäººèµ„æ–™é¡µé¢ã€‚",
                "output": '{"entities":[],"relationships":[]}'
            },
            {
                "input": "ç”¨æˆ·åœ¨ç³»ç»Ÿä¸­æµè§ˆäº†äº§å“ä¿¡æ¯ã€‚",
                "output": '{"entities":[],"relationships":[]}'
            },
            {
                "input": "The user browsed product information in the system.",
                "output": '{"entities":[],"relationships":[]}'
            },
            {
                "input": "ä»–æ­£åœ¨é˜…è¯»ä¸€ç¯‡å…³äºç§‘æŠ€å‘å±•çš„æ–‡ç« ã€‚",
                "output": '{"entities":[],"relationships":[]}'
            }
        ]
        augmented.extend(empty_samples)

        logger.info(f"ğŸ“ˆ æ•°æ®å¢å¼ºå®Œæˆ: {len(samples)} -> {len(augmented)}ä¸ªæ ·æœ¬")
        return augmented

    def _format_for_training(self, samples: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ ¼å¼"""
        formatted = []

        # æ›´ç°å®çš„ç³»ç»Ÿæç¤ºè¯
        system_prompt = '''ä½ æ˜¯ç½‘ç»œå®‰å…¨çŸ¥è¯†å›¾è°±æå–ä¸“å®¶ã€‚ä»å¨èƒæƒ…æŠ¥æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚

    å®ä½“ç±»å‹åŒ…æ‹¬ï¼šThreatOrganization(å¨èƒç»„ç»‡), AttackEvent(æ”»å‡»äº‹ä»¶), Tool(å·¥å…·), Technique(æŠ€æœ¯), Target(ç›®æ ‡), Report(æŠ¥å‘Š), Tactic(æˆ˜æœ¯), Procedure(ç¨‹åº), Asset(èµ„äº§)
    å…³ç³»ç±»å‹åŒ…æ‹¬ï¼šLAUNCH(å‘èµ·), ATTACK(æ”»å‡»), USE(ä½¿ç”¨), BELONG(å±äº), HAS(æ‹¥æœ‰), IMPLEMENT(å®ç°)

    è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
    {"entities":[{"labels":"ç±»å‹","id":"å”¯ä¸€ID","name":"åç§°","description":"æè¿°"}],"relationships":[{"type":"å…³ç³»ç±»å‹","source":"æºå®ä½“ID","target":"ç›®æ ‡å®ä½“ID","confidence":0.95,"evidence":"è¯æ®"}]}

    æ³¨æ„äº‹é¡¹ï¼š
    1. åªæå–ä¸ç½‘ç»œå®‰å…¨ç›¸å…³çš„ä¿¡æ¯
    2. å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰å®‰å…¨ç›¸å…³å†…å®¹ï¼Œå¯ä»¥è¿”å›ç©ºæ•°ç»„
    3. å…³ç³»ä¸­çš„sourceå’Œtargetå¿…é¡»æ˜¯å®ä½“ä¸­çš„id
    4. å®ä½“IDåº”è¯¥æœ‰æ„ä¹‰ä¸”å”¯ä¸€
    5. è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—'''

        for sample in samples:
            formatted_sample = {
                "text": f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{sample['input']}<|im_end|>\n<|im_start|>assistant\n{sample['output']}<|im_end|>"
            }
            formatted.append(formatted_sample)

        return formatted

    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        logger.info("ğŸ”§ å¼€å§‹è®¾ç½®æ¨¡å‹...")
        self._setup_model_modelscope()

        # if self.use_alternative:
        #     self._setup_model_alternative()
        # else:
        #     self._setup_model_unsloth()

    def _setup_model_modelscope(self):
        """ä½¿ç”¨ModelScopeæ›¿ä»£æ–¹æ¡ˆï¼ˆå›½å†…ç”¨æˆ·æ¨èï¼‰"""
        try:
            from modelscope import AutoModelForCausalLM, AutoTokenizer
            from peft import LoraConfig, get_peft_model

            logger.info("ğŸ”§ ä½¿ç”¨ModelScopeè®¾ç½®æ¨¡å‹...")

            # ä½¿ç”¨ModelScopeçš„æ¨¡å‹ID
            model_id = self.config['model_id']

            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id,
                trust_remote_code=True,
                padding_side="right",
                truncation_side="right"
            )

            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # ç¡®ä¿tokenizeræœ‰pad_token_id
            if self.tokenizer.pad_token_id is None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            logger.info(
                f"ğŸ“‹ Tokenizeré…ç½®: pad_token={self.tokenizer.pad_token}, pad_token_id={self.tokenizer.pad_token_id}")

            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                quantization_config=None,
                low_cpu_mem_usage=True,
                max_memory={0: "20GiB"},  # é™åˆ¶GPUå†…å­˜ä½¿ç”¨
            )

            # é…ç½®LoRA
            lora_config = LoraConfig(
                r=self.config['lora_r'],
                lora_alpha=self.config['lora_alpha'],
                target_modules=self.config['target_modules'],
                lora_dropout=self.config['lora_dropout'],
                bias="none",
                task_type="CAUSAL_LM"
            )

            # åº”ç”¨LoRA
            self.model = get_peft_model(self.model, lora_config)

            # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼
            self.model.train()

            # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
            self.model.gradient_checkpointing_enable()

            logger.info("âœ… ModelScopeæ¨¡å‹è®¾ç½®å®Œæˆ")

        except ImportError:
            logger.error("âŒ è¯·å®‰è£…ModelScope: pip install modelscope")
            raise
        except Exception as e:
            logger.error(f"âŒ ModelScopeæ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            raise

    def _setup_model_unsloth(self):
        """ä½¿ç”¨ unsloth è®¾ç½®æ¨¡å‹"""
        try:
            from unsloth import FastLanguageModel

            # åŠ è½½æ¨¡å‹
            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=self.config['base_model'],
                max_seq_length=self.config['max_seq_length'],
                dtype=None,
                load_in_4bit=self.config['load_in_4bit'],
            )

            # æ·»åŠ LoRAé€‚é…å™¨
            self.model = FastLanguageModel.get_peft_model(
                self.model,
                r=self.config['lora_r'],
                target_modules=self.config['target_modules'],
                lora_alpha=self.config['lora_alpha'],
                lora_dropout=self.config['lora_dropout'],
                bias="none",
                use_gradient_checkpointing="unsloth",
                random_state=self.config['seed'],
            )

            logger.info("âœ… unsloth æ¨¡å‹è®¾ç½®å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ unsloth æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            logger.info("ğŸ”„ åˆ‡æ¢åˆ°æ›¿ä»£æ–¹æ¡ˆ...")
            self.use_alternative = True
            self._setup_model_alternative()

    def train(self, dataset: Dataset):
        """è®­ç»ƒæ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")

        try:
            from trl import SFTTrainer
            from transformers import TrainingArguments

            # ç¡®ä¿tokenizeré…ç½®æ­£ç¡®
            self.tokenizer.padding_side = "right"
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            # é¢„å¤„ç†æ•°æ®é›†
            def preprocess_function(examples):
                # å¯¹æ–‡æœ¬è¿›è¡Œtokenizationï¼Œæ·»åŠ paddingå’Œtruncation
                tokenized = self.tokenizer(
                    examples["text"],
                    truncation=True,
                    padding=True,
                    max_length=2048,  # å‡å°‘æœ€å¤§é•¿åº¦ä»¥èŠ‚çœæ˜¾å­˜
                    return_tensors="pt"
                )

                # ç¡®ä¿labelsä¸input_idsç›¸åŒï¼Œè¿™æ˜¯è¯­è¨€æ¨¡å‹çš„æ ‡å‡†åšæ³•
                tokenized["labels"] = tokenized["input_ids"].clone()
                return tokenized

            # å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†
            logger.info("ğŸ“Š é¢„å¤„ç†æ•°æ®é›†...")
            tokenized_dataset = dataset.map(
                preprocess_function,
                batched=True,
                remove_columns=dataset.column_names,
            )

            # è®­ç»ƒå‚æ•°
            training_args = TrainingArguments(
                per_device_train_batch_size=self.config['per_device_train_batch_size'],
                gradient_accumulation_steps=self.config['gradient_accumulation_steps'],
                warmup_steps=self.config['warmup_steps'],
                max_steps=self.config['max_steps'],
                learning_rate=self.config['learning_rate'],
                fp16=self.config['fp16'],
                bf16=self.config['bf16'],
                logging_steps=self.config['logging_steps'],
                optim=self.config['optim'],
                weight_decay=self.config['weight_decay'],
                lr_scheduler_type=self.config['lr_scheduler_type'],
                seed=self.config['seed'],
                output_dir=str(self.output_dir),
                save_steps=self.config['save_steps'],
                save_total_limit=self.config['save_total_limit'],
                dataloader_pin_memory=False,
                remove_unused_columns=False,
                dataloader_num_workers=0,
                gradient_checkpointing=True,
                gradient_checkpointing_kwargs={"use_reentrant": False},
                max_grad_norm=0.3,
                report_to=[],  # ç¦ç”¨wandbç­‰æŠ¥å‘Šå·¥å…·å¯èƒ½å‡å°‘å†…å­˜ä½¿ç”¨
            )

            # åˆ›å»ºSFTTrainer
            self.trainer = SFTTrainer(
                model=self.model,
                tokenizer=self.tokenizer,
                train_dataset=tokenized_dataset,
                args=training_args,
                max_seq_length=2048,
                packing=False,
            )

            # å¼€å§‹è®­ç»ƒ
            start_time = time.time()
            self.trainer.train()
            training_time = time.time() - start_time

            logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
            self.save_model()

        except ImportError:
            logger.error("âŒ æœªå®‰è£…trlï¼Œè¯·å…ˆå®‰è£…ï¼špip install trl")
            raise
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            raise

    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        model_path = self.output_dir / "final_model"

        try:
            self.model.save_pretrained(str(model_path))
            self.tokenizer.save_pretrained(str(model_path))
            logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

            # ä¿å­˜é…ç½®
            config_path = self.output_dir / "config.json"
            with open(config_path, 'w') as f:
                json.dump(self.config, f, indent=2)
            logger.info(f"ğŸ’¾ é…ç½®å·²ä¿å­˜: {config_path}")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            raise

    def load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            from unsloth import FastLanguageModel

            self.model, self.tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_path,
                max_seq_length=self.config['max_seq_length'],
                dtype=None,
                load_in_4bit=self.config['load_in_4bit'],
            )

            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")

        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise

    def test_inference(self, test_inputs: List[str]) -> List[Dict[str, Any]]:
        """
        æµ‹è¯•æ¨ç†æ•ˆæœ

        Args:
            test_inputs: æµ‹è¯•è¾“å…¥æ–‡æœ¬åˆ—è¡¨

        Returns:
            List[Dict]: æ¨ç†ç»“æœå’Œè§£æçŠ¶æ€
        """
        logger.info("ğŸ§ª å¼€å§‹æ¨ç†æµ‹è¯•...")

        if self.model is None:
            logger.error("âŒ æ¨¡å‹æœªåŠ è½½")
            return []

        results = []

        for i, input_text in enumerate(test_inputs):
            try:
                # æ„å»ºè¾“å…¥
                # æ›´å®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
                system_prompt = '''ä½ æ˜¯ç½‘ç»œå®‰å…¨çŸ¥è¯†å›¾è°±æå–ä¸“å®¶ã€‚ä»å¨èƒæƒ…æŠ¥æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚

        å®ä½“ç±»å‹åŒ…æ‹¬ï¼šThreatOrganization(å¨èƒç»„ç»‡), AttackEvent(æ”»å‡»äº‹ä»¶), Tool(å·¥å…·), Technique(æŠ€æœ¯), Target(ç›®æ ‡), Report(æŠ¥å‘Š), Tactic(æˆ˜æœ¯), Procedure(ç¨‹åº), Asset(èµ„äº§)
        å…³ç³»ç±»å‹åŒ…æ‹¬ï¼šLAUNCH(å‘èµ·), ATTACK(æ”»å‡»), USE(ä½¿ç”¨), BELONG(å±äº), HAS(æ‹¥æœ‰), IMPLEMENT(å®ç°)

        è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
        {"entities":[{"labels":"ç±»å‹","id":"å”¯ä¸€ID","name":"åç§°"}],"relationships":[{"type":"å…³ç³»ç±»å‹","source":"æºå®ä½“ID","target":"ç›®æ ‡å®ä½“ID"}]}

        æ³¨æ„äº‹é¡¹ï¼š
        1. åªæå–ä¸ç½‘ç»œå®‰å…¨ç›¸å…³çš„ä¿¡æ¯
        2. å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰å®‰å…¨ç›¸å…³å†…å®¹ï¼Œè¿”å›ç©ºæ•°ç»„ï¼š{"entities":[],"relationships":[]}
        3. å…³ç³»ä¸­çš„sourceå’Œtargetå¿…é¡»æ˜¯å®ä½“ä¸­çš„id
        4. å®ä½“IDåº”è¯¥æœ‰æ„ä¹‰ä¸”å”¯ä¸€
        5. è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—'''
                prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n"

                # æ¨ç†
                inputs = self.tokenizer(prompt, return_tensors="pt")

                # å°†è¾“å…¥å¼ é‡ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡
                inputs = {k: v.to(self.model.device)
                          for k, v in inputs.items()}

                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=2048,
                        temperature=0.1,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id
                    )

                # è§£ç è¾“å‡º
                response = self.tokenizer.decode(
                    outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)

                # å°è¯•è§£æJSON
                json_parse_success = False
                parsed_json = None

                try:
                    parsed_json = json.loads(response.strip())
                    json_parse_success = True
                except json.JSONDecodeError:
                    pass

                result = {
                    'input': input_text,
                    'raw_output': response,
                    'json_parse_success': json_parse_success,
                    'parsed_json': parsed_json
                }

                results.append(result)

                logger.info(
                    f"âœ… æµ‹è¯• {i+1}/{len(test_inputs)} - JSONè§£æ: {'æˆåŠŸ' if json_parse_success else 'å¤±è´¥'}")

            except Exception as e:
                logger.error(f"âŒ æ¨ç†å¤±è´¥ {i+1}: {e}")
                results.append({
                    'input': input_text,
                    'raw_output': '',
                    'json_parse_success': False,
                    'parsed_json': None,
                    'error': str(e)
                })

        return results

    def evaluate_json_parsing_improvement(self, test_inputs: List[str], finetuned_results: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        è¯„ä¼°JSONè§£ææ”¹å–„æ•ˆæœ

        å¯¹æ¯”fine-tuneå‰åçš„JSONè§£ææˆåŠŸç‡
        """
        logger.info("ğŸ“Š å¼€å§‹è¯„ä¼°JSONè§£ææ”¹å–„æ•ˆæœ...")
        if finetuned_results is None:
            logger.info("ğŸ§ª å¼€å§‹æ¨ç†æµ‹è¯•...")
            finetuned_results = self.test_inference(test_inputs)

        # ç»Ÿè®¡ç»“æœ
        total_tests = len(test_inputs)
        finetuned_success = sum(
            1 for r in finetuned_results if r['json_parse_success'])

        # æ¨¡æ‹ŸåŸå§‹æ¨¡å‹çš„è§£ææˆåŠŸç‡ï¼ˆåŸºäºå®é™…ç»éªŒï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯çœŸå®çš„åŸå§‹æ¨¡å‹æµ‹è¯•ç»“æœ
        original_success_rate = 0.65  # å‡è®¾åŸå§‹æˆåŠŸç‡65%
        original_success = int(total_tests * original_success_rate)

        improvement = {
            'total_tests': total_tests,
            'original_success': original_success,
            'original_success_rate': original_success_rate,
            'finetuned_success': finetuned_success,
            'finetuned_success_rate': finetuned_success / total_tests if total_tests > 0 else 0,
            'improvement': (finetuned_success / total_tests - original_success_rate) if total_tests > 0 else 0,
            'detailed_results': finetuned_results
        }

        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_path = self.output_dir / "evaluation_results.json"
        with open(eval_path, 'w', encoding='utf-8') as f:
            json.dump(improvement, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“ˆ è¯„ä¼°å®Œæˆ:")
        logger.info(f"   åŸå§‹æˆåŠŸç‡: {improvement['original_success_rate']:.1%}")
        logger.info(
            f"   Fine-tuneåæˆåŠŸç‡: {improvement['finetuned_success_rate']:.1%}")
        logger.info(f"   æ”¹å–„å¹…åº¦: {improvement['improvement']:+.1%}")
        logger.info(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {eval_path}")

        return improvement


def main(train_mode=True):
    """ä¸»å‡½æ•° - Fine-tune Demoæ¼”ç¤º

    Args:
        train_mode (bool): Trueè¡¨ç¤ºä»è®­ç»ƒå¼€å§‹ï¼ŒFalseè¡¨ç¤ºç›´æ¥åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†
    """

    logger.info("ğŸ¯ å¼€å§‹Fine-tune Demoæ¼”ç¤º")
    logger.info("=" * 60)

    # 1. åˆå§‹åŒ–Fine-tuner
    logger.info("ğŸ“‹ æ­¥éª¤1: åˆå§‹åŒ–Fine-tuner")
    fine_tuner = KnowledgeGraphFineTuner()

    if train_mode:
        # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
        logger.info("\nğŸ“‹ æ­¥éª¤2: å‡†å¤‡è®­ç»ƒæ•°æ®")
        dataset = fine_tuner.prepare_training_data()

        # 3. è®¾ç½®æ¨¡å‹ï¼ˆæ³¨æ„ï¼šéœ€è¦å®‰è£…unslothï¼‰
        logger.info("\nğŸ“‹ æ­¥éª¤3: è®¾ç½®æ¨¡å‹")
        try:
            fine_tuner.setup_model()
        except ImportError:
            logger.warning("âš ï¸ æœªå®‰è£…unslothï¼Œè·³è¿‡æ¨¡å‹è®­ç»ƒæ­¥éª¤")
            logger.info("ğŸ’¡ è¦å®Œæ•´è¿è¡Œdemoï¼Œè¯·å®‰è£…: pip install unsloth")
            return

        # 4. è®­ç»ƒæ¨¡å‹
        logger.info("\nğŸ“‹ æ­¥éª¤4: è®­ç»ƒæ¨¡å‹")
        fine_tuner.train(dataset)

        # ç¡®ä¿æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å¼
        if fine_tuner.model is not None:
            fine_tuner.model.eval()

    else:
        # ç›´æ¥åŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹
        logger.info("\nğŸ“‹ ç›´æ¥åŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹")
        try:
            # ä½¿ç”¨ModelScopeæ–¹å¼åŠ è½½æ¨¡å‹
            from modelscope import AutoModelForCausalLM, AutoTokenizer
            from peft import PeftModel

            model_path = "./fine_tune_output/final_model"

            # åŠ è½½tokenizer
            fine_tuner.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                padding_side="right",
                truncation_side="right"
            )

            # è®¾ç½®pad_token
            if fine_tuner.tokenizer.pad_token is None:
                fine_tuner.tokenizer.pad_token = fine_tuner.tokenizer.eos_token

            if fine_tuner.tokenizer.pad_token_id is None:
                fine_tuner.tokenizer.pad_token_id = fine_tuner.tokenizer.eos_token_id

            # åŠ è½½åŸºç¡€æ¨¡å‹ ä¿®æ”¹æˆæœåŠ¡å™¨ä¸Šå¯¹åº”çš„åŸºç¡€æ¨¡å‹
            base_model_path = "/root/.cache/modelscope/hub/models/qwen/Qwen2___5-7B-Instruct"
            fine_tuner.model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            )

            # åŠ è½½é€‚é…å™¨
            fine_tuner.model = PeftModel.from_pretrained(
                fine_tuner.model, model_path)

            # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            fine_tuner.model.eval()

            logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return

    # 5. æµ‹è¯•æ¨ç†æ•ˆæœ
    logger.info("\nğŸ“‹ æ­¥éª¤5: æµ‹è¯•æ¨ç†æ•ˆæœ")
    test_inputs = [
        "APT41 World Tour 2021 on a tight schedule\n=========================================  \nShare this article  \nFound it interesting? Don't hesitate to share it to wow your friends or colleagues  \n[â† Blog](/blog/)  \nShare this article  \nFound it interesting? Don't hesitate to share it to wow your friends or colleagues  \n[Nikita Rostovcev  \nCyber Intelligence Researcher](https://www.group-ib.com/author/nikita-rostovtsev/)  \nAPT41 World Tour 2021 on a tight schedule\n=========================================  \n4 malicious campaigns, 13 confirmed victims, and a new wave of Cobalt Strike infections  \nAugust 18, 2022 Â·  min to read Â· Advanced Persistent Threats  \nAPT41  \nThreat Intelligence",
        "# APT41 World Tour 2021 on a tight schedule\n\n4 malicious campaigns, 13 confirmed victims, and a new wave of Cobalt Strike infections  \nAugust 18, 2022 Â·  min to read Â· Advanced Persistent Threats  \nAPT41  \nThreat Intelligence  \nIn March 2022 one of the oldest state-sponsored hacker groups, **APT41**, breached government networks in six US states, including by exploiting a vulnerability in a livestock management system, Mandiant investigators [have reported](https://www.mandiant.com/resources/apt41-initiates-global-intrusion-campaign-using-multiple-exploits).",
        "# APT41 World Tour 2021 on a tight schedule\n\nInterestingly, according to sqlmap logs, the threat actors breached only half of the websites they were interested in. This suggests that even hackers like APT41 do not always go out of their way to ensure that a breach is successful.  \nThis blog post also uncovers subnets from which the threat actors connected to their C&C servers, which is further evidence confirming the threatâ€™s country of origin.  \nFor the first time, we were able to identify the groupâ€™s working hours in 2021, which are similar to regular office business hours.  \nIT directors, heads of cybersecurity teams, SOC analysts and incident response specialists are likely to find this material useful. Our goal is to reduce financial losses and infrastructure downtime as well as to help take preventive measures to fend off APT41 attacks.",
        "# APT41 World Tour 2021 on a tight schedule\n\nKey findings\n------------  \n* We estimate that in 2021 APT41 compromised and gained various levels of access to at least 13 organizations worldwide.\n* The groupâ€™s targets include government and private organizations based in the US, Taiwan, India, Thailand, China, Hong Kong, Mongolia, Indonesia, Vietnam, Bangladesh, Ireland, Brunei, and the UK.\n* In the campaigns that we analyzed, APT41 targeted the following industries: the government sector, manufacturing, healthcare, logistics, hospitality, finance, education, telecommunications, consulting, sports, media, and travel. The targets also included a political group, military organizations, and airlines.\n* To conduct reconnaissance, the threat actors use tools such as Acunetix, Nmap, Sqlmap, OneForAll, subdomain3, subDomainsBrute, and Sublist3r.\n* As an initial vector, the group uses web applications vulnerable to SQL injection attacks."
    ]

    results = fine_tuner.test_inference(test_inputs)

    # æ­¥éª¤6ï¼šè¯„ä¼°æ”¹å–„æ•ˆæœï¼ˆä¼ å…¥å·²æœ‰ç»“æœï¼Œé¿å…é‡å¤æ¨ç†ï¼‰
    logger.info("\nğŸ“‹ æ­¥éª¤6: è¯„ä¼°æ”¹å–„æ•ˆæœ")
    evaluation = fine_tuner.evaluate_json_parsing_improvement(
        test_inputs, results)

    # 7. æ€»ç»“
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ Fine-tune Demoæ¼”ç¤ºå®Œæˆ!")
    logger.info(f"ğŸ“Š JSONè§£ææˆåŠŸç‡æå‡: {evaluation['improvement']:+.1%}")
    logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {fine_tuner.output_dir}")
    logger.info("=" * 60)


if __name__ == "__main__":
    import sys
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦‚æœæä¾›äº† --inference å‚æ•°ï¼Œåˆ™ç›´æ¥è¿›è¡Œæ¨ç†
    train_mode = "--inference" not in sys.argv
    main(train_mode=train_mode)
