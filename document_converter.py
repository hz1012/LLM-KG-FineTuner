# coding:utf-8
"""
PDFè½¬æ¢æ¨¡å— - è´Ÿè´£å°†PDFæ–‡æ¡£è½¬æ¢ä¸ºMarkdownæ ¼å¼
"""
from typing import Tuple, List, Dict, Any
from docling.datamodel.base_models import InputFormat
from docling.document_converter import DocumentConverter as DoclingDocumentConverter
from docling.document_converter import PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any
import re

# æ–°å¢HTMLè½¬æ¢ç›¸å…³å¯¼å…¥
from markdownify import MarkdownConverter
from bs4 import BeautifulSoup
import requests

logger = logging.getLogger(__name__)

# coding:utf-8
"""
ç»Ÿä¸€æ–‡æ¡£è½¬æ¢å™¨ - è´Ÿè´£å„ç§æ–‡æ¡£æ ¼å¼çš„ç±»å‹æ£€æµ‹å’Œè½¬æ¢
"""

logger = logging.getLogger(__name__)


class DocumentConverter(DoclingDocumentConverter):
    """ç»Ÿä¸€æ–‡æ¡£è½¬æ¢å™¨ - æ”¯æŒPDFå’ŒHTML"""

    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–è½¬æ¢å™¨"""
        self.config = config

        # åˆå§‹åŒ–å„ç§è½¬æ¢å™¨
        pdf_config = config.get('pdf_converter', {})
        self.pdf_converter = PDFConverter(
            artifacts_path=pdf_config.get(
                'artifacts_path', './docling-models'),
            do_ocr=pdf_config.get('do_ocr', False)
        )

        html_config = config.get('html_converter', {})
        self.html_converter = HTMLConverter(**html_config)

        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹æ˜ å°„
        self.type_handlers = {
            'pdf': self._convert_pdf,
            'html': self._convert_html
        }

        logger.info(f"ğŸ“„ æ–‡æ¡£è½¬æ¢å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒç±»å‹: {list(self.type_handlers.keys())}")

    def detect_and_convert(self, file_path: str) -> Tuple[str, str, List[Dict[str, Any]]]:
        """
        æ£€æµ‹æ–‡ä»¶ç±»å‹å¹¶è½¬æ¢ä¸ºMarkdown

        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆæœ¬åœ°æ–‡ä»¶æˆ–URLï¼‰

        Returns:
            Tuple[æ–‡ä»¶ç±»å‹, Markdownå†…å®¹, æå–çš„å›¾ç‰‡ä¿¡æ¯]
        """
        # æ£€æµ‹æ–‡ä»¶ç±»å‹
        file_type = self.detect_file_type(file_path)
        logger.info(f"ğŸ“‹ æ£€æµ‹åˆ°æ–‡ä»¶ç±»å‹: {file_type}")

        # è½¬æ¢ä¸ºMarkdown
        markdown_content, images = self.convert_to_markdown(
            file_path, file_type)

        return file_type, markdown_content, images

    def detect_file_type(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        if file_path.startswith(('http://', 'https://')):
            # URLç±»å‹åˆ¤æ–­
            if file_path.lower().endswith('.pdf'):
                return 'pdf'
            else:
                return 'html'  # é»˜è®¤è®¤ä¸ºæ˜¯HTMLé¡µé¢
        else:
            # æœ¬åœ°æ–‡ä»¶ï¼Œæ ¹æ®æ‰©å±•ååˆ¤æ–­
            file_ext = Path(file_path).suffix.lower()
            if file_ext == '.pdf':
                return 'pdf'
            elif file_ext in ['.html', '.htm']:
                return 'html'
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")

    def convert_to_markdown(self, file_path: str, file_type: str) -> Tuple[str, List[Dict[str, Any]]]:
        """æ ¹æ®æ–‡ä»¶ç±»å‹è½¬æ¢ä¸ºMarkdown"""
        if file_type not in self.type_handlers:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")

        handler = self.type_handlers[file_type]
        return handler(file_path)

    def _convert_pdf(self, file_path: str) -> Tuple[str, List[Dict[str, Any]]]:
        """PDFè½¬æ¢"""
        markdown_content = self.pdf_converter.convert_pdf_to_markdown(
            file_path)
        return markdown_content, []  # PDFè½¬æ¢å™¨ä¸æå–å›¾ç‰‡ä¿¡æ¯

    def _convert_html(self, file_path: str) -> Tuple[str, List[Dict[str, Any]]]:
        """HTMLè½¬æ¢"""
        if file_path.startswith(('http://', 'https://')):
            # URL
            markdown_content, images = self.html_converter.convert_html_url_to_markdown(
                file_path)
        else:
            # æœ¬åœ°æ–‡ä»¶
            markdown_content, images = self.html_converter.convert_html_file_to_markdown(
                file_path)
        return markdown_content, images

    def get_supported_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨"""
        return list(self.type_handlers.keys())

    def add_converter(self, file_type: str, converter_func):
        """åŠ¨æ€æ·»åŠ æ–°çš„è½¬æ¢å™¨ï¼ˆä¾¿äºæ‰©å±•ï¼‰"""
        self.type_handlers[file_type] = converter_func
        logger.info(f"âœ… æ·»åŠ äº†æ–°çš„è½¬æ¢å™¨: {file_type}")


class PDFConverter:
    """PDFè½¬æ¢å™¨"""

    def __init__(self, artifacts_path: str = "./docling-models", do_ocr: bool = False):
        """
        åˆå§‹åŒ–PDFè½¬æ¢å™¨

        Args:
            artifacts_path: æ¨¡å‹è·¯å¾„
            do_ocr: æ˜¯å¦å¯ç”¨OCR
        """
        self.artifacts_path = artifacts_path
        self.pipeline_options = PdfPipelineOptions(
            artifacts_path=artifacts_path,
            do_ocr=do_ocr
        )

        self.doc_converter = DoclingDocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=self.pipeline_options)
            }
        )

    def convert_pdf_to_markdown(self, file_path: str) -> str:
        """
        å°†PDFè½¬æ¢ä¸ºMarkdownæ ¼å¼

        Args:
            file_path: PDFæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒæœ¬åœ°æ–‡ä»¶å’Œè¿œç¨‹URLï¼‰

        Returns:
            è½¬æ¢åçš„markdownå†…å®¹
        """
        try:
            logger.info(f"å¼€å§‹è½¬æ¢PDF: {file_path}")
            result = self.doc_converter.convert(source=file_path)
            markdown_content = result.document.export_to_markdown()
            logger.info("PDFè½¬æ¢å®Œæˆ")
            return markdown_content
        except Exception as e:
            logger.error(f"PDFè½¬æ¢å¤±è´¥: {e}")
            raise


class HTMLConverter:
    """HTMLè½¬æ¢å™¨ - å°†HTMLè½¬æ¢ä¸ºMarkdownå¹¶å•ç‹¬æå–å›¾ç‰‡ä¿¡æ¯"""

    def __init__(self, extract_images: bool = True, **options):
        """
        åˆå§‹åŒ–HTMLè½¬æ¢å™¨

        Args:
            extract_images : æ˜¯å¦æå–å¹¶ä¸‹è½½å›¾ç‰‡
            **other_options: å…¶ä½™ markdownify é€‰é¡¹
        """
        self.extract_images = extract_images
        self.options = options
        self.extracted_images = []  # å­˜å‚¨æå–çš„å›¾ç‰‡ä¿¡æ¯

    def convert_html_to_markdown(self, html_content: str) -> Tuple[str, List[Dict[str, Any]]]:
        """
        å°†HTMLè½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œå¹¶å•ç‹¬æå–å›¾ç‰‡ä¿¡æ¯

        Args:
            html_content: HTMLå†…å®¹å­—ç¬¦ä¸²

        Returns:
            Tuple[markdownå†…å®¹, å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨]
        """
        try:
            logger.info("å¼€å§‹è½¬æ¢HTMLåˆ°Markdown")

            # é‡ç½®å›¾ç‰‡æå–åˆ—è¡¨
            self.extracted_images = []

            # ğŸ”¥ æ–°å¢ï¼šHTMLé¢„å¤„ç†ï¼Œç§»é™¤æ— å…³å†…å®¹
            cleaned_html = self._preprocess_html_content(html_content)

            # ä½¿ç”¨è‡ªå®šä¹‰è½¬æ¢å™¨
            converter = self._create_custom_converter(**self.options)
            markdown_content = converter.convert(cleaned_html)

            logger.info(f"HTMLè½¬æ¢å®Œæˆï¼Œæå–äº†{len(self.extracted_images)}å¼ å›¾ç‰‡")

            return markdown_content, self.extracted_images.copy()

        except Exception as e:
            logger.error(f"HTMLè½¬æ¢å¤±è´¥: {e}")
            raise

    def convert_html_file_to_markdown(self, file_path: str, encoding: str = 'utf-8') -> Tuple[str, List[Dict[str, Any]]]:
        """
        ä»HTMLæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼

        Args:
            file_path: HTMLæ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 

        Returns:
            Tuple[markdownå†…å®¹, å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨]
        """
        try:
            logger.info(f"å¼€å§‹è½¬æ¢HTMLæ–‡ä»¶: {file_path}")

            with open(file_path, 'r', encoding=encoding) as f:
                html_content = f.read()

            return self.convert_html_to_markdown(html_content)

        except Exception as e:
            logger.error(f"HTMLæ–‡ä»¶è½¬æ¢å¤±è´¥: {e}")
            raise

    def convert_html_url_to_markdown(self, url: str, timeout: int = 30) -> Tuple[str, List[Dict[str, Any]]]:
        """
        ä»URLè·å–HTMLå¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼

        Args:
            url: HTMLé¡µé¢URL
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´

        Returns:
            Tuple[markdownå†…å®¹, å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨]
        """
        try:
            logger.info(f"å¼€å§‹ä»URLè·å–HTML: {url}")

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            response.encoding = response.apparent_encoding

            return self.convert_html_to_markdown(response.text)

        except Exception as e:
            logger.error(f"ä»URLè½¬æ¢HTMLå¤±è´¥: {e}")
            raise

    def _preprocess_html_content(self, html_content: str) -> str:
        """HTMLå†…å®¹é¢„å¤„ç†ï¼šç§»é™¤æ— å…³å†…å®¹ï¼Œä¿ç•™æ ¸å¿ƒå†…å®¹"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # ğŸ”¥ æ–°å¢ï¼šæå–æ–‡æ¡£æ ‡é¢˜
            document_title = self._extract_document_title(soup)

            # 1. ç§»é™¤æ— å…³å…ƒç´ 
            unwanted_selectors = [
                'nav', '.nav', '.navigation', '.navbar', '.menu',
                'header', '.header', '.site-header',
                'footer', '.footer', '.site-footer',
                '.sidebar', '.widget', '.advertisement', '.ads',
                '.breadcrumb', '.pagination', '.social-share',
                'script', 'style', 'noscript'
            ]

            for selector in unwanted_selectors:
                for element in soup.select(selector):
                    element.decompose()

            # 2. æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = self._extract_main_content_area(soup)

            # 3. æ¸…ç†ä¼ªè¡¨æ ¼
            self._clean_fake_tables(main_content)

            # ğŸ”¥ æ–°å¢ï¼šå¦‚æœæå–åˆ°æ ‡é¢˜ï¼Œå°†å…¶æ’å…¥åˆ°å†…å®¹å¼€å¤´
            content_html = str(main_content)
            if document_title:
                # åˆ›å»ºæ ‡é¢˜æ ‡ç­¾å¹¶æ’å…¥åˆ°å†…å®¹å¼€å¤´
                title_tag = soup.new_tag('h1')
                title_tag.string = document_title

                # å°†æ ‡é¢˜æ’å…¥åˆ°main_contentçš„å¼€å¤´
                if main_content.find():
                    main_content.insert(0, title_tag)
                else:
                    main_content.append(title_tag)

                logger.info(f"ğŸ“‹ æå–åˆ°æ–‡æ¡£æ ‡é¢˜: {document_title}")

            return str(main_content)

        except Exception as e:
            logger.warning(f"HTMLé¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹: {e}")
            return html_content

    def _extract_document_title(self, soup) -> str:
        """æå–æ–‡æ¡£æ ‡é¢˜"""
        try:
            # ğŸ”¥ æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šç§æ ‡é¢˜æ¥æº
            title_sources = [
                # 1. Metaæ ‡ç­¾æ ‡é¢˜ï¼ˆæœ€å¯é ï¼‰
                lambda: soup.find('meta', property='og:title'),
                lambda: soup.find('meta', attrs={'name': 'title'}),
                lambda: soup.find('meta', attrs={'name': 'dc.title'}),

                # 2. ä¸»å†…å®¹åŒºåŸŸçš„ç¬¬ä¸€ä¸ªh1
                lambda: self._find_main_h1(soup),

                # 3. é¡µé¢ç¬¬ä¸€ä¸ªh1
                lambda: soup.find('h1'),

                # 4. titleæ ‡ç­¾ï¼ˆä½†éœ€è¦æ¸…ç†ï¼‰
                lambda: soup.find('title'),

                # 5. ç‰¹å®šclassçš„æ ‡é¢˜å…ƒç´ 
                lambda: soup.find(
                    class_=['article-title', 'post-title', 'entry-title', 'title']),
            ]

            for source_func in title_sources:
                try:
                    element = source_func()
                    if element:
                        title_text = None

                        if element.name == 'meta':
                            title_text = element.get('content', '').strip()
                        else:
                            title_text = element.get_text().strip()

                        if title_text and len(title_text) > 5:  # æ ‡é¢˜è‡³å°‘5ä¸ªå­—ç¬¦
                            # æ¸…ç†æ ‡é¢˜
                            cleaned_title = self._clean_title_text(title_text)
                            if cleaned_title:
                                return cleaned_title
                except Exception as e:
                    logger.debug(f"æ ‡é¢˜æå–å¤±è´¥: {e}")
                    continue

            return ""

        except Exception as e:
            logger.warning(f"æ ‡é¢˜æå–è¿‡ç¨‹å‡ºé”™: {e}")
            return ""

    def _find_main_h1(self, soup):
        """åœ¨ä¸»å†…å®¹åŒºåŸŸæŸ¥æ‰¾h1æ ‡é¢˜"""
        # å…ˆå°è¯•åœ¨ä¸»å†…å®¹åŒºåŸŸæŸ¥æ‰¾
        main_selectors = [
            'main', '[role="main"]', 'article', '.article',
            '.post', '.entry', '.main-content', '.content',
            '.post-content', '#content', '#main-content'
        ]

        for selector in main_selectors:
            main_element = soup.select_one(selector)
            if main_element:
                h1 = main_element.find('h1')
                if h1:
                    return h1
        return None

    def _clean_title_text(self, title_text: str) -> str:
        """æ¸…ç†æ ‡é¢˜æ–‡æœ¬"""
        if not title_text:
            return ""

        # ç§»é™¤å¸¸è§çš„ç½‘ç«™åç¼€
        title_text = re.sub(r'\s*[\|\-â€“â€”]\s*.+$', '', title_text)

        # ç§»é™¤å¤šä½™ç©ºç™½
        title_text = ' '.join(title_text.split())

        # éªŒè¯æ ‡é¢˜è´¨é‡
        if len(title_text) < 5 or len(title_text) > 200:
            return ""

        # è¿‡æ»¤æ— æ„ä¹‰æ ‡é¢˜
        meaningless_patterns = [
            r'^(home|index|main|default)$',
            r'^(é¡µé¢|page)\s*\d*$',
            r'^(untitled|æ— æ ‡é¢˜)$'
        ]

        for pattern in meaningless_patterns:
            if re.match(pattern, title_text, re.IGNORECASE):
                return ""

        return title_text

    def _extract_main_content_area(self, soup):
        """æå–ä¸»è¦å†…å®¹åŒºåŸŸ"""
        # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾ä¸»å†…å®¹å®¹å™¨
        main_selectors = [
            'main', '[role="main"]',
            'article', '.article', '.post', '.entry',
            '.main-content', '.content', '.post-content',
            '#content', '#main-content'
        ]

        for selector in main_selectors:
            main_element = soup.select_one(selector)
            if main_element:
                logger.info(f"æ‰¾åˆ°ä¸»å†…å®¹åŒºåŸŸ: {selector}")
                return main_element

        # å¦‚æœæ‰¾ä¸åˆ°æ˜ç¡®çš„ä¸»å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨body
        body = soup.find('body')
        return body if body else soup

    def _clean_fake_tables(self, soup):
        """æ¸…ç†ä¼ªè¡¨æ ¼ï¼šç§»é™¤æ˜æ˜¾ç”¨äºå¸ƒå±€çš„è¡¨æ ¼"""
        tables = soup.find_all('table')

        for table in tables:
            if self._is_layout_table(table):
                # å°†å¸ƒå±€è¡¨æ ¼è½¬æ¢ä¸ºdivï¼Œä¿ç•™å†…å®¹
                table.name = 'div'
                table.attrs = {}
                logger.debug("æ¸…ç†å¸ƒå±€è¡¨æ ¼")

    def _is_layout_table(self, table) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¸ƒå±€è¡¨æ ¼ï¼ˆè€Œéæ•°æ®è¡¨æ ¼ï¼‰"""
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®è¡¨æ ¼çš„å…¸å‹ç‰¹å¾
        has_th = table.find('th') is not None
        has_thead = table.find('thead') is not None

        if has_th or has_thead:
            return False  # æœ‰è¡¨å¤´ï¼Œå¯èƒ½æ˜¯æ•°æ®è¡¨æ ¼

        # 2. æ£€æŸ¥è¡Œåˆ—ç»“æ„
        rows = table.find_all('tr')
        if len(rows) < 2:
            return True  # è¡Œæ•°å¤ªå°‘

        # 3. æ£€æŸ¥å•å…ƒæ ¼å†…å®¹
        cells = table.find_all(['td', 'th'])
        if len(cells) < 4:
            return True  # å•å…ƒæ ¼å¤ªå°‘

        # 4. æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§é‡é“¾æ¥ï¼ˆå¯¼èˆªç‰¹å¾ï¼‰
        links = table.find_all('a')
        if len(links) > len(cells) * 0.6:
            return True  # é“¾æ¥å¯†åº¦è¿‡é«˜

        # 5. æ£€æŸ¥å•å…ƒæ ¼å†…å®¹é•¿åº¦
        cell_texts = [cell.get_text().strip() for cell in cells]
        avg_length = sum(len(text) for text in cell_texts) / \
            len(cell_texts) if cell_texts else 0
        if avg_length < 15:
            return True  # å¹³å‡å†…å®¹è¿‡çŸ­

        return False

    def _create_custom_converter(self, **options) -> 'MarkdownConverter':
        """åˆ›å»ºè‡ªå®šä¹‰çš„Markdownè½¬æ¢å™¨"""

        class CustomMarkdownConverter(MarkdownConverter):
            """è‡ªå®šä¹‰Markdownè½¬æ¢å™¨ - æå–å›¾ç‰‡ä¿¡æ¯ä½†ä¸åœ¨markdownä¸­æ˜¾ç¤º"""

            def __init__(self, parent_html_converter, **options):
                super().__init__(**options)
                self.parent = parent_html_converter
                self.image_counter = 0

            def convert_img(self, el, text, parent_tags):
                # å¦‚æœå…¨å±€å¼€å…³å…³é—­ï¼Œç›´æ¥å¿½ç•¥
                if not self.parent.extract_images:
                    return ''

                """é‡å†™å›¾ç‰‡è½¬æ¢æ–¹æ³• - æå–å›¾ç‰‡ä¿¡æ¯ä½†ä¸æ˜¾ç¤ºåœ¨markdownä¸­"""
                # è·å–å›¾ç‰‡å±æ€§
                placement = el.attrs.get('placement', None)
                alt = el.attrs.get('alt', '') or ''
                src = el.attrs.get('src', '') or ''
                title = el.attrs.get('title', '') or ''
                width = el.attrs.get('width', '')
                height = el.attrs.get('height', '')

                # å¦‚æœæ˜¯inlineå›¾æ ‡ï¼Œç›´æ¥å¿½ç•¥
                if placement == 'inline':
                    return ''

                # æ„å»ºå›¾ç‰‡ä¿¡æ¯
                self.image_counter += 1
                image_info = {
                    'id': f"image_{self.image_counter}",
                    'src': src,
                    'alt': alt,
                    'title': title,
                    'width': width,
                    'height': height,
                    'placement': placement,
                    'attributes': dict(el.attrs)  # ä¿å­˜æ‰€æœ‰å±æ€§
                }

                # å°† parent_converter æ”¹ä¸º parent
                self.parent.extracted_images.append(image_info)

                # åœ¨markdownä¸­ä¸æ˜¾ç¤ºå›¾ç‰‡ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²æˆ–è€…å›¾ç‰‡å ä½ç¬¦
                return ''  # å®Œå…¨ä¸æ˜¾ç¤ºå›¾ç‰‡

            def convert_a(self, el, text, parent_tags):
                """é‡å†™é“¾æ¥è½¬æ¢æ–¹æ³• - å¤„ç†é“¾æ¥ä¸­çš„å›¾ç‰‡"""
                # æ£€æŸ¥é“¾æ¥æ˜¯å¦æŒ‡å‘å›¾ç‰‡æ–‡ä»¶
                href = el.attrs.get('href', '')
                # å¢å¼ºå›¾ç‰‡é“¾æ¥è¯†åˆ«èƒ½åŠ›ï¼Œæ”¯æŒæ›´å¤šæƒ…å†µ
                image_extensions = ['.png', '.jpg',
                                    '.jpeg', '.gif', '.bmp', '.svg', '.webp']
                is_image_link = href and any(
                    href.lower().endswith(ext) for ext in image_extensions)

                # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœé“¾æ¥æ–‡æœ¬æ˜¯"download"ä¸”é“¾æ¥æŒ‡å‘å›¾ç‰‡ï¼Œåˆ™è®¤ä¸ºæ˜¯å›¾ç‰‡é“¾æ¥
                is_download_image = (text.lower().strip(
                ) == 'download' or 'ä¸‹è½½' in text) and is_image_link

                if is_image_link or is_download_image:
                    # å¦‚æœæ˜¯å›¾ç‰‡é“¾æ¥ï¼Œæå–å›¾ç‰‡ä¿¡æ¯
                    if self.parent.extract_images:
                        self.image_counter += 1
                        image_info = {
                            'id': f"image_{self.image_counter}",
                            'src': href,
                            'alt': el.attrs.get('title', '') or text or '',
                            'title': el.attrs.get('title', '') or text or '',
                            'width': '',
                            'height': '',
                            'placement': 'link',
                            'attributes': dict(el.attrs)
                        }
                        self.parent.extracted_images.append(image_info)

                    # è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä¸åœ¨markdownä¸­æ˜¾ç¤ºå›¾ç‰‡é“¾æ¥
                    return ''

                # éå›¾ç‰‡é“¾æ¥ä½¿ç”¨é»˜è®¤å¤„ç†
                return super().convert_a(el, text, parent_tags)

            def convert_table(self, el, text, parent_tags):
                """å¤„ç†è¡¨æ ¼è½¬æ¢ - å¢å¼ºä¼ªè¡¨æ ¼æ£€æµ‹"""
                # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„parentå¼•ç”¨
                if hasattr(self.parent, '_is_valid_data_table') and not self.parent._is_valid_data_table(el):
                    # å¦‚æœæ˜¯å¸ƒå±€è¡¨æ ¼ï¼Œè½¬æ¢ä¸ºæ™®é€šæ–‡æœ¬
                    return self.process_tag(el, convert_children_only=True)

                # å¯¹äºæœ‰æ•ˆçš„æ•°æ®è¡¨æ ¼ï¼Œä½¿ç”¨é»˜è®¤è½¬æ¢
                return super().convert_table(el, text, parent_tags)

        return CustomMarkdownConverter(self, **options)

    def _is_valid_data_table(self, table_element) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ•°æ®è¡¨æ ¼"""
        # æ£€æŸ¥è¡¨æ ¼ç»“æ„
        rows = table_element.find_all('tr')
        if len(rows) < 2:
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰è¡¨å¤´
        has_headers = table_element.find(
            'th') is not None or table_element.find('thead') is not None

        # æ£€æŸ¥å•å…ƒæ ¼æ•°é‡å’Œå†…å®¹è´¨é‡
        cells = table_element.find_all(['td', 'th'])
        if len(cells) < 4:
            return False

        # æ£€æŸ¥å†…å®¹è´¨é‡ï¼ˆéå¯¼èˆªé“¾æ¥ï¼‰
        text_content = table_element.get_text().strip()
        if len(text_content) < 50:  # å†…å®¹å¤ªå°‘
            return False

        return True

    def get_extracted_images(self) -> List[Dict[str, Any]]:
        """è·å–æœ€åä¸€æ¬¡è½¬æ¢æå–çš„å›¾ç‰‡ä¿¡æ¯"""
        return self.extracted_images.copy()

    def save_extracted_images(self, output_path: str) -> None:
        """ä¿å­˜æå–çš„å›¾ç‰‡ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
        try:
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.extracted_images, f,
                          ensure_ascii=False, indent=2)
            logger.info(f"å›¾ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜å›¾ç‰‡ä¿¡æ¯å¤±è´¥: {e}")
            raise

    def download_images(self, output_dir: str, base_url: str = None) -> Dict[str, str]:
        """
        ä¸‹è½½æå–çš„å›¾ç‰‡åˆ°æŒ‡å®šç›®å½•

        Args:
            output_dir: è¾“å‡ºç›®å½•
            base_url: åŸºç¡€URLï¼ˆç”¨äºç›¸å¯¹è·¯å¾„çš„å›¾ç‰‡ï¼‰

        Returns:
            Dict[åŸå§‹URL, æœ¬åœ°æ–‡ä»¶è·¯å¾„]
        """
        import os
        import urllib.parse
        from pathlib import Path
        from urllib.parse import urlparse, urljoin

        downloaded_files = {}
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        for i, img_info in enumerate(self.extracted_images):
            src = img_info['src']
            if not src:
                continue

            try:
                # è·³è¿‡data URL - æ”¹è¿›æç¤ºä¿¡æ¯
                if src.startswith('data:'):
                    logger.info(f"è·³è¿‡data URLå›¾ç‰‡: {img_info['id']} (æ•°æ®å·²åµŒå…¥HTML)")
                    continue

                # æ”¹è¿›çš„URLå¤„ç†é€»è¾‘
                final_url = self._resolve_image_url(src, base_url)

                if not final_url:
                    logger.warning(f"æ— æ³•è§£æå›¾ç‰‡URL: {src}")
                    continue

                # éªŒè¯URLæ ¼å¼
                parsed = urlparse(final_url)
                if not parsed.scheme or not parsed.netloc:
                    logger.warning(f"æ— æ•ˆçš„å›¾ç‰‡URL: {final_url}")
                    continue

                # ç”Ÿæˆæ–‡ä»¶å
                file_name = self._generate_image_filename(final_url, i + 1)
                file_path = output_path / file_name

                # ä¸‹è½½å›¾ç‰‡
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }

                response = requests.get(final_url, headers=headers, timeout=30)
                response.raise_for_status()

                with open(file_path, 'wb') as f:
                    f.write(response.content)

                downloaded_files[img_info['src']] = str(file_path)
                logger.debug(f"âœ… ä¸‹è½½æˆåŠŸ: {final_url} -> {file_path}")

            except Exception as e:
                logger.warning(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥ {src}: {e}")

        logger.info(f"ğŸ“¥ æˆåŠŸä¸‹è½½{len(downloaded_files)}å¼ å›¾ç‰‡åˆ°: {output_dir}")
        return downloaded_files

    def _resolve_image_url(self, src: str, base_url: str = None) -> str:
        """
        è§£æå›¾ç‰‡URLï¼Œå¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„

        Args:
            src: åŸå§‹å›¾ç‰‡URL
            base_url: åŸºç¡€URL

        Returns:
            è§£æåçš„å®Œæ•´URL
        """
        from urllib.parse import urlparse, urljoin

        # å¦‚æœå·²ç»æ˜¯å®Œæ•´URLï¼Œç›´æ¥è¿”å›
        if src.startswith(('http://', 'https://')):
            return src

        # å¦‚æœæ²¡æœ‰base_urlï¼Œæ— æ³•å¤„ç†ç›¸å¯¹è·¯å¾„
        if not base_url:
            logger.warning(f"ç¼ºå°‘base_urlï¼Œæ— æ³•å¤„ç†ç›¸å¯¹è·¯å¾„: {src}")
            return None

        # è§£æbase_url
        parsed_base = urlparse(base_url)
        if not parsed_base.scheme or not parsed_base.netloc:
            logger.warning(f"æ— æ•ˆçš„base_url: {base_url}")
            return None

        # å¤„ç†ä¸åŒç±»å‹çš„ç›¸å¯¹è·¯å¾„
        if src.startswith('/'):
            # ç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºåŸŸåæ ¹ç›®å½•ï¼‰
            # ä¾‹å¦‚ï¼š/wp-content/themes/... -> https://example.com/wp-content/themes/...
            final_url = f"{parsed_base.scheme}://{parsed_base.netloc}{src}"
        else:
            # ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºå½“å‰é¡µé¢ï¼‰
            # ä¾‹å¦‚ï¼šimages/pic.jpg -> https://example.com/page/images/pic.jpg
            final_url = urljoin(base_url, src)

        return final_url

    def _generate_image_filename(self, url: str, index: int) -> str:
        """
        ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å

        Args:
            url: å›¾ç‰‡URL
            index: å›¾ç‰‡åºå·

        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶å
        """
        from pathlib import Path
        import re

        try:
            # ä»URLä¸­æå–æ–‡ä»¶å
            path = Path(url)
            original_name = path.name

            # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œæ·»åŠ é»˜è®¤æ‰©å±•å
            if not path.suffix:
                # æ ¹æ®URLæˆ–å†…å®¹ç±»å‹æ¨æ–­æ‰©å±•å
                if 'svg' in url.lower():
                    original_name += '.svg'
                elif any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                    pass  # å·²æœ‰æ‰©å±•å
                else:
                    original_name += '.jpg'  # é»˜è®¤æ‰©å±•å

            # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
            clean_name = re.sub(r'[<>:"/\\|?*]', '_', original_name)

            # ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶å
            final_name = f"image_{index:03d}_{clean_name}"

            return final_name

        except Exception as e:
            logger.warning(f"ç”Ÿæˆæ–‡ä»¶åå¤±è´¥: {e}")
            return f"image_{index:03d}.jpg"


def markdownify_with_custom_converter(html: str, **options) -> Tuple[str, List[Dict[str, Any]]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨è‡ªå®šä¹‰è½¬æ¢å™¨å°†HTMLè½¬æ¢ä¸ºMarkdownå¹¶æå–å›¾ç‰‡

    Args:
        html: HTMLå†…å®¹
        **options: markdownifyé€‰é¡¹

    Returns:
        Tuple[markdownå†…å®¹, å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨]
    """
    converter = HTMLConverter(**options)
    return converter.convert_html_to_markdown(html)
