# coding:utf-8
"""
æ–‡æ¡£åˆ†å—æ¨¡å— - è´Ÿè´£å°†æ–‡æ¡£åˆ‡åˆ†æˆåˆé€‚å¤§å°çš„å—
"""
import re
from typing import List, Dict, Any, Tuple
from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
import logging
from openai import OpenAI

logger = logging.getLogger(__name__)

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ç”¨äºè®¡ç®—tokené•¿åº¦
embedding_client = OpenAI(
    api_key="sk-d8626ac601d843d1800a0e349f7c3c8b",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)


def get_token_length(text: str) -> int:
    """
    ä½¿ç”¨OpenAI embeddingæ¨¡å‹è®¡ç®—æ–‡æœ¬çš„tokené•¿åº¦

    Args:
        text: è¾“å…¥æ–‡æœ¬

    Returns:
        int: æ–‡æœ¬çš„tokenæ•°é‡
    """
    try:
        # ä½¿ç”¨OpenAIçš„embeddingæ¥å£è®¡ç®—tokenæ•°é‡
        response = embedding_client.embeddings.create(
            input=text,
            model="text-embedding-v2"
        )
        # ä»responseä¸­è·å–tokenä½¿ç”¨æƒ…å†µ
        if hasattr(response, 'usage') and hasattr(response.usage, 'prompt_tokens'):
            return response.usage.prompt_tokens
        else:
            # å¦‚æœæ— æ³•è·å–usageä¿¡æ¯ï¼Œåˆ™ä½¿ç”¨ä¸€ä¸ªä¼°ç®—æ–¹æ³•
            # æ ¹æ®DashScopeæ–‡æ¡£ï¼Œtext-embedding-v2æ¨¡å‹çš„tokenè®¡ç®—æ–¹æ³•
            logger.warning("æ— æ³•è·å–usageä¿¡æ¯ï¼Œä½¿ç”¨ç²—ç•¥çš„ä¼°ç®—æ–¹æ³•")
            return len(text) // 4  # ä¸€ä¸ªç²—ç•¥çš„ä¼°ç®—ï¼Œä¸€èˆ¬ä¸­æ–‡æ¯ä¸ªtokençº¦4ä¸ªå­—ç¬¦
    except Exception as e:
        logger.warning(f"æ— æ³•ä½¿ç”¨OpenAIæ¨¡å‹è®¡ç®—tokené•¿åº¦ï¼Œä½¿ç”¨å­—ç¬¦é•¿åº¦ä¼°ç®—: {e}")
        # å‡ºé”™æ—¶å›é€€åˆ°å­—ç¬¦é•¿åº¦ä¼°ç®—
        return len(text) // 4


class ChunkSplitter:
    """æ–‡æ¡£åˆ†å—å™¨"""

    def __init__(self, max_chunk_size: int = 2000, chunk_overlap: int = 200, document_title: str = None):
        """
        åˆå§‹åŒ–åˆ†å—å™¨

        Args:
            max_chunk_size: æœ€å¤§å—å¤§å°
            chunk_overlap: å—é‡å å¤§å°
        """
        self.max_chunk_size = max_chunk_size
        self.chunk_overlap = chunk_overlap
        self.document_title = document_title

        # è®¾ç½®æ ‡é¢˜å±‚çº§ï¼ŒåŒ…å«åˆ°4çº§æ ‡é¢˜
        self.headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
            ("####", "Header 4"),
        ]

        self.markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=self.headers_to_split_on,
            strip_headers=False
        )

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=max_chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=get_token_length,
            separators=["\n\n", "\n", " ", ""]
        )

    def split_markdown_by_headers(self, markdown_content: str) -> List[Document]:
        """ä½¿ç”¨æ ‡é¢˜åˆ†å‰²markdownæ–‡æ¡£"""
        try:
            # ğŸ”¥ æ–°å¢ï¼šæå–å¹¶ç¼“å­˜æ–‡æ¡£ä¸»æ ‡é¢˜ å¦‚æœæ²¡æœ‰ç»™æ ‡é¢˜
            if not self.document_title:
                self.document_title = self._extract_main_title(
                    markdown_content)

            # è·å–åˆ†å‰²åçš„æ–‡æ¡£å—
            docs = self.markdown_splitter.split_text(markdown_content)

            # ä¸ºæ¯ä¸ªæ–‡æ¡£å—æ·»åŠ å®Œæ•´çš„æ ‡é¢˜å±‚æ¬¡ç»“æ„ä¿¡æ¯
            enhanced_docs = []
            for doc in docs:
                # æå–å½“å‰å—çš„å®Œæ•´æ ‡é¢˜å±‚æ¬¡ç»“æ„
                header_hierarchy = self._extract_header_hierarchy(
                    markdown_content, doc)
                # å°†æ ‡é¢˜å±‚æ¬¡ç»“æ„å­˜å‚¨åœ¨metadataä¸­
                doc.metadata['header_hierarchy'] = header_hierarchy
                enhanced_docs.append(doc)

            return enhanced_docs
        except Exception as e:
            logger.error(f"æ ‡é¢˜åˆ†å‰²å¤±è´¥: {e}")
            # é™çº§åˆ°æ™®é€šæ–‡æœ¬åˆ†å‰²
            return [Document(page_content=markdown_content, metadata={})]

    def _extract_header_hierarchy(self, markdown_content: str, doc: Document) -> List[str]:
        """æå–æ–‡æ¡£å—çš„å®Œæ•´æ ‡é¢˜å±‚æ¬¡ç»“æ„"""
        # è·å–æ–‡æ¡£çš„æ‰€æœ‰æ ‡é¢˜è¡Œ
        lines = markdown_content.split('\n')
        all_headers = []

        for line in lines:
            line_stripped = line.strip()
            # æ£€æµ‹å„çº§æ ‡é¢˜
            if (line_stripped.startswith('# ') or
                line_stripped.startswith('## ') or
                line_stripped.startswith('### ') or
                    line_stripped.startswith('#### ')) and not line_stripped.startswith('#####'):
                all_headers.append(line_stripped)

        # è·å–å½“å‰æ–‡æ¡£å—çš„å†…å®¹è¡Œ
        doc_lines = doc.page_content.split('\n')
        # æ‰¾åˆ°å½“å‰å—ä¸­çš„æ ‡é¢˜
        current_headers = []
        for line in doc_lines:
            line_stripped = line.strip()
            if (line_stripped.startswith('# ') or
                line_stripped.startswith('## ') or
                line_stripped.startswith('### ') or
                    line_stripped.startswith('#### ')) and not line_stripped.startswith('#####'):
                current_headers.append(line_stripped)

        # å¦‚æœå½“å‰å—æ²¡æœ‰æ ‡é¢˜ï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨
        if not current_headers:
            return []

        # è·å–å½“å‰å—çš„ä¸»è¦æ ‡é¢˜ï¼ˆæœ€åä¸€ä¸ªæ ‡é¢˜ï¼‰
        target_header = current_headers[-1] if current_headers else None
        if not target_header:
            return []

        # æ„å»ºä»æ–‡æ¡£æ ‡é¢˜åˆ°ç›®æ ‡æ ‡é¢˜çš„å®Œæ•´è·¯å¾„
        header_hierarchy = []
        if self.document_title:
            header_hierarchy.append(f"# {self.document_title}")

        # è·Ÿè¸ªå½“å‰çš„æ ‡é¢˜å±‚æ¬¡ç»“æ„
        current_hierarchy = [
            f"# {self.document_title}"] if self.document_title else []

        # éå†æ‰€æœ‰æ ‡é¢˜ï¼Œæ„å»ºå±‚æ¬¡ç»“æ„
        for header in all_headers:
            # è·³è¿‡æ–‡æ¡£ä¸»æ ‡é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æ·»åŠ äº†
            if header.startswith('# ') and self.document_title and self.document_title in header:
                continue

            # ç¡®å®šæ ‡é¢˜çº§åˆ«
            level = header.count('#')

            # è°ƒæ•´å½“å‰å±‚æ¬¡ç»“æ„çš„æ·±åº¦
            if level > len(current_hierarchy):
                current_hierarchy.append(header)
            else:
                current_hierarchy = current_hierarchy[:level-1]
                current_hierarchy.append(header)

            # å¦‚æœæ‰¾åˆ°äº†ç›®æ ‡æ ‡é¢˜ï¼Œåˆ™ä¿å­˜å½“å‰å±‚æ¬¡ç»“æ„
            if header == target_header:
                # åªä¿ç•™ä»æ–‡æ¡£æ ‡é¢˜å¼€å§‹çš„å®Œæ•´è·¯å¾„
                header_hierarchy = current_hierarchy[:]
                break

        return header_hierarchy

    def extract_markdown_table_and_text(self, docs: List[Document]) -> List[Document]:
        """åŒºåˆ†è¡¨æ ¼å’Œæ–‡æœ¬å†…å®¹ï¼Œå°†æ··åˆå†…å®¹æ‹†åˆ†æˆä¸åŒçš„doc"""
        result_docs = []

        for doc in docs:
            content = doc.page_content
            lines = content.split('\n')

            current_segment = []
            current_type = None

            for line in lines:
                line_stripped = line.strip()

                # åˆ¤æ–­æ˜¯å¦ä¸ºè¡¨æ ¼è¡Œ
                is_table = (line_stripped.startswith('|') and
                            line_stripped.endswith('|') and
                            line_stripped.count('|') >= 2)

                # åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜è¡Œ
                is_header = (line_stripped.startswith('#') and 
                           not line_stripped.startswith('#####'))

                if is_table:
                    # å¦‚æœä¹‹å‰æ˜¯æ–‡æœ¬ï¼Œå…ˆä¿å­˜æ–‡æœ¬æ®µ
                    if current_type == 'text' and current_segment:
                        text_content = '\n'.join(current_segment).strip()
                        # ğŸ”¥ ä¿®æ”¹ï¼šè¿‡æ»¤æ‰åªåŒ…å«æ ‡é¢˜å’Œç©ºè¡Œçš„æ–‡æœ¬æ®µ
                        if text_content and not self._is_only_headers_and_empty_lines(current_segment):
                            new_doc = Document(
                                page_content=text_content,
                                metadata={**doc.metadata,
                                          'content_type': 'text'}
                            )
                            result_docs.append(new_doc)
                        current_segment = []

                    current_type = 'table'
                    current_segment.append(line)

                else:
                    # å¦‚æœä¹‹å‰æ˜¯è¡¨æ ¼ï¼Œå…ˆä¿å­˜è¡¨æ ¼æ®µ
                    if current_type == 'table' and current_segment:
                        table_content = '\n'.join(current_segment).strip()
                        if table_content:
                            new_doc = Document(
                                page_content=table_content,
                                metadata={**doc.metadata,
                                          'content_type': 'table'}
                            )
                            result_docs.append(new_doc)
                        current_segment = []

                    current_type = 'text'
                    current_segment.append(line)

            # å¤„ç†æœ€åä¸€æ®µ
            if current_segment:
                final_content = '\n'.join(current_segment).strip()
                # ğŸ”¥ ä¿®æ”¹ï¼šè¿‡æ»¤æ‰åªåŒ…å«æ ‡é¢˜å’Œç©ºè¡Œçš„æ–‡æœ¬æ®µ
                if final_content and not self._is_only_headers_and_empty_lines(current_segment):
                    new_doc = Document(
                        page_content=final_content,
                        metadata={**doc.metadata,
                                  'content_type': current_type or 'text'}
                    )
                    result_docs.append(new_doc)

        return result_docs

    def _is_only_headers_and_empty_lines(self, lines: List[str]) -> bool:
        """
        ğŸ”¥ æ–°å¢ï¼šåˆ¤æ–­æ–‡æœ¬æ®µæ˜¯å¦åªåŒ…å«æ ‡é¢˜å’Œç©ºè¡Œï¼Œæ²¡æœ‰å®è´¨æ€§å†…å®¹
        
        Args:
            lines: æ–‡æœ¬è¡Œåˆ—è¡¨
            
        Returns:
            bool: å¦‚æœåªåŒ…å«æ ‡é¢˜å’Œç©ºè¡Œè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        substantive_content_count = 0
        
        for line in lines:
            line_stripped = line.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œæ ‡é¢˜è¡Œ
            if not line_stripped or (line_stripped.startswith('#') and not line_stripped.startswith('#####')):
                continue
                
            # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†éš”ç¬¦è¡Œï¼ˆå¦‚ ---ï¼‰
            if re.match(r'^\s*[-=]{3,}\s*$', line_stripped):
                continue
                
            # å…¶ä»–è¡Œéƒ½ç®—ä½œå®è´¨æ€§å†…å®¹
            substantive_content_count += 1
            
        # å¦‚æœæ²¡æœ‰å®è´¨æ€§å†…å®¹ï¼Œè¿”å›True
        return substantive_content_count == 0

    def _extract_header_context(self, content: str) -> List[str]:
        """ğŸ”¥ æ–°å¢ï¼šæå–å®Œæ•´çš„æ ‡é¢˜ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒ1-4çº§æ ‡é¢˜ï¼‰"""
        lines = content.split('\n')
        header_context = []

        # ç”¨äºè·Ÿè¸ªå½“å‰å„çº§æ ‡é¢˜çš„çŠ¶æ€
        current_headers = ["", "", "", ""]  # å¯¹åº” #, ##, ###, ####

        for line in lines:
            line_stripped = line.strip()

            # æ£€æµ‹å„çº§æ ‡é¢˜å¹¶æ›´æ–°çŠ¶æ€
            if line_stripped.startswith('#### ') and not line_stripped.startswith('#####'):
                current_headers[3] = line_stripped
            elif line_stripped.startswith('### ') and not line_stripped.startswith('####'):
                current_headers[2] = line_stripped
                current_headers[3] = ""  # æ¸…ç©ºæ›´ä½çº§åˆ«çš„æ ‡é¢˜çŠ¶æ€
            elif line_stripped.startswith('## ') and not line_stripped.startswith('###'):
                current_headers[1] = line_stripped
                current_headers[2] = ""  # æ¸…ç©ºæ›´ä½çº§åˆ«çš„æ ‡é¢˜çŠ¶æ€
                current_headers[3] = ""
            elif line_stripped.startswith('# ') and not line_stripped.startswith('##'):
                current_headers[0] = line_stripped
                current_headers[1] = ""  # æ¸…ç©ºæ›´ä½çº§åˆ«çš„æ ‡é¢˜çŠ¶æ€
                current_headers[2] = ""
                current_headers[3] = ""

        # æ„å»ºå½“å‰æœ‰æ•ˆçš„æ ‡é¢˜å±‚çº§ï¼ˆæŒ‰ç…§å±‚çº§é¡ºåºï¼‰
        for header in current_headers:
            if header:
                header_context.append(header)

        return header_context

    def _extract_main_title(self, markdown_content: str) -> str:
        """ğŸ”¥ æ–°å¢ï¼šæå–æ–‡æ¡£ä¸»æ ‡é¢˜ï¼ˆç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜ï¼‰"""
        lines = markdown_content.split('\n')

        for line in lines:
            line_stripped = line.strip()
            # æ£€æµ‹ä¸€çº§æ ‡é¢˜
            if line_stripped.startswith('# ') and not line_stripped.startswith('## '):
                title = line_stripped[2:].strip()
                logger.info(f"ğŸ“‹ æå–åˆ°æ–‡æ¡£ä¸»æ ‡é¢˜: {title}")
                return title
            # ä¹Ÿæ”¯æŒä½¿ç”¨ = ç¬¦å·çš„ä¸€çº§æ ‡é¢˜
            elif line_stripped and len(line_stripped) > 0:
                # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯ === åˆ†éš”ç¬¦
                next_line_idx = lines.index(line) + 1
                if (next_line_idx < len(lines) and
                        lines[next_line_idx].strip().startswith('==')):
                    title = line_stripped
                    logger.info(f"ğŸ“‹ æå–åˆ°æ–‡æ¡£ä¸»æ ‡é¢˜ï¼ˆ=æ ¼å¼ï¼‰: {title}")
                    return title

        logger.error("æœªè§£æå‡ºä¸»æ ‡é¢˜ï¼Œåœ¨config.jsoné‡Œé¢æ·»åŠ document_title")
        raise ValueError("æœªè§£æå‡ºä¸»æ ‡é¢˜ï¼Œåœ¨config.jsoné‡Œé¢æ·»åŠ document_title")

    def _split_text_with_header_context(self, doc: Document) -> List[Document]:
        """ğŸ”¥ æ–°å¢ï¼šåˆ‡åˆ†æ–‡æœ¬æ—¶ä¿æŒæ ‡é¢˜ä¸Šä¸‹æ–‡"""
        content = doc.page_content

        # æå–å®Œæ•´çš„æ ‡é¢˜ä¸Šä¸‹æ–‡
        header_context = doc.metadata.get('header_hierarchy', [])
        if not header_context:
            header_context = self._extract_header_context(content)

        # ä½¿ç”¨RecursiveCharacterTextSplitteråˆ‡åˆ†
        sub_docs = self.text_splitter.split_documents([doc])

        # ä¸ºæ¯ä¸ªå­chunkè¡¥å……æ ‡é¢˜ä¸Šä¸‹æ–‡
        enhanced_sub_docs = []
        for i, sub_doc in enumerate(sub_docs):
            # æ£€æŸ¥å­chunkæ˜¯å¦å·²åŒ…å«æ ‡é¢˜
            sub_content = sub_doc.page_content.strip()

            # ğŸ”¥ æ–°å¢ï¼šæ„å»ºå®Œæ•´çš„æ ‡é¢˜ä¸Šä¸‹æ–‡
            full_header_context = self._build_full_header_context(
                sub_content, header_context)

            # å¦‚æœæœ‰å®Œæ•´æ ‡é¢˜ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°chunkå¼€å¤´
            if full_header_context:
                enhanced_content = f"{full_header_context}\n\n{sub_content}"
                enhanced_metadata = {
                    **sub_doc.metadata,
                    'has_full_header_context': True,
                    'document_title': self.document_title,
                    'section_header': '\n'.join(header_context) if header_context else "",
                    'sub_chunk_index': i
                }

                # æ·»åŠ Headerå±‚çº§å­—æ®µ
                self._add_header_fields(enhanced_metadata, header_context)
            else:
                enhanced_content = sub_content
                enhanced_metadata = {
                    **sub_doc.metadata,
                    'has_full_header_context': False,
                    'sub_chunk_index': i
                }

                # å³ä½¿æ²¡æœ‰å®Œæ•´æ ‡é¢˜ä¸Šä¸‹æ–‡ï¼Œä¹Ÿå°è¯•æ·»åŠ Headerå±‚çº§å­—æ®µ
                self._add_header_fields(enhanced_metadata, header_context)

            enhanced_doc = Document(
                page_content=enhanced_content,
                metadata=enhanced_metadata
            )
            enhanced_sub_docs.append(enhanced_doc)

        logger.debug(
            f"ğŸ”— æ–‡æœ¬åˆ‡åˆ†: {len(sub_docs)}ä¸ªå­chunkï¼Œ{len([d for d in enhanced_sub_docs if d.metadata.get('has_full_header_context')])}ä¸ªè¡¥å……äº†å®Œæ•´æ ‡é¢˜ä¸Šä¸‹æ–‡")

        return enhanced_sub_docs

    def _add_header_fields(self, metadata: dict, header_context: List[str]):
        """æ·»åŠ Headerå±‚çº§å­—æ®µåˆ°metadata"""
        # åˆå§‹åŒ–æ‰€æœ‰Headerå­—æ®µ
        for i in range(1, 5):
            metadata[f'Header {i}'] = ""

        # æ ¹æ®header_contextæŒ‰å±‚çº§é¡ºåºå¡«å……Headerå­—æ®µ
        current_headers = ["", "", "", ""]  # å¯¹åº” #, ##, ###, ####

        # ä»header_contextä¸­æå–å½“å‰æœ‰æ•ˆçš„æ ‡é¢˜
        for header in header_context:
            if header.startswith('# ') and not header.startswith('##'):
                current_headers[0] = header
            elif header.startswith('## ') and not header.startswith('###'):
                current_headers[1] = header
            elif header.startswith('### ') and not header.startswith('####'):
                current_headers[2] = header
            elif header.startswith('#### ') and not header.startswith('#####'):
                current_headers[3] = header

        # æŒ‰å±‚çº§é¡ºåºè®¾ç½®Headerå­—æ®µ
        for i in range(4):
            if current_headers[i]:
                metadata[f'Header {i+1}'] = current_headers[i].strip()

    def _build_full_header_context(self, content: str, section_headers: List[str]) -> str:
        """ğŸ”¥ æ–°å¢ï¼šæ„å»ºå®Œæ•´çš„æ ‡é¢˜ä¸Šä¸‹æ–‡ï¼ˆæ–‡æ¡£æ ‡é¢˜+å®Œæ•´çš„å±‚çº§æ ‡é¢˜ï¼‰"""
        context_parts = []

        # æ·»åŠ æ–‡æ¡£ä¸»æ ‡é¢˜ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        if self.document_title and not any(self.document_title in line for line in content.split('\n') if line.strip().startswith('#')):
            context_parts.append(f"# {self.document_title}")

        # æ·»åŠ å±‚çº§æ ‡é¢˜ï¼ˆå¦‚æœå†…å®¹ä¸­è¿˜æ²¡æœ‰ï¼‰
        # å…ˆè§£æå½“å‰å†…å®¹ä¸­çš„æ ‡é¢˜å±‚çº§
        existing_levels = {}
        content_lines = content.split('\n')
        for line in content_lines:
            line_stripped = line.strip()
            if line_stripped.startswith('#'):
                # è®¡ç®—æ ‡é¢˜çº§åˆ«
                level = line_stripped.split(' ')[0]
                level_num = level.count('#')
                existing_levels[level_num] = line_stripped

        # æŒ‰ç…§æ ‡é¢˜å±‚çº§é¡ºåºæ·»åŠ ç¼ºå¤±çš„æ ‡é¢˜
        current_headers = ["", "", "", ""]  # å¯¹åº” #, ##, ###, ####

        # ä»section_headersä¸­æå–å½“å‰æœ‰æ•ˆçš„æ ‡é¢˜
        for header in section_headers:
            if header.startswith('# ') and not header.startswith('##'):
                current_headers[0] = header
            elif header.startswith('## ') and not header.startswith('###'):
                current_headers[1] = header
            elif header.startswith('### ') and not header.startswith('####'):
                current_headers[2] = header
            elif header.startswith('#### ') and not header.startswith('#####'):
                current_headers[3] = header

        # æŒ‰å±‚çº§é¡ºåºæ·»åŠ ç¼ºå¤±çš„æ ‡é¢˜
        for i in range(4):
            header = current_headers[i]
            if header and (i+1) not in existing_levels:
                # ç‰¹æ®Šå¤„ç†ï¼šä¸æ·»åŠ æ–‡æ¡£ä¸»æ ‡é¢˜ï¼ˆå·²ç»å•ç‹¬å¤„ç†è¿‡äº†ï¼‰
                header_stripped = header.strip()
                if not (header_stripped.startswith('# ') and self.document_title and self.document_title in header_stripped):
                    context_parts.append(header_stripped)
                    existing_levels[i+1] = header_stripped  # æ·»åŠ åˆ°å·²å­˜åœ¨çº§åˆ«ä¸­ï¼Œé˜²æ­¢é‡å¤

        return '\n'.join(context_parts) if context_parts else ""

    def further_split_large_chunks(self, docs: List[Document]) -> List[Document]:
        """è¿›ä¸€æ­¥åˆ‡åˆ†è¿‡å¤§çš„å—ï¼Œå¯¹è¡¨æ ¼è¿›è¡Œç‰¹æ®Šå¤„ç†"""
        result_docs = []

        for doc in docs:
            # åœ¨metadataä¸­æ·»åŠ token_lengthå­—æ®µï¼Œé¿å…åç»­é‡å¤è®¡ç®—
            if 'token_length' not in doc.metadata:
                doc.metadata['token_length'] = get_token_length(
                    doc.page_content)

            if doc.metadata['token_length'] > self.max_chunk_size:
                content_type = doc.metadata.get('content_type', 'text')

                if content_type == 'table':
                    # ğŸ”¥ è¡¨æ ¼ç±»å‹ï¼šç‰¹æ®Šå¤„ç†ï¼Œç¡®ä¿æ¯ä¸ªå­chunkéƒ½åŒ…å«æ ‡é¢˜è¡Œ
                    sub_docs = self._split_large_table(doc)
                    result_docs.extend(sub_docs)
                else:
                    # æ™®é€šæ–‡æœ¬ï¼šä½¿ç”¨é»˜è®¤åˆ‡åˆ†å™¨
                    sub_docs = self._split_text_with_header_context(doc)
                    result_docs.extend(sub_docs)
            else:
                # ğŸ”¥ å¯¹äºä¸éœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†çš„å°å—ï¼Œä¹Ÿè¦æ£€æŸ¥å¹¶æ·»åŠ æ–‡æ¡£ä¸»æ ‡é¢˜
                enhanced_doc = self._ensure_document_title_context(doc)
                result_docs.append(enhanced_doc)

        return result_docs

    def _ensure_document_title_context(self, doc: Document) -> Document:
        """ğŸ”¥ æ–°å¢ï¼šç¡®ä¿æ–‡æ¡£å—åŒ…å«æ–‡æ¡£ä¸»æ ‡é¢˜ä¸Šä¸‹æ–‡"""
        content = doc.page_content.strip()

        # æ£€æŸ¥å†…å®¹æ˜¯å¦å·²åŒ…å«æ–‡æ¡£ä¸»æ ‡é¢˜
        has_main_title = False
        if self.document_title:
            content_lines = content.split('\n')
            for line in content_lines:
                if line.strip().startswith('# ') and self.document_title in line:
                    has_main_title = True
                    break

        # å¦‚æœæ²¡æœ‰æ–‡æ¡£ä¸»æ ‡é¢˜ä¸”å­˜åœ¨æ–‡æ¡£ä¸»æ ‡é¢˜ï¼Œåˆ™æ·»åŠ 
        if self.document_title and not has_main_title:
            # æå–å½“å‰å†…å®¹çš„æ ‡é¢˜ä¸Šä¸‹æ–‡
            header_context = doc.metadata.get('header_hierarchy', [])
            if not header_context:
                header_context = self._extract_header_context(content)

            # æ„å»ºå®Œæ•´çš„æ ‡é¢˜ä¸Šä¸‹æ–‡
            full_header_context = self._build_full_header_context(
                content, header_context)

            if full_header_context:
                enhanced_content = f"{full_header_context}\n\n{content}"
            else:
                enhanced_content = f"# {self.document_title}\n\n{content}"

            enhanced_metadata = {
                **doc.metadata,
                'has_full_header_context': True,
                'document_title': self.document_title
            }

            # æ·»åŠ Headerå±‚çº§å­—æ®µ
            self._add_header_fields(enhanced_metadata, header_context)

            logger.debug(f"ğŸ“‹ ä¸ºå°å—æ·»åŠ æ–‡æ¡£ä¸»æ ‡é¢˜: {self.document_title}")

            return Document(
                page_content=enhanced_content,
                metadata=enhanced_metadata
            )
        else:
            # ç¡®ä¿metadataä¸­åŒ…å«document_titleä¿¡æ¯
            enhanced_metadata = {
                **doc.metadata,
                'document_title': self.document_title
            }
            if self.document_title and has_main_title:
                enhanced_metadata['has_full_header_context'] = True

            # æ·»åŠ Headerå±‚çº§å­—æ®µ
            header_context = doc.metadata.get('header_hierarchy', [])
            if not header_context:
                header_context = self._extract_header_context(doc.page_content)
            self._add_header_fields(enhanced_metadata, header_context)

            return Document(
                page_content=content,
                metadata=enhanced_metadata
            )

    def _split_large_table(self, doc: Document) -> List[Document]:
        """
        ğŸ”¥ æ–°å¢ï¼šåˆ‡åˆ†å¤§è¡¨æ ¼ï¼Œç¡®ä¿æ¯ä¸ªå­chunkéƒ½åŒ…å«æ ‡é¢˜è¡Œ
        """
        content = doc.page_content
        lines = content.split('\n')

        # è¯†åˆ«è¡¨æ ¼ç»“æ„
        table_lines = []
        header_lines = []
        separator_line = None
        data_lines = []

        for i, line in enumerate(lines):
            line_stripped = line.strip()
            if not line_stripped:
                continue

            if line_stripped.startswith('|') and line_stripped.endswith('|'):
                table_lines.append(line)

                # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†éš”è¡Œ (|---|---|)
                if re.match(r'^\|\s*:?-+:?\s*(\|\s*:?-+:?\s*)*\|$', line_stripped):
                    separator_line = line

                    # ğŸ”¥ ä¿®å¤ï¼šå¤„ç†åˆ†éš”è¡Œåœ¨ç¬¬ä¸€è¡Œçš„æƒ…å†µ
                    if i == 0 or len(table_lines) == 1:
                        # åˆ†éš”è¡Œåœ¨ç¬¬ä¸€è¡Œï¼Œä¸‹ä¸€è¡Œåº”è¯¥æ˜¯æ ‡é¢˜è¡Œ
                        header_lines = []  # æš‚æ—¶ä¸ºç©ºï¼Œåé¢ä»æ•°æ®è¡Œä¸­æå–
                    else:
                        # æ ‡å‡†æ ¼å¼ï¼šåˆ†éš”è¡Œä¹‹å‰çš„éƒ½æ˜¯æ ‡é¢˜è¡Œ
                        header_lines = table_lines[:-1]  # ä¸åŒ…æ‹¬åˆ†éš”è¡Œæœ¬èº«

                elif separator_line is None:
                    # è¿˜æ²¡é‡åˆ°åˆ†éš”è¡Œï¼Œå½“å‰è¡Œå¯èƒ½æ˜¯æ ‡é¢˜è¡Œ
                    continue
                else:
                    # åˆ†éš”è¡Œä¹‹åçš„è¡Œ
                    data_lines.append(line)

        # å¤„ç†åˆ†éš”è¡Œåœ¨ç¬¬ä¸€è¡Œçš„ç‰¹æ®Šæƒ…å†µ
        if separator_line and not header_lines and data_lines:
            # å¦‚æœåˆ†éš”è¡Œåœ¨ç¬¬ä¸€è¡Œä¸”æ²¡æœ‰æ ‡é¢˜è¡Œï¼Œå°†ç¬¬ä¸€ä¸ªæ•°æ®è¡Œä½œä¸ºæ ‡é¢˜è¡Œ
            if data_lines:
                header_lines = [data_lines[0]]
                data_lines = data_lines[1:]
                logger.info(f"æ£€æµ‹åˆ°åˆ†éš”è¡Œåœ¨ç¬¬ä¸€è¡Œï¼Œå°† '{header_lines[0].strip()}' ä½œä¸ºæ ‡é¢˜è¡Œ")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†éš”è¡Œï¼Œå°†ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜è¡Œ
        if separator_line is None and table_lines:
            header_lines = [table_lines[0]]
            # ç”Ÿæˆåˆ†éš”è¡Œ
            columns = table_lines[0].split('|')[1:-1]  # å»æ‰é¦–å°¾ç©ºå…ƒç´ 
            separator_line = '|' + '|'.join(['---' for _ in columns]) + '|'
            data_lines = table_lines[1:]

        if not header_lines or not data_lines:
            # å¦‚æœæ— æ³•è§£æè¡¨æ ¼ç»“æ„ï¼Œå°†ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜è¡Œ
            logger.warning("æ— æ³•è§£æè¡¨æ ¼ç»“æ„ï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬åˆ‡åˆ†")
            logger.info(f"åŸå§‹æ–‡æœ¬:\n{content}")
            return self.text_splitter.split_documents([doc])

        # è®¡ç®—æ ‡é¢˜éƒ¨åˆ†çš„å¤§å°ï¼ˆä½¿ç”¨tokené•¿åº¦ï¼‰
        header_content = '\n'.join(header_lines + [separator_line])
        header_size = get_token_length(header_content)

        # è®¡ç®—æ¯ä¸ªå­chunkå¯ä»¥å®¹çº³å¤šå°‘æ•°æ®è¡Œ
        available_size = self.max_chunk_size - header_size - 20  # é¢„ç•™20 tokenç¼“å†²

        if available_size < 50:
            # å¦‚æœæ ‡é¢˜å¤ªé•¿ï¼Œæ— æ³•åˆç†åˆ‡åˆ†ï¼Œä½¿ç”¨é»˜è®¤åˆ‡åˆ†
            logger.warning(f"è¡¨æ ¼æ ‡é¢˜è¿‡é•¿({header_size} tokens)ï¼Œæ— æ³•åˆç†åˆ‡åˆ†")
            return self.text_splitter.split_documents([doc])

        # æŒ‰ç…§å¯ç”¨ç©ºé—´åˆ‡åˆ†æ•°æ®è¡Œ
        result_docs = []
        chunk_data_lines = []
        current_size = 0

        for data_line in data_lines:
            line_size = get_token_length(data_line) + 1  # +1 for newline

            if current_size + line_size > available_size and chunk_data_lines:
                # å½“å‰chunkå·²æ»¡ï¼Œåˆ›å»ºæ–°çš„chunk
                chunk_content = '# '+self.document_title + '\n' + header_content + \
                    '\n' + '\n'.join(chunk_data_lines)

                chunk_doc = Document(
                    page_content=chunk_content,
                    metadata={
                        **doc.metadata,
                        'is_table_chunk': True,
                        'has_header': True,
                        'chunk_data_rows': len(chunk_data_lines),
                        # æ·»åŠ token_lengthåˆ°metadata
                        'token_length': get_token_length(chunk_content)
                    }
                )
                result_docs.append(chunk_doc)

                # é‡ç½®å½“å‰chunk
                chunk_data_lines = []
                current_size = 0

            chunk_data_lines.append(data_line)
            current_size += line_size

        # å¤„ç†æœ€åä¸€ä¸ªchunk
        if chunk_data_lines:
            chunk_content = '# ' + self.document_title + '\n' + \
                header_content + '\n' + '\n'.join(chunk_data_lines)
            # è®¡ç®—chunkçš„tokené•¿åº¦å¹¶æ·»åŠ åˆ°metadata
            chunk_token_length = get_token_length(chunk_content)
            chunk_doc = Document(
                page_content=chunk_content,
                metadata={
                    **doc.metadata,
                    'is_table_chunk': True,
                    'has_header': True,
                    'chunk_data_rows': len(chunk_data_lines),
                    'token_length': chunk_token_length  # æ·»åŠ token_lengthåˆ°metadata
                }
            )
            result_docs.append(chunk_doc)

        logger.info(f"âœ… è¡¨æ ¼åˆ‡åˆ†å®Œæˆ: {len(result_docs)}ä¸ªchunkï¼Œæ¯ä¸ªchunkåŒ…å«å®Œæ•´æ ‡é¢˜è¡Œ")

        return result_docs

    def process_document(self, markdown_content: str) -> List[Document]:
        """å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹"""
        logger.info("å¼€å§‹æ–‡æ¡£åˆ†å—å¤„ç†")

        # 1. æŒ‰æ ‡é¢˜åˆ†å‰²
        docs = self.split_markdown_by_headers(markdown_content)
        logger.info(f"æ ‡é¢˜åˆ†å‰²åå¾—åˆ° {len(docs)} ä¸ªæ–‡æ¡£å—")

        # 2. åŒºåˆ†è¡¨æ ¼å’Œæ–‡æœ¬
        docs = self.extract_markdown_table_and_text(docs)
        logger.info(f"è¡¨æ ¼æ–‡æœ¬åˆ†ç¦»åå¾—åˆ° {len(docs)} ä¸ªæ–‡æ¡£å—")

        # 3. è¿›ä¸€æ­¥åˆ†å‰²å¤§å—
        docs = self.further_split_large_chunks(docs)
        logger.info(f"æœ€ç»ˆå¾—åˆ° {len(docs)} ä¸ªæ–‡æ¡£å—")

        # 4. æ·»åŠ ç´¢å¼•ä¿¡æ¯å’Œtokené•¿åº¦
        for i, doc in enumerate(docs):
            doc.metadata['chunk_id'] = i
            doc.metadata['total_chunks'] = len(docs)
            # ç¡®ä¿æ¯ä¸ªchunkéƒ½æœ‰token_lengthå­—æ®µ
            if 'token_length' not in doc.metadata:
                doc.metadata['token_length'] = get_token_length(
                    doc.page_content)

        return docs
