# coding:utf-8
"""
å›¾è°±å¢å¼ºæ¨¡å— - é€šè¿‡ESå‘é‡æ£€ç´¢å¢å¼ºçŸ¥è¯†å›¾è°±
"""
import logging
import json
import time
import re
from typing import List, Dict, Any, Optional, Tuple
from elasticsearch import Elasticsearch
from openai import OpenAI
from utils import OpenAIAPIManager

logger = logging.getLogger(__name__)


class GraphEnhancer:
    """å›¾è°±å¢å¼ºå™¨ - é€šè¿‡ESå‘é‡æ£€ç´¢å¢å¼ºå·²æœ‰çŸ¥è¯†å›¾è°±"""

    def __init__(self, config: Dict[str, Any], api_manager: Optional[OpenAIAPIManager] = None):
        """
        åˆå§‹åŒ–å›¾è°±å¢å¼ºå™¨

        Args:
            config: å›¾è°±å¢å¼ºé…ç½®
            api_manager: APIç®¡ç†å™¨å®ä¾‹
        """
        self.config = config
        self.api_manager = api_manager

        # ESé…ç½®
        es_config = config.get('elasticsearch', {})
        self.es_hosts = es_config.get(
            'hosts', ["https://121.43.228.183:19200"])
        self.es_auth = es_config.get(
            'auth', ('elastic', '1E79E697-7AFB-4018-9016-1775AD245B1F'))
        self.index_name = es_config.get(
            'index_name', 'test_ttp_embedding_index')

        # å¢å¼ºé…ç½®
        self.top_k = config.get('top_k', 3)
        self.similarity_threshold = config.get('similarity_threshold', 0.7)
        self.max_enhance_per_procedure = config.get(
            'max_enhance_per_procedure', 2)
        self.enable_deduplication = config.get('enable_deduplication', True)
        self.embedding_model = config.get(
            'embedding_model', 'text-embedding-v2')

        # åˆå§‹åŒ–ESå®¢æˆ·ç«¯
        self.es_client = None
        self._init_es_client()

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆç”¨äºç”Ÿæˆembeddingï¼‰
        self.openai_client = OpenAI(
            api_key="sk-d8626ac601d843d1800a0e349f7c3c8b",
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

        logger.info(f"ğŸ”§ å›¾è°±å¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ESç´¢å¼•: {self.index_name}")
        logger.info(f"   TopK: {self.top_k}")
        logger.info(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")

    def _init_es_client(self):
        """åˆå§‹åŒ–ESå®¢æˆ·ç«¯"""
        try:
            self.es_client = Elasticsearch(
                hosts=self.es_hosts,
                basic_auth=self.es_auth,
                verify_certs=False,
                ssl_show_warn=False,
                request_timeout=30,
                max_retries=3,
                retry_on_timeout=True,
            )

            if self.es_client.ping():
                logger.info("âœ… Elasticsearch è¿æ¥æˆåŠŸ")
            else:
                logger.error("âŒ Elasticsearch è¿æ¥å¤±è´¥")
                self.es_client = None

        except Exception as e:
            logger.error(f"âŒ è¿æ¥ Elasticsearch æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            self.es_client = None

    def get_embedding(self, text: str) -> Optional[List[float]]:
        """ç”Ÿæˆæ–‡æœ¬å‘é‡ï¼ˆç®€å•é™æµç‰ˆï¼‰"""
        max_retries = 2
        base_wait_time = 1200  # åŸºç¡€ç­‰å¾…æ—¶é—´ï¼š20min

        for attempt in range(max_retries + 1):
            try:
                # ğŸ”¥ ç®€å•ç²—æš´ï¼šæ¯æ¬¡è°ƒç”¨å‰ç­‰å¾…è¶³å¤Ÿé•¿æ—¶é—´
                if attempt > 0:  # é‡è¯•æ—¶ç­‰å¾…æ›´ä¹…
                    wait_time = base_wait_time * (attempt + 1)
                    logger.warning(f"â³ ç¬¬{attempt}æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait_time} ç§’...")
                    time.sleep(wait_time)

                logger.info(
                    f"ğŸ” ç”Ÿæˆembedding (å°è¯• {attempt + 1}/{max_retries + 1})")

                response = self.openai_client.embeddings.create(
                    input=text,
                    model=self.embedding_model
                )

                if response.data and len(response.data) > 0:
                    embedding = response.data[0].embedding
                    logger.info(f"âœ… Embeddingç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
                    return embedding
                else:
                    raise ValueError("APIè¿”å›ç©ºembedding")

            except Exception as e:
                error_msg = str(e)

                # æ£€æŸ¥æ˜¯å¦ä¸ºé™æµé”™è¯¯
                if any(indicator in error_msg.lower() for indicator in
                       ['irc-001', 'èµ„æºé™åˆ¶ç­–ç•¥', '10æ¬¡/60.0åˆ†é’Ÿ', 'rate limit']):
                    if attempt < max_retries:
                        logger.warning(f"âš ï¸  è§¦å‘APIé™æµ: {error_msg}")
                        continue  # ç»§ç»­é‡è¯•
                    else:
                        logger.error(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒembeddingç”Ÿæˆ")
                        break
                else:
                    logger.error(f"âŒ ç”Ÿæˆembeddingå¤±è´¥: {error_msg}")
                    if attempt < max_retries:
                        time.sleep(30)  # éé™æµé”™è¯¯ï¼ŒçŸ­æš‚ç­‰å¾…
                        continue
                    else:
                        break

        logger.error("ğŸ’¥ Embeddingç”Ÿæˆæœ€ç»ˆå¤±è´¥")

    def enhance_knowledge_graph(self, kg_data: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        å¢å¼ºçŸ¥è¯†å›¾è°±

        Args:
            kg_data: åŸå§‹çŸ¥è¯†å›¾è°±æ•°æ® (06_knowledge_graph_simple.jsonæ ¼å¼)

        Returns:
            (å¢å¼ºåçš„çŸ¥è¯†å›¾è°±, å¢å¼ºç»Ÿè®¡ä¿¡æ¯)
        """
        logger.info("ğŸš€ å¼€å§‹å›¾è°±å¢å¼º...")

        if not self.es_client:
            logger.error("âŒ ESå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å›¾è°±å¢å¼º")
            return kg_data, {"status": "failed", "reason": "ESå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"}

        # ğŸ”¥ æ–°å¢ï¼šå…¨å±€çº§åˆ«è·Ÿè¸ªè¢«å¢å¼ºçš„procedure
        global_enhanced_procedures = set()

        start_time = time.time()
        enhancement_stats = {
            "original_graph_count": len(kg_data),
            "procedures_found": 0,
            "procedures_enhanced": 0,  # ğŸ”¥ é‡å‘½åï¼šå®é™…è¢«å¢å¼ºçš„procedureæ•°é‡
            "es_queries": 0,
            "matched_results": 0,
            "new_entities_added": 0,
            "new_relationships_added": 0,
            "processing_time": 0
        }

        enhanced_kg_data = []

        # å¤„ç†æ¯ä¸ªçŸ¥è¯†å›¾è°±æ•°æ®
        for graph_idx, graph_data in enumerate(kg_data):
            logger.info(f"ğŸ“ å¤„ç†ç¬¬{graph_idx + 1}/{len(kg_data)}ä¸ªçŸ¥è¯†å›¾è°±çš„å¢å¼º")

            enhanced_graph = self._enhance_single_knowledge(
                graph_data, enhancement_stats, global_enhanced_procedures)
            enhanced_kg_data.append(enhanced_graph)

        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨å…¨å±€å»é‡åçš„æ•°é‡
        enhancement_stats["procedures_enhanced"] = len(
            global_enhanced_procedures)
        enhancement_stats["processing_time"] = time.time() - start_time
        logger.info("âœ… å›¾è°±å¢å¼ºå®Œæˆ!")
        logger.info(
            f"   - å¤„ç†çŸ¥è¯†å›¾è°±: {enhancement_stats['original_graph_count']}")
        logger.info(
            f"   - å‘ç°procedures: {enhancement_stats['procedures_found']}")
        logger.info(
            f"   - è¢«å¢å¼ºprocedures: {enhancement_stats['procedures_enhanced']}")  # ğŸ”¥ ä¿®å¤æ—¥å¿—
        logger.info(f"   - ESæŸ¥è¯¢æ¬¡æ•°: {enhancement_stats['es_queries']}")
        logger.info(f"   - åŒ¹é…ç»“æœ: {enhancement_stats['matched_results']}")
        logger.info(f"   - æ–°å¢å®ä½“: {enhancement_stats['new_entities_added']}")
        logger.info(
            f"   - æ–°å¢å…³ç³»: {enhancement_stats['new_relationships_added']}")
        logger.info(f"   - å¤„ç†è€—æ—¶: {enhancement_stats['processing_time']:.2f}ç§’")
        return enhanced_kg_data, enhancement_stats

    def _enhance_single_knowledge(self, chunk_data: Dict[str, Any], stats: Dict[str, Any],
                                  global_enhanced_procedures: set) -> Dict[str, Any]:
        """å¢å¼ºå•ä¸ªçŸ¥è¯†å›¾è°±çš„å›¾è°±æ•°æ®"""
        try:
            # æ·±æ‹·è´åŸå§‹æ•°æ®
            enhanced_chunk = json.loads(json.dumps(chunk_data))

            nodes = enhanced_chunk.get("nodes", {})

            # ğŸ”¥ æ–°å¢ï¼šè®°å½•è¢«å¢å¼ºçš„åŸå§‹å®ä½“
            current_enhanced_procedures = set()

            procedure_entities = []
            for node_key, node_count in nodes.items():
                try:
                    # è§£æJSONæ ¼å¼çš„èŠ‚ç‚¹é”®
                    node_info = json.loads(node_key)
                    entity_type = node_info.get("entity_type", "")

                    # æŸ¥æ‰¾Proceduresç±»å‹çš„å®ä½“
                    if entity_type.lower() in ["procedures", "procedure"]:
                        procedure_entities.append(
                            (node_key, node_info, node_count))
                        stats["procedures_found"] += 1

                except json.JSONDecodeError:
                    # å¦‚æœä¸èƒ½è§£æä¸ºJSONï¼Œè·³è¿‡
                    logger.debug(f"   è·³è¿‡æ— æ³•è§£æçš„èŠ‚ç‚¹é”®: {node_key}")
                    continue

            if not procedure_entities:
                return enhanced_chunk

            # ä¸ºæ¯ä¸ªprocedureå®ä½“è¿›è¡ŒESæŸ¥è¯¢å’Œå¢å¼º
            for node_key, node_info, node_count in procedure_entities:
                # ğŸ”¥ ä¿®æ”¹ï¼šæ£€æŸ¥æ˜¯å¦æˆåŠŸå¢å¼ºäº†è¯¥procedure
                was_enhanced = self._enhance_procedure_entity(
                    node_key, node_info, node_count, enhanced_chunk, stats, global_enhanced_procedures
                )

                # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœæˆåŠŸå¢å¼ºï¼Œæ·»åŠ åˆ°å…¨å±€é›†åˆï¼ˆè‡ªåŠ¨å»é‡ï¼‰
                if was_enhanced:
                    global_enhanced_procedures.add(node_key)

            return enhanced_chunk

        except Exception as e:
            logger.error(f"âŒ å¢å¼ºå•ä¸ªchunkå¤±è´¥: {e}")
            return chunk_data

    def _enhance_procedure_entity(self, node_key: str, node_info: Dict[str, Any], node_count: int,
                                  enhanced_chunk: Dict[str, Any], stats: Dict[str, Any],
                                  global_enhanced_procedures: set) -> bool:
        """ä¸ºå•ä¸ªprocedureå®ä½“è¿›è¡Œå¢å¼º"""
        try:
            proc_name = node_info.get("label", "")
            proc_desc = node_info.get("description", "") or proc_name
            query_text = proc_desc if proc_desc and len(
                proc_desc) > 10 else proc_name

            if not query_text or len(query_text.strip()) < 5:
                return False

            similar_ttps = self._query_similar_ttps(query_text)
            stats["es_queries"] += 1

            if not similar_ttps:
                return False

            stats["matched_results"] += len(similar_ttps)

            # æ·»åŠ ç›¸ä¼¼TTPåˆ°å›¾è°±
            actually_enhanced = self._add_similar_ttps_to_graph(
                node_key, node_info, similar_ttps, enhanced_chunk, stats, global_enhanced_procedures
            )

            return actually_enhanced  # è¿”å›æ˜¯å¦çœŸæ­£è¿›è¡Œäº†å¢å¼º

        except Exception as e:
            logger.error(f"âŒ å¢å¼ºprocedureå®ä½“å¤±è´¥: {e}")
            return False

    def _add_similar_ttps_to_graph(self, original_node_key: str, original_node_info: Dict[str, Any],
                                   similar_ttps: List[Dict[str, Any]],
                                   enhanced_chunk: Dict[str, Any], stats: Dict[str, Any],
                                   enhanced_original_entities: set):
        """å°†ç›¸ä¼¼çš„TTPæ•°æ®æ·»åŠ åˆ°å›¾è°±ä¸­"""
        try:
            nodes = enhanced_chunk["nodes"]
            edges = enhanced_chunk["edges"]

            # è®°å½•æ˜¯å¦çœŸæ­£æ·»åŠ äº†æ–°çš„å®ä½“æˆ–å…³ç³»
            actually_enhanced = False

            # æ”¶é›†similar proceduresä¿¡æ¯
            similar_procedures_info = []

            for i, ttp in enumerate(similar_ttps):
                procedure_text = ttp["procedure"]
                tactics_text = ttp["tactics"]
                techniques_text = ttp["techniques"]
                similarity_score = ttp["score"]

                # æ”¶é›†similar procedureä¿¡æ¯ç”¨äºä¿å­˜åˆ°åŸå§‹èŠ‚ç‚¹
                if procedure_text and procedure_text.strip():
                    similar_procedures_info.append({
                        "text": procedure_text[:100] + "..." if len(procedure_text) > 100 else procedure_text,
                        "similarity_score": similarity_score,
                        "source": "ES_enhancement"
                    })

                new_node_keys = []

                # ğŸ”¥ å¯¹äºprocedureï¼Œç›´æ¥å¢å¼ºåŸæœ‰å®ä½“ï¼ˆå› ä¸ºåŸèŠ‚ç‚¹è‚¯å®šæ˜¯procedureç±»å‹ï¼‰
                if procedure_text and procedure_text.strip():
                    # å¢å¼ºåŸæœ‰procedureå®ä½“ï¼Œå¢åŠ è®¡æ•°å’Œç›¸ä¼¼ä¿¡æ¯
                    nodes[original_node_key] += 1
                    enhanced_original_entities.add(
                        original_node_key)  # è®°å½•è¢«å¢å¼ºçš„åŸå§‹å®ä½“

                    # å°†åŸprocedureèŠ‚ç‚¹åŠ å…¥å…³ç³»åˆ›å»ºåˆ—è¡¨
                    new_node_keys.append(original_node_key)

                # åˆ›å»ºæ–°çš„Tacticså®ä½“ï¼ˆä¿æŒåŸé€»è¾‘ï¼Œå› ä¸ºtacticså¯èƒ½ç¡®å®ä¸åŒï¼‰
                if tactics_text and tactics_text.strip():
                    pkey = self._generate_simple_pkey("tactics", tactics_text)
                    new_tactic_key = json.dumps({
                        "entity_type": "tactics",
                        "label": tactics_text,
                        "pkey": pkey,
                        "source": "ES_enhancement",
                        "similarity_score": similarity_score,
                        "enhanced_from": original_node_key
                    }, ensure_ascii=False)

                    # å³ä½¿æ˜¯é‡å¤èŠ‚ç‚¹ä¹Ÿè¦æ·»åŠ åˆ°new_node_keysåˆ—è¡¨ä¸­ç”¨äºå…³ç³»åˆ›å»º
                    if not self._is_duplicate_node_key(nodes, new_tactic_key):
                        nodes[new_tactic_key] = 1
                        stats["new_entities_added"] += 1
                        actually_enhanced = True  # æ ‡è®°ä¸ºçœŸæ­£å¢å¼º

                    # æ— è®ºæ˜¯å¦æ˜¯é‡å¤èŠ‚ç‚¹ï¼Œéƒ½è¦æ·»åŠ åˆ°new_node_keysç”¨äºå…³ç³»åˆ›å»º
                    new_node_keys.append(new_tactic_key)

                # åˆ›å»ºæ–°çš„Techniqueså®ä½“ï¼ˆä¿æŒåŸé€»è¾‘ï¼Œå› ä¸ºtechniqueså¯èƒ½ç¡®å®ä¸åŒï¼‰
                if techniques_text and techniques_text.strip():
                    pkey = self._generate_simple_pkey(
                        "techniques", techniques_text)
                    new_technique_key = json.dumps({
                        "entity_type": "techniques",
                        "label": techniques_text,
                        "pkey": pkey,
                        "source": "ES_enhancement",
                        "similarity_score": similarity_score,
                        "enhanced_from": original_node_key
                    }, ensure_ascii=False)

                    # å³ä½¿æ˜¯é‡å¤èŠ‚ç‚¹ä¹Ÿè¦æ·»åŠ åˆ°new_node_keysåˆ—è¡¨ä¸­ç”¨äºå…³ç³»åˆ›å»º
                    if not self._is_duplicate_node_key(nodes, new_technique_key):
                        nodes[new_technique_key] = 1
                        stats["new_entities_added"] += 1
                        actually_enhanced = True  # æ ‡è®°ä¸ºçœŸæ­£å¢å¼º

                    # æ— è®ºæ˜¯å¦æ˜¯é‡å¤èŠ‚ç‚¹ï¼Œéƒ½è¦æ·»åŠ åˆ°new_node_keysç”¨äºå…³ç³»åˆ›å»º
                    new_node_keys.append(new_technique_key)

                # åˆ›å»ºå®ä½“é—´çš„å…³ç³»
                relationships_added = self._create_ttp_relationships(
                    new_node_keys, original_node_key, edges, similarity_score, stats
                )

                # å¦‚æœæ·»åŠ äº†å…³ç³»ï¼Œåˆ™æ ‡è®°ä¸ºçœŸæ­£å¢å¼º
                if relationships_added > 0:
                    actually_enhanced = True

            # æ›´æ–°åŸå§‹èŠ‚ç‚¹ä¿¡æ¯ï¼Œæ·»åŠ similar_procedureså­—æ®µ
            if similar_procedures_info:
                original_node_updated = original_node_info.copy()
                original_node_updated["similar_procedures"] = similar_procedures_info
                updated_node_key = json.dumps(
                    original_node_updated, ensure_ascii=False)

                # æ›´æ–°èŠ‚ç‚¹é”®
                if original_node_key in nodes:
                    node_count = nodes.pop(original_node_key)
                    nodes[updated_node_key] = node_count

            # åªæœ‰çœŸæ­£æ·»åŠ äº†å®ä½“æˆ–å…³ç³»æ‰è¿”å›True
            return actually_enhanced

        except Exception as e:
            logger.error(f"æ·»åŠ ç›¸ä¼¼TTPåˆ°å›¾è°±æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def _generate_simple_pkey(self, entity_type: str, label: str) -> str:
        """ç”Ÿæˆç®€æ´çš„å®ä½“pkey"""
        # æ¸…ç†æ ‡ç­¾ï¼Œä¿ç•™å­—æ¯æ•°å­—å’Œç‚¹å·
        clean_label = re.sub(r'[^\w\.]', '', label.lower())
        return f"{entity_type}--{clean_label}"

    def _is_duplicate_node_key(self, nodes: Dict[str, Any], new_node_key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤èŠ‚ç‚¹"""
        if not self.enable_deduplication:
            return False

        try:
            new_node_info = json.loads(new_node_key)
            new_label = new_node_info.get("label", "").lower().strip()
            new_type = new_node_info.get("entity_type", "").lower()

            for existing_key in nodes.keys():
                try:
                    existing_info = json.loads(existing_key)
                    existing_label = existing_info.get(
                        "label", "").lower().strip()
                    existing_type = existing_info.get(
                        "entity_type", "").lower()

                    if existing_type == new_type and existing_label == new_label:
                        return True
                except json.JSONDecodeError:
                    continue

            return False
        except json.JSONDecodeError:
            return False

    def _create_ttp_relationships(self, new_node_keys: List[str],
                                  original_node_key: str, edges: Dict[str, Any],
                                  similarity_score: float, stats: Dict[str, Any]):
        """åˆ›å»ºTTPå®ä½“é—´çš„å…³ç³»"""
        relationships_added = 0  # è®°å½•æ·»åŠ çš„å…³ç³»æ•°é‡
        try:
            original_node_info = json.loads(original_node_key)
            original_pkey = original_node_info.get("pkey", "")

            # æ”¶é›†æ–°å¢èŠ‚ç‚¹çš„pkeyä¿¡æ¯
            tactics_pkey = None
            techniques_pkey = None

            for new_node_key in new_node_keys:
                try:
                    new_node_info = json.loads(new_node_key)
                    entity_type = new_node_info.get("entity_type", "")

                    if entity_type == "tactics":
                        tactics_pkey = new_node_info.get("pkey", "")
                    elif entity_type == "techniques":
                        techniques_pkey = new_node_info.get("pkey", "")
                except json.JSONDecodeError:
                    continue

            # åªåˆ›å»ºä¸¤ç§å…³ç³»ç±»å‹:
            # 1. tactics HAS techniquesï¼šæˆ˜æœ¯åŒ…å«æŠ€æœ¯
            # 2. techniques LAUNCH procedureï¼šæŠ€æœ¯å¯åŠ¨å…·ä½“çš„è¿‡ç¨‹

            # åˆ›å»º tactics HAS techniques å…³ç³»
            if tactics_pkey and techniques_pkey:
                rel_key = json.dumps({
                    "label": "HAS",
                    "pkey": tactics_pkey,      # tacticsä½œä¸ºä¸»ä½“
                    "skey": techniques_pkey,   # techniquesä½œä¸ºå®¢ä½“
                    "confidence": similarity_score,
                    "source": "ES_enhancement"
                }, ensure_ascii=False)

                if rel_key not in edges:
                    edges[rel_key] = 1
                    stats["new_relationships_added"] += 1
                    relationships_added += 1

            # åˆ›å»º techniques LAUNCH procedure å…³ç³»
            if techniques_pkey:
                rel_key = json.dumps({
                    "label": "LAUNCH",
                    "pkey": techniques_pkey,   # techniquesä½œä¸ºä¸»ä½“
                    "skey": original_pkey,     # procedureä½œä¸ºå®¢ä½“
                    "confidence": similarity_score,
                    "source": "ES_enhancement"
                }, ensure_ascii=False)

                if rel_key not in edges:
                    edges[rel_key] = 1
                    stats["new_relationships_added"] += 1
                    relationships_added += 1

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºTTPå…³ç³»å¤±è´¥: {e}")

        return relationships_added  # è¿”å›æ·»åŠ çš„å…³ç³»æ•°é‡

    def _query_similar_ttps(self, query_text: str) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢ç›¸ä¼¼çš„TTPæ•°æ®"""
        try:
            logger.info(f"ğŸ” å¼€å§‹æŸ¥è¯¢ç›¸ä¼¼TTP: {query_text[:50]}...")

            # ğŸ”¥ é¢„æ£€æŸ¥ï¼šé¿å…æ— æ•ˆè°ƒç”¨
            if not query_text or len(query_text.strip()) < 5:
                logger.warning("âš ï¸  æŸ¥è¯¢æ–‡æœ¬è¿‡çŸ­ï¼Œè·³è¿‡TTPæŸ¥è¯¢")
                return []

            # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¸¦é™æµæ§åˆ¶ï¼‰
            query_embedding = self.get_embedding(query_text)
            if not query_embedding:
                logger.warning("âš ï¸  æ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œè·³è¿‡TTPæŸ¥è¯¢")
                return []

            # ES k-NNæŸ¥è¯¢
            search_body = {
                "knn": {
                    "field": "dense_embedding",
                    "query_vector": query_embedding,
                    "k": self.top_k,
                    "num_candidates": 100
                },
                "fields": ["procedure", "tactics", "techniques"],
                "_source": False
            }

            response = self.es_client.search(
                index=self.index_name,
                body=search_body
            )

            similar_ttps = []
            for hit in response.get("hits", {}).get("hits", []):
                score = hit.get("_score", 0)
                fields = hit.get("fields", {})

                # åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                if score >= self.similarity_threshold:
                    ttp_data = {
                        "score": score,
                        "procedure": fields.get("procedure", [""])[0],
                        "tactics": fields.get("tactics", [""])[0],
                        "techniques": fields.get("techniques", [""])[0]
                    }
                    similar_ttps.append(ttp_data)

            # é™åˆ¶æ¯ä¸ªprocedureçš„å¢å¼ºæ•°é‡
            return similar_ttps[:self.max_enhance_per_procedure]

        except Exception as e:
            logger.error(f"âŒ ESæŸ¥è¯¢å¤±è´¥: {e}")
            return []
