# coding:utf-8
"""
å·¥å…·å‡½æ•°æ¨¡å— - æä¾›å„ç§è¾…åŠ©åŠŸèƒ½
"""
import re
import os
import json
import time
import requests
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from openai import OpenAI
import openai
import threading
from functools import wraps

logger = logging.getLogger(__name__)


def timeout_handler(func):
    """è¶…æ—¶è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        timeout = kwargs.pop('timeout', 90)  # é»˜è®¤60ç§’è¶…æ—¶

        result = [None]
        exception = [None]

        def target():
            try:
                result[0] = func(*args, **kwargs)
            except Exception as e:
                exception[0] = e

        thread = threading.Thread(target=target)
        thread.daemon = True
        thread.start()
        thread.join(timeout)

        if thread.is_alive():
            logger.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)")
            raise TimeoutError(f"APIè°ƒç”¨è¶…æ—¶: {timeout}ç§’")

        if exception[0]:
            raise exception[0]

        return result[0]

    return wrapper


class OpenAIAPIManager:
    """OpenAI APIç®¡ç†å™¨ï¼Œè´Ÿè´£å¤„ç†é‡è¯•ã€è¶…æ—¶ç­‰æœºåˆ¶"""

    def __init__(self, config: Dict[str, Any]):
        # æå–é…ç½®å‚æ•°
        self.api_key = config.get('api_key')
        self.base_url = config.get('base_url')
        self.model = config.get('model', 'qwen-plus')
        self.timeout = config.get('timeout', 90)
        self.max_tokens = config.get('max_tokens', 4000)
        self.max_retries = config.get('max_retries', 3)
        self.temperature = config.get('temperature', 0.7)
        self.top_p = config.get('top_p', 1)
        self.frequency_penalty = config.get('frequency_penalty', 0)
        self.presence_penalty = config.get('presence_penalty', 0)

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

    def fix_json_call_api(self, broken_json: str) -> str:
        """ä½¿ç”¨GPTä¿®å¤æŸåçš„JSONæ ¼å¼"""
        try:
            prompt = """ä½ æ˜¯JSONä¿®å¤ä¸“å®¶ã€‚

    ä»»åŠ¡ï¼šä¿®å¤æŸåçš„JSONï¼Œç¡®ä¿è¯­æ³•å®Œå…¨æ­£ç¡®ã€‚

    è¾“å‡ºæ ¼å¼ï¼š{"entities":[],"relationships":[]}

    å®ä½“æ ¼å¼ï¼š{"labels":"EntityType","id":"entity-id","name":"Entity Name","description":"æè¿°"}
    å…³ç³»æ ¼å¼ï¼š{"type":"RELATION_TYPE","source":"source-id","target":"target-id","confidence":0.95,"evidence":"è¯æ®"}

    æ ¸å¿ƒè¦æ±‚ï¼š
    1. æ‰€æœ‰å±æ€§åå’Œå­—ç¬¦ä¸²å€¼å¿…é¡»ç”¨åŒå¼•å·åŒ…å›´
    2. ç§»é™¤å¤šä½™ç©ºæ ¼å’Œéšè—å­—ç¬¦
    3. ç¡®ä¿æ‹¬å·åŒ¹é…å’Œé€—å·æ­£ç¡®ï¼Œjsonç»“å°¾å¿…é¡»æ˜¯"}]}"
    4. å¿…é¡»åŒ…å«entitieså’Œrelationshipså­—æ®µ
    5. å®ä½“ç»“æ„æ‰å¹³åŒ–ï¼šç›´æ¥åŒ…å«labels,id,name,descriptionå­—æ®µï¼Œä¸ä½¿ç”¨propertiesåµŒå¥—

    ç›´æ¥è¾“å‡ºä¿®å¤åçš„JSONï¼Œæ— å…¶ä»–å†…å®¹ã€‚

    å¾…ä¿®å¤JSONï¼š"""

            messages = [
                {"role": "system", "content": "ä½ æ˜¯JSONè¯­æ³•ä¿®å¤ä¸“å®¶ã€‚ä¸¥æ ¼éµå®ˆJSONè§„èŒƒï¼Œåªè¾“å‡ºä¿®å¤åçš„JSONã€‚ç¡®ä¿å®ä½“ç»“æ„æ‰å¹³åŒ–ï¼Œä¸ä½¿ç”¨propertiesåµŒå¥—ã€‚"},
                {"role": "user", "content": f"{prompt}\n\n{broken_json}"}
            ]

            response = self.call_api(
                messages=messages,
                model=self.model,
                temperature=0.1,
                max_tokens=min(8000, len(broken_json) + 1000)
            )

            if not response:
                logger.error("âŒ GPTè¿”å›ç©ºå“åº”")
                return broken_json

            logger.debug(f"ğŸ“‹ ä¿®å¤åJSONé•¿åº¦: {len(response)}")
            return response

        except Exception as e:
            logger.error(f"âŒ JSONä¿®å¤å¼‚å¸¸: {e}")
            return broken_json

    def _should_retry(self, exception: Exception, retry_count: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        if retry_count >= self.max_retries:
            return False

        # OpenAI APIç‰¹å®šçš„é‡è¯•æ¡ä»¶
        if isinstance(exception, openai.RateLimitError):
            logger.warning(f"é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
            return True

        if isinstance(exception, openai.APITimeoutError):
            logger.warning(f"APIè¶…æ—¶ï¼Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
            return True

        if isinstance(exception, openai.InternalServerError):
            logger.warning(f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
            return True

        if isinstance(exception, openai.APIConnectionError):
            logger.warning(f"è¿æ¥é”™è¯¯ï¼Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
            return True

        # ğŸ”¥ æ–°å¢ï¼šBadRequestErrorçš„ç‰¹å®šå¤„ç†
        if isinstance(exception, openai.BadRequestError):
            error_str = str(exception)
            # å¯¹äºå†…å®¹å®¡æŸ¥å¤±è´¥ï¼Œä¸é‡è¯•ï¼ˆå› ä¸ºé‡è¯•ä¹Ÿä¼šå¤±è´¥ï¼‰
            if 'data_inspection_failed' in error_str:
                logger.warning(f"å†…å®¹å®¡æŸ¥å¤±è´¥ï¼Œè·³è¿‡é‡è¯•: {error_str}")
                return False
            # å¯¹äºå…¶ä»–400é”™è¯¯ï¼Œå¯ä»¥è€ƒè™‘é‡è¯•ï¼ˆå¯èƒ½æ˜¯ä¸´æ—¶é—®é¢˜ï¼‰
            elif retry_count < 2:  # æœ€å¤šé‡è¯•2æ¬¡
                logger.warning(f"BadRequesté”™è¯¯ï¼Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
                return True
            return False

        if isinstance(exception, (requests.exceptions.ConnectionError,
                                  requests.exceptions.Timeout,
                                  requests.exceptions.RequestException)):
            logger.warning(f"ç½‘ç»œé”™è¯¯ï¼Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
            return True

        # å…¶ä»–å¯é‡è¯•çš„å¼‚å¸¸
        if "timeout" in str(exception).lower():
            logger.warning(f"è¶…æ—¶é”™è¯¯ï¼Œç¬¬{retry_count + 1}æ¬¡é‡è¯•...")
            return True

        return False

    def _get_retry_delay(self, retry_count: int) -> float:
        """è·å–é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ï¼‰"""
        if retry_count < len(self.retry_delays):
            return self.retry_delays[retry_count]
        return self.retry_delays[-1]

    @timeout_handler
    def call_api(
        self,
        messages: List[Dict[str, str]],
        response_format: Optional[Dict[str, str]] = None,
        **kwargs
    ) -> str:
        """
        API è°ƒç”¨å‡½æ•°ï¼Œè¿”å›å“åº”å†…å®¹å­—ç¬¦ä¸²ã€‚
        é™¤response_format å¤–ï¼Œå…¶ä½™ LLM è¶…å‚å‡å¤ç”¨ OpenAIAPIManager
        åˆå§‹åŒ–å€¼ï¼ˆmodelã€max_tokensã€timeoutã€max_retriesï¼‰ã€‚
        """
        retry_count = 0
        last_exception = None
        start_time = time.time()

        logger.info(
            f"ğŸš€ å¼€å§‹ API è°ƒç”¨ - æ¨¡å‹: {self.model}, "
            f"æ¶ˆæ¯æ•°: {len(messages)}, è¶…æ—¶: {self.timeout}s"
        )

        while retry_count <= self.max_retries:
            try:
                # ğŸ”¥ ä¿®å¤ï¼šç®€åŒ–å‚æ•°å¤„ç†é€»è¾‘
                api_params = {
                    "model": kwargs.get("model", self.model),
                    "messages": messages,
                    "temperature": kwargs.get("temperature", self.temperature),
                    "max_tokens": kwargs.get("max_tokens", self.max_tokens),
                    "timeout": kwargs.get("timeout", self.timeout),
                }

                if response_format:
                    api_params["response_format"] = response_format

                # æ’é™¤å·²ç»æ˜ç¡®å¤„ç†çš„å‚æ•°
                excluded_keys = {"model", "messages", "temperature",
                                 "max_tokens", "timeout", "response_format"}
                for key, value in kwargs.items():
                    if key not in excluded_keys:
                        api_params[key] = value

                # è®°å½•é‡è¯•ä¿¡æ¯
                if retry_count > 0:
                    logger.info(f"ğŸ”„ ç¬¬{retry_count}æ¬¡é‡è¯•APIè°ƒç”¨...")

                # æ‰§è¡ŒAPIè°ƒç”¨
                call_start_time = time.time()
                response = self.client.chat.completions.create(**api_params)
                call_duration = time.time() - call_start_time

                # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
                if not response or not response.choices:
                    raise APICallError("APIè¿”å›ç©ºå“åº”æˆ–æ— choices")

                # ğŸ”¥ ç¡®ä¿è¿”å›å­—ç¬¦ä¸²ç±»å‹
                content = response.choices[0].message.content
                if content is None:
                    raise APICallError("APIè¿”å›ç©ºå†…å®¹")

                # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²ç±»å‹
                if not isinstance(content, str):
                    content = str(content)

                # è®°å½•æˆåŠŸä¿¡æ¯
                total_duration = time.time() - start_time
                usage = response.usage if hasattr(response, 'usage') else None

                logger.info(f"âœ… APIè°ƒç”¨æˆåŠŸ!")
                logger.info(f"   - æœ¬æ¬¡è°ƒç”¨è€—æ—¶: {call_duration:.2f}ç§’")
                logger.info(f"   - æ€»è€—æ—¶: {total_duration:.2f}ç§’")
                logger.info(f"   - é‡è¯•æ¬¡æ•°: {retry_count}")
                if usage:
                    logger.info(
                        f"   - Tokenä½¿ç”¨: {usage.prompt_tokens}+{usage.completion_tokens}={usage.total_tokens}")

                return content

            except Exception as e:
                last_exception = e

                # è®°å½•é”™è¯¯è¯¦æƒ…
                error_type = type(e).__name__
                error_msg = str(e)
                logger.error(
                    f"âŒ APIè°ƒç”¨å¤±è´¥ (ç¬¬{retry_count + 1}æ¬¡): {error_type}: {error_msg}")

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if not self._should_retry(e, retry_count):
                    logger.error(f"ğŸš« ä¸å¯é‡è¯•çš„é”™è¯¯æˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåœæ­¢é‡è¯•")
                    break

                # ç­‰å¾…åé‡è¯•
                if retry_count < self.max_retries:
                    retry_delay = self._get_retry_delay(retry_count)
                    logger.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    time.sleep(retry_delay)

                retry_count += 1

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        total_duration = time.time() - start_time
        error_msg = f"APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œé‡è¯•{retry_count}æ¬¡ï¼Œæ€»è€—æ—¶{total_duration:.2f}ç§’"
        logger.error(f"ğŸ’¥ {error_msg}")

        if last_exception:
            logger.error(
                f"æœ€åä¸€æ¬¡é”™è¯¯: {type(last_exception).__name__}: {last_exception}")

        raise APICallError(
            message=error_msg,
            status_code=getattr(last_exception, 'status_code', None),
            retry_count=retry_count
        ) from last_exception


class GPTResponseParser:
    """GPTå“åº”è§£æå·¥å…·ç±»"""

    @staticmethod
    def post_process_json(response: str) -> str:
        """åŸºäºè§„åˆ™çš„åå¤„ç†ï¼Œä»GPTå“åº”ä¸­æå–çº¯JSONå†…å®¹"""
        try:
            response = response.strip()

            # ğŸ”¥ æ­¥éª¤1ï¼šç§»é™¤ä»£ç å—æ ‡è®°
            if '```' in response:
                lines = response.split('\n')
                json_content = []
                in_code_block = False

                for line in lines:
                    line_stripped = line.strip()
                    # æ£€æµ‹ä»£ç å—å¼€å§‹/ç»“æŸ
                    if line_stripped.startswith('```'):
                        in_code_block = not in_code_block
                        continue
                    # åœ¨ä»£ç å—å†…æˆ–ä¸åœ¨ä»£ç å—æ—¶æ”¶é›†å†…å®¹
                    if in_code_block or (not in_code_block and line_stripped):
                        json_content.append(line)

                response = '\n'.join(json_content).strip()

            # ğŸ”¥ æ­¥éª¤2ï¼šæŸ¥æ‰¾JSONå¯¹è±¡è¾¹ç•Œï¼ˆæ›´ç²¾ç¡®ï¼‰
            if '{' in response and '}' in response:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ª{
                start = response.find('{')

                # ä»startä½ç½®å¼€å§‹ï¼Œæ­£ç¡®åŒ¹é…å¤§æ‹¬å·
                brace_count = 0
                end_pos = -1

                for i, char in enumerate(response[start:], start):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end_pos = i
                            break

                if end_pos != -1:
                    response = response[start:end_pos + 1]
                else:
                    # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„}ï¼Œå–åˆ°æœ€åä¸€ä¸ª}
                    last_brace = response.rfind('}')
                    if last_brace > start:
                        response = response[start:last_brace + 1]

             # ğŸ”¥ æ­¥éª¤3ï¼šå¢å¼ºæ¸…ç† - ç§»é™¤éšè—å­—ç¬¦å’Œè§„èŒƒåŒ–
            # ç§»é™¤å„ç§éšè—çš„Unicodeå­—ç¬¦
            response = re.sub(
                r'[\u200b\u200c\u200d\ufeff\u00a0\u2028\u2029]', '', response)

            # ç§»é™¤æ¢è¡Œå’Œå¤šä½™ç©ºç™½
            response = re.sub(r'\n\s*', '', response)
            response = re.sub(r'\s+', ' ', response)
            response = response.strip()

            # ğŸ”¥ æ­¥éª¤4ï¼šå¼•å·éªŒè¯å’Œä¿®å¤
            response = GPTResponseParser._fix_quote_issues(response)

            # ğŸ”¥ æ­¥éª¤5ï¼šåŸºç¡€è¯­æ³•æ£€æŸ¥å’Œä¿®å¤
            response = GPTResponseParser._basic_syntax_fix(response)

            return response

        except Exception as e:
            logger.warning(f"âš ï¸ JSONæå–å¤±è´¥: {e}")
            return response

    @staticmethod
    def _fix_quote_issues(json_str: str) -> str:
        """ä¿®å¤å¼•å·ç›¸å…³é—®é¢˜"""
        try:
            # ğŸ”¥ ä¿®å¤å­—ç¬¦ä¸²å†…éƒ¨çš„åµŒå¥—å¼•å·
            # æŸ¥æ‰¾ "text"inner"text" æ¨¡å¼å¹¶ä¿®å¤ä¸º "text inner text"
            json_str = re.sub(
                r'"([^"]*)"([^"{}[\],]*)"([^"]*)"', r'"\1 \2 \3"', json_str)

            # ğŸ”¥ ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å·ï¼ˆåœ¨JSONä¸­å¿…é¡»æ˜¯åŒå¼•å·ï¼‰
            # ä½†è¦å°å¿ƒä¸è¦æ›¿æ¢å­—ç¬¦ä¸²å†…å®¹ä¸­çš„å•å¼•å·
            json_str = re.sub(r"'([^']*)':", r'"\1":', json_str)  # å±æ€§å

            return json_str
        except Exception as e:
            logger.warning(f"âš ï¸ å¼•å·ä¿®å¤å¤±è´¥: {e}")
            return json_str

    @staticmethod
    def _basic_syntax_fix(json_str: str) -> str:
        """åŸºç¡€è¯­æ³•ä¿®å¤"""
        try:
            # ğŸ”¥ ä¿®å¤å¸¸è§çš„è¯­æ³•é—®é¢˜
            # ç§»é™¤å¯¹è±¡/æ•°ç»„æœ«å°¾çš„å¤šä½™é€—å·
            json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)

            # ğŸ”¥ ä¿®å¤ç¼ºå°‘é€—å·çš„é—®é¢˜ - åœ¨}{ æˆ– ]} ä¹‹é—´æ·»åŠ é€—å·
            json_str = re.sub(r'}(\s*)([{\[])', r'},\1\2', json_str)
            json_str = re.sub(r'](\s*)([{\[])', r'],\1\2', json_str)

            # ğŸ”¥ æ£€æŸ¥å¹¶ä¿®å¤æœ«å°¾é—®é¢˜
            if json_str.endswith('}]'):
                # éªŒè¯æ˜¯å¦éœ€è¦ç§»é™¤æœ«å°¾çš„]
                open_braces = json_str.count('{')
                close_braces = json_str.count('}')
                open_brackets = json_str.count('[')
                close_brackets = json_str.count(']')

                if close_brackets > open_brackets:
                    test_json = json_str[:-1]
                    try:
                        import json
                        json.loads(test_json)
                        json_str = test_json
                        logger.debug("ğŸ”§ ç§»é™¤æœ«å°¾å¤šä½™çš„']'")
                    except json.JSONDecodeError:
                        pass

            return json_str
        except Exception as e:
            logger.warning(f"âš ï¸ åŸºç¡€è¯­æ³•ä¿®å¤å¤±è´¥: {e}")
            return json_str

    @staticmethod
    def parse_json_response(response: str, expected_key: str = None, api_manager=None) -> Dict[str, Any]:
        """é€šç”¨çš„GPT JSONå“åº”è§£ææ–¹æ³•"""
        try:
            # ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
            if not isinstance(response, str):
                if isinstance(response, dict):
                    if expected_key and expected_key in response:
                        return {expected_key: response[expected_key]}
                    return response
                response = str(response)

            response = response.strip()
            if not response:
                logger.warning("âš ï¸ æ”¶åˆ°ç©ºå“åº”")
                return {}

            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
            try:
                result = json.loads(response)
                logger.info("âœ… ç›´æ¥JSONè§£ææˆåŠŸ")
                if expected_key and expected_key in result:
                    return {expected_key: result[expected_key]}
                return result
            except json.JSONDecodeError as e:
                logger.info(f"âŒ ç›´æ¥JSONè§£æå¤±è´¥: {e}")
                logger.debug(
                    f"ğŸ“‹ å“åº”å†…å®¹: {response}")

                # ä½¿ç”¨å¤§æ¨¡å‹ä¿®å¤JSONï¼ˆå¦‚æœæä¾›äº†api_managerï¼‰
                if api_manager is not None:
                    logger.info("ğŸ¤– ä½¿ç”¨GPTä¿®å¤JSONæ ¼å¼...")
                    fixed_response = api_manager.fix_json_call_api(
                        response)
                    # éªŒè¯ä¿®å¤ç»“æœ
                    try:
                        result = json.loads(fixed_response)
                        logger.info("âœ… GPTä¿®å¤JSONæˆåŠŸ")
                        if expected_key and expected_key in result:
                            return {expected_key: result[expected_key]}
                        return result
                    except json.JSONDecodeError as repair_error:
                        logger.warning(f"âš ï¸ GPTä¿®å¤åä»è§£æå¤±è´¥: {repair_error}")
                        logger.warning(f"âš ï¸ GPTä¿®å¤åå“åº”å†…å®¹: {fixed_response}")
                        logger.info("å°è¯•åŸºäºè§„åˆ™çš„ä¿®å¤")
                        # ğŸ”¥ æ–°å¢ï¼šåŸºäºè§„åˆ™çš„åå¤„ç†
                        fixed_response = GPTResponseParser.post_process_json(
                            fixed_response)
                        # éªŒè¯ä¿®å¤ç»“æœ
                        try:
                            result = json.loads(fixed_response)
                            logger.info("âœ… GPTä¿®å¤JSONæˆåŠŸ")
                            if expected_key and expected_key in result:
                                return {expected_key: result[expected_key]}
                            return result
                        except json.JSONDecodeError as repair_error:
                            logger.error(f"âŒ æ‰€æœ‰ä¿®å¤æ–¹å¼å‡å¤±è´¥ï¼ŒJSONè§£æå¤±è´¥")
                            return {}
                else:
                    logger.error("âŒ æœªé…ç½®api_managerï¼ŒJSONè§£æå¤±è´¥")
                    return {}

        except Exception as e:
            logger.error(f"âŒ JSONè§£æå‘ç”Ÿå¼‚å¸¸: {e}")
            logger.debug(
                f"ğŸ“‹ å“åº”å†…å®¹: {response}")
            return {}

    @staticmethod
    def parse_knowledge_graph_result(response: str, api_manager=None) -> Dict[str, Any]:
        """è§£æçŸ¥è¯†å›¾è°±æå–ç»“æœ"""

        def get_parsed_data(resp):
            if isinstance(resp, dict):
                return resp
            return GPTResponseParser.parse_json_response(response=str(resp), api_manager=api_manager)

        def validate_entity(entity):
            """éªŒè¯å®ä½“æ ¼å¼ - é€‚é…æ–°çš„labelså­—ç¬¦ä¸²æ ¼å¼"""
            if not isinstance(entity, dict):
                return None

            # ğŸ”¥ æ–°æ ¼å¼ï¼šå±æ€§ç›´æ¥åœ¨å®ä½“å¯¹è±¡ä¸Š
            if 'labels' in entity and 'name' in entity and 'id' in entity:
                return {
                    'name': entity.get('name', ''),
                    'type': entity.get('labels', ''),  # ğŸ”¥ ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                    'id': entity.get('id', ''),
                    'description': entity.get('description', ''),
                    'labels': entity.get('labels', ''),
                }

            # ğŸ”¥ ğŸ”¥ å‘åå…¼å®¹ï¼šæ—§æ ¼å¼ï¼ˆlabelsä¸ºå­—ç¬¦ä¸² + propertiesï¼‰
            if 'labels' in entity and 'properties' in entity:
                props = entity.get('properties', {})
                if 'name' in props and 'id' in props:
                    return {
                        'name': props['name'],
                        'type': entity.get('labels', ''),  # ğŸ”¥ ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                        'id': props['id'],
                        'description': props.get('description', ''),
                        'labels': entity.get('labels', ''),
                        'properties': props
                    }
            return None

        def validate_relationship(rel):
            """éªŒè¯å…³ç³»æ ¼å¼"""
            if not isinstance(rel, dict) or not all(k in rel for k in ['source', 'target', 'type']):
                return None

            # ğŸ”¥ ç®€åŒ–ç½®ä¿¡åº¦å¤„ç†
            confidence = rel.get('confidence', 0.7)
            if isinstance(confidence, (int, float)) and 0 <= confidence <= 1:
                rel['confidence'] = round(float(confidence), 2)
            else:
                rel['confidence'] = 0.7

            return rel

        # ä¸»å¤„ç†é€»è¾‘
        try:
            result = get_parsed_data(response)
            entities = result.get('entities', [])
            relationships = result.get('relationships', [])

            # éªŒè¯å¹¶è¿‡æ»¤æ•°æ®
            valid_entities = [e for e in map(
                validate_entity, entities) if e is not None]
            valid_relationships = [r for r in map(
                validate_relationship, relationships) if r is not None]

            logger.info(
                f"âœ… è§£æå®Œæˆ: å®ä½“ {len(valid_entities)}ä¸ª, å…³ç³» {len(valid_relationships)}ä¸ª")

            return {
                "entities": valid_entities,
                "relationships": valid_relationships
            }

        except Exception as e:
            logger.error(f"âŒ è§£æçŸ¥è¯†å›¾è°±ç»“æœå¤±è´¥: {e}")
            return {"entities": [], "relationships": []}

    @staticmethod
    def parse_merge_groups(response: str, api_manager: OpenAIAPIManager) -> List[List[int]]:
        """è§£æå®ä½“å¯¹é½çš„åˆå¹¶ç»„"""
        try:
            # ğŸ”¥ æ–°å¢ï¼šå¤„ç†å¸¦æœ‰ä»£ç å—æ ‡è¯†ç¬¦çš„å“åº”
            if response.startswith("```"):
                # æå–ä»£ç å—ä¸­çš„JSONå†…å®¹
                import re
                json_match = re.search(
                    r"```(?:json)?\s*({.*?})\s*```", response, re.DOTALL)
                if json_match:
                    response = json_match.group(1)
                else:
                    # å¦‚æœæ— æ³•æå–JSONï¼Œå°è¯•ç›´æ¥å»é™¤ä»£ç å—æ ‡è®°
                    response = re.sub(r"^```(?:json)?\s*|\s*```$",
                                      "", response, flags=re.DOTALL).strip()

            result = GPTResponseParser.parse_json_response(
                response, 'merge_groups', api_manager)
            merge_groups = result.get('merge_groups', [])

            # éªŒè¯æ ¼å¼
            valid_groups = []
            for group in merge_groups:
                if isinstance(group, list) and len(group) >= 2:
                    try:
                        int_group = [int(idx) for idx in group]
                        if all(idx >= 0 for idx in int_group):
                            valid_groups.append(int_group)
                    except (ValueError, TypeError):
                        continue

            return valid_groups

        except Exception as e:
            logger.error(f"âŒ è§£æåˆå¹¶ç»„å¤±è´¥: {e}")
            # ğŸ”¥ æ–°å¢ï¼šè®°å½•åŸå§‹å“åº”å†…å®¹ä»¥ä¾¿è°ƒè¯•
            logger.error(f"ğŸ“‹ å“åº”å‰500å­—ç¬¦: {response[:500]}")
            logger.error(f"ğŸ“‹ å“åº”å500å­—ç¬¦: {response[-500:]}")
            return []

    @staticmethod
    def parse_qa_json(response: str, api_manager: OpenAIAPIManager) -> List[Dict[str, Any]]:
        """è§£æQAç”Ÿæˆçš„JSONå“åº”"""
        def validate_qa_pair(qa: Dict[str, Any]) -> bool:
            """éªŒè¯QAå¯¹æ ¼å¼"""
            required_fields = ['question', 'answer']

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            for field in required_fields:
                if field not in qa or not qa[field] or not isinstance(qa[field], str):
                    return False

            # æ£€æŸ¥å†…å®¹é•¿åº¦
            if len(qa['question'].strip()) < 5 or len(qa['answer'].strip()) < 10:
                return False

            # è®¾ç½®é»˜è®¤å€¼
            if 'type' not in qa:
                qa['type'] = 'factual'
            if 'confidence' not in qa:
                qa['confidence'] = 0.8

            return True
        try:
            # ğŸ”¥ æ–°å¢ï¼šå¤„ç†å¸¦æœ‰ä»£ç å—æ ‡è¯†ç¬¦çš„å“åº”
            if response.startswith("```"):
                # æå–ä»£ç å—ä¸­çš„JSONå†…å®¹
                import re
                json_match = re.search(
                    r"```(?:json)?\s*({.*?})\s*```", response, re.DOTALL)
                if json_match:
                    response = json_match.group(1)
                else:
                    # å¦‚æœæ— æ³•æå–JSONï¼Œå°è¯•ç›´æ¥å»é™¤ä»£ç å—æ ‡è®°
                    response = re.sub(r"^```(?:json)?\s*|\s*```$",
                                      "", response, flags=re.DOTALL).strip()

            result = GPTResponseParser.parse_json_response(
                response, 'qa_pairs', api_manager)
            qa_pairs = result.get('qa_pairs', [])

            # éªŒè¯QAå¯¹æ ¼å¼
            validated_pairs = []
            for qa in qa_pairs:
                if validate_qa_pair(qa):
                    validated_pairs.append(qa)

            return validated_pairs

        except Exception as e:
            logger.error(f"âŒ è§£æQAå¯¹JSONå¤±è´¥: {e}")
            # ğŸ”¥ æ–°å¢ï¼šè®°å½•åŸå§‹å“åº”å†…å®¹ä»¥ä¾¿è°ƒè¯•
            logger.error(f"ğŸ“‹ å“åº”å‰500å­—ç¬¦: {response[:500]}")
            if len(response) > 500:
                logger.error(f"ğŸ“‹ å“åº”å500å­—ç¬¦: {response[-500:]}")
            return []


class APICallStats:
    """APIè°ƒç”¨ç»Ÿè®¡"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.total_calls = 0
        self.successful_calls = 0
        self.failed_calls = 0
        self.total_retries = 0
        self.total_tokens = 0
        self.total_duration = 0.0

    def record_call(self, success: bool, retry_count: int = 0, tokens: int = 0, duration: float = 0.0):
        self.total_calls += 1
        self.total_retries += retry_count
        self.total_tokens += tokens
        self.total_duration += duration

        if success:
            self.successful_calls += 1
        else:
            self.failed_calls += 1

    def get_stats(self) -> Dict[str, Any]:
        success_rate = (self.successful_calls /
                        self.total_calls * 100) if self.total_calls > 0 else 0
        avg_retries = (self.total_retries /
                       self.total_calls) if self.total_calls > 0 else 0
        avg_duration = (self.total_duration /
                        self.total_calls) if self.total_calls > 0 else 0

        return {
            'total_calls': self.total_calls,
            'successful_calls': self.successful_calls,
            'failed_calls': self.failed_calls,
            'success_rate': f"{success_rate:.1f}%",
            'total_retries': self.total_retries,
            'avg_retries_per_call': f"{avg_retries:.1f}",
            'total_tokens': self.total_tokens,
            'total_duration': f"{self.total_duration:.2f}s",
            'avg_duration_per_call': f"{avg_duration:.2f}s"
        }


api_stats = APICallStats()


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""

    _instance = None
    _config = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ConfigManager, cls).__new__(cls)
        return cls._instance

    @classmethod
    def load_config(cls, config_path: str = None) -> Dict[str, Any]:
        """
        åŠ è½½é…ç½®æ–‡ä»¶

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„

        Returns:
            é…ç½®å­—å…¸
        """
        if cls._config is not None:
            return cls._config

        if config_path is None:
            # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆpdf2mdç›®å½•ï¼‰
            current_dir = Path(__file__).parent
            config_path = current_dir / "config.json"

        config_path = Path(config_path)

        # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
        if not config_path.exists():
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œåˆ›å»ºé»˜è®¤é…ç½®...")
            default_config = cls._get_default_config()
            cls._save_config(default_config, config_path)
            cls._config = default_config
            return default_config

        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            # éªŒè¯å’Œè¡¥å……é…ç½®
            config = cls._validate_and_complete_config(config)
            cls._config = config

            logger.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
            return config

        except json.JSONDecodeError as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")
            logger.info("ä½¿ç”¨é»˜è®¤é…ç½®...")
            default_config = cls._get_default_config()
            cls._config = default_config
            return default_config

        except Exception as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            logger.info("ä½¿ç”¨é»˜è®¤é…ç½®...")
            default_config = cls._get_default_config()
            cls._config = default_config
            return default_config

    @classmethod
    def _get_default_config(cls) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "openai": {
                "api_key": "",
                "base_url": "https://api.openai.com/v1",
                "model": "gpt-3.5-turbo",
                "timeout": 90,
                "max_retries": 5
            },
            "knowledge_extractor": {
                "min_quality_score": 65,
                "batch_size": 5,
                "enable_api_health_check": True,
                "entity_types": {
                    "ThreatActor": "å¨èƒè¡Œä¸ºè€…/æ”»å‡»ç»„ç»‡/APTç»„ç»‡",
                    "Tactics": "æˆ˜æœ¯å±‚é¢çš„æ”»å‡»ç­–ç•¥",
                    "Techniques": "å…·ä½“çš„æ”»å‡»æŠ€æœ¯æ–¹æ³•",
                    "Procedures": "è¯¦ç»†çš„æ‰§è¡Œæ­¥éª¤è¿‡ç¨‹",
                    "Tools": "ä½¿ç”¨çš„å·¥å…·è½¯ä»¶"
                },
                "relationship_types": {
                    "USE": "å¨èƒè¡Œä¸ºè€…ä½¿ç”¨æˆ˜æœ¯",
                    "IMPLEMENT": "æˆ˜æœ¯å®ç°æŠ€æœ¯",
                    "EXECUTE": "æŠ€æœ¯æ‰§è¡Œè¿‡ç¨‹",
                    "APPLY": "è¿‡ç¨‹åº”ç”¨å·¥å…·",
                    "ABSTRACT": "å°†è¿‡ç¨‹æŠ½è±¡ä¸ºæŠ€æœ¯/æˆ˜æœ¯"
                },
                "entity_examples": {
                    "ThreatActor": "APT29, Lazarus Group",
                    "Tactics": "Initial Access, Defense Evasion",
                    "Techniques": "Spear Phishing, DLL Injection",
                    "Procedures": "Create scheduled task, Execute PowerShell",
                    "Tools": "Cobalt Strike, Mimikatz"
                }
            },
            "pdf_converter": {
                "artifacts_path": "./docling-models",
                "do_ocr": False
            },
            "html_converter": {
                "extract_images": True,
                "timeout": 30,
                "max_image_size": 5242880
            },
            "chunk_splitter": {
                "max_chunk_size": 2000,
                "chunk_overlap": 200,
                "separators": ["\n\n", "\n", " ", ""]
            },
            "graph_processor": {
                "entity_alignment": {
                    "similarity_threshold": 0.75,
                    "enable_acronym_match": True,
                    "enable_contains_match": True
                },
                "quality_filters": {
                    "min_entity_name_length": 2,
                    "min_relationship_confidence": 0.6
                }
            },
            "output": {
                "save_intermediate": True,
                "create_timestamp_dirs": False,
                "compression": False
            },
            "logging": {
                "level": "INFO",
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                "file_output": False
            }
        }

    @classmethod
    def _validate_and_complete_config(cls, config: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å’Œè¡¥å……é…ç½®"""
        default_config = cls._get_default_config()

        # é€’å½’åˆå¹¶é…ç½®ï¼Œç¡®ä¿æ‰€æœ‰å¿…éœ€çš„é”®éƒ½å­˜åœ¨
        def merge_configs(default: Dict[str, Any], user: Dict[str, Any]) -> Dict[str, Any]:
            merged = default.copy()
            for key, value in user.items():
                if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
                    merged[key] = merge_configs(merged[key], value)
                else:
                    merged[key] = value
            return merged

        complete_config = merge_configs(default_config, config)

        # éªŒè¯å…³é”®é…ç½®
        cls._validate_critical_settings(complete_config)

        return complete_config

    @classmethod
    def _validate_critical_settings(cls, config: Dict[str, Any]):
        """éªŒè¯å…³é”®é…ç½®é¡¹"""
        # æ£€æŸ¥OpenAI APIå¯†é’¥
        openai_config = config.get('openai', {})
        api_key = openai_config.get('api_key', '')

        if not api_key or api_key == "your-api-key-here":
            logger.warning("âš ï¸ OpenAI APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨config.jsonä¸­è®¾ç½®api_key")

        # æ£€æŸ¥è¶…æ—¶è®¾ç½®
        timeout = openai_config.get('timeout', 90)
        if timeout < 10:
            logger.warning(f"âš ï¸ APIè¶…æ—¶è®¾ç½®è¿‡ä½: {timeout}ç§’ï¼Œå»ºè®®è‡³å°‘30ç§’")

        # æ£€æŸ¥chunkå¤§å°è®¾ç½®
        chunk_config = config.get('chunk_splitter', {})
        max_chunk_size = chunk_config.get('max_chunk_size', 2000)
        if max_chunk_size < 500:
            logger.warning(f"âš ï¸ chunkå¤§å°è®¾ç½®è¿‡å°: {max_chunk_size}ï¼Œå¯èƒ½å½±å“ä¿¡æ¯æå–æ•ˆæœ")

        # æ£€æŸ¥è´¨é‡è¯„åˆ†è®¾ç½®
        kg_config = config.get('knowledge_extractor', {})
        min_quality_score = kg_config.get('min_quality_score', 65)
        if min_quality_score < 30:
            logger.warning(f"âš ï¸ è´¨é‡è¯„åˆ†é˜ˆå€¼è¿‡ä½: {min_quality_score}ï¼Œå¯èƒ½åŒ…å«ä½è´¨é‡æ•°æ®")

    @classmethod
    def _save_config(cls, config: Dict[str, Any], config_path: Path):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            config_path.parent.mkdir(parents=True, exist_ok=True)

            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}")

        except Exception as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")

    @classmethod
    def update_config(cls, key_path: str, value: Any) -> bool:
        """
        æ›´æ–°é…ç½®é¡¹

        Args:
            key_path: é…ç½®é”®è·¯å¾„ï¼Œå¦‚ "openai.api_key"
            value: æ–°å€¼

        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        if cls._config is None:
            cls.load_config()

        try:
            # åˆ†è§£é”®è·¯å¾„
            keys = key_path.split('.')
            current = cls._config

            # å¯¼èˆªåˆ°çˆ¶çº§
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]

            # è®¾ç½®å€¼
            current[keys[-1]] = value

            # ä¿å­˜é…ç½®
            current_dir = Path(__file__).parent
            config_path = current_dir / "config.json"
            cls._save_config(cls._config, config_path)

            logger.info(f"âœ… é…ç½®å·²æ›´æ–°: {key_path} = {value}")
            return True

        except Exception as e:
            logger.error(f"âŒ é…ç½®æ›´æ–°å¤±è´¥: {e}")
            return False

    @classmethod
    def get_config_value(cls, key_path: str, default_value: Any = None) -> Any:
        """
        è·å–é…ç½®å€¼

        Args:
            key_path: é…ç½®é”®è·¯å¾„ï¼Œå¦‚ "openai.api_key"
            default_value: é»˜è®¤å€¼

        Returns:
            é…ç½®å€¼
        """
        if cls._config is None:
            cls.load_config()

        try:
            keys = key_path.split('.')
            current = cls._config

            for key in keys:
                if key in current:
                    current = current[key]
                else:
                    return default_value

            return current

        except Exception as e:
            logger.error(f"âŒ è·å–é…ç½®å€¼å¤±è´¥: {e}")
            return default_value

    @classmethod
    def reload_config(cls, config_path: str = None) -> Dict[str, Any]:
        """é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶"""
        cls._config = None
        return cls.load_config(config_path)

    @classmethod
    def print_config_summary(cls):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        if cls._config is None:
            cls.load_config()

        logger.info("ğŸ“‹ å½“å‰é…ç½®æ‘˜è¦:")
        logger.info(
            f"   - OpenAIæ¨¡å‹: {cls._config.get('openai', {}).get('model', 'N/A')}")
        logger.info(
            f"   - APIè¶…æ—¶: {cls._config.get('openai', {}).get('timeout', 'N/A')}ç§’")
        logger.info(
            f"   - æœ€å¤§é‡è¯•: {cls._config.get('openai', {}).get('max_retries', 'N/A')}æ¬¡")
        logger.info(
            f"   - chunkå¤§å°: {cls._config.get('chunk_splitter', {}).get('max_chunk_size', 'N/A')}")
        logger.info(
            f"   - è´¨é‡é˜ˆå€¼: {cls._config.get('quality_filter', {}).get('min_quality_score', 'N/A')}")
        logger.info(
            f"   - ä¿å­˜ä¸­é—´ç»“æœ: {cls._config.get('output', {}).get('save_intermediate', 'N/A')}")


class APICallError(Exception):
    """APIè°ƒç”¨å¼‚å¸¸"""

    def __init__(self, message: str, status_code: Optional[int] = None, retry_count: int = 0):
        super().__init__(message)
        self.status_code = status_code
        self.retry_count = retry_count


class TokenCounter:
    """Tokenè®¡æ•°å™¨"""

    @staticmethod
    def get_tokens_num(text_list: List[str]) -> int:
        """
        ç›´æ¥è°ƒç”¨EASæ¥å£ï¼ŒåŸºäºLLMæ¨¡å‹åˆ†è¯ï¼Œè®¡ç®—tokenæ•°é‡
        æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸æ”¯æŒæ‰¹é‡å¤„ç†ï¼Œæ‰¹é‡éœ€è‡ªå·±æ”¹é€ ä¸ºå¼‚æ­¥æ¨¡å¼
        """
        api_key = "MTM0NWE3NDBhYzEyZjE4YmUwNTU3OTg3MjM5ZGIyOGJkMTAzOTM0YQ=="
        api_base = "http://jianyu.aliyun-inc.com/agent/pai/api/predict/yundun_prehost"

        def _headers(api_key: str) -> dict:
            return {"Authorization": f"Bearer {api_key}"}

        try:
            response = requests.post(
                url=api_base + '/count_token',
                headers=_headers(api_key),
                json={"prompt": text_list[0]}
            )

            if response.text.strip():
                time.sleep(0.1)
                rst = response.json()
                logger.info("token_num: %d", rst['count'])
                return rst['count']
            else:
                logger.info("Empty or whitespace-only response received.")
                return 0

        except json.JSONDecodeError as e:
            logger.exception("Invalid JSON format: %s", e)
            logger.exception("text_list: %s", text_list)
            logger.exception("response: %s", response.text)
            return 0
        except Exception as e:
            logger.error(f"Tokenè®¡ç®—è¯·æ±‚å¤±è´¥: {e}")
            return 0

    @staticmethod
    def count_tokens(text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡ - é€‚é…å‡½æ•°"""
        try:
            return TokenCounter.get_tokens_num([text])
        except Exception as e:
            logger.error(f"Tokenè®¡ç®—å¤±è´¥: {e}")
            # ç®€å•ä¼°ç®—ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            english_words = len(re.findall(r'[a-zA-Z]+', text))
            return chinese_chars + english_words


class FileManager:
    """æ–‡ä»¶ç®¡ç†å™¨"""

    @staticmethod
    def ensure_directory(file_path: str):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)

    @staticmethod
    def save_json(data: Dict[str, Any], file_path: str):
        """ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶"""
        try:
            FileManager.ensure_directory(file_path)
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
            raise

    @staticmethod
    def load_json(file_path: str) -> Dict[str, Any]:
        """ä»æ–‡ä»¶åŠ è½½JSONæ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
            raise

    @staticmethod
    def save_text(content: str, file_path: str):
        """ä¿å­˜æ–‡æœ¬å†…å®¹åˆ°æ–‡ä»¶"""
        try:
            FileManager.ensure_directory(file_path)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"æ–‡æœ¬å·²ä¿å­˜åˆ°: {file_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
            raise


class StatisticsReporter:
    """ç»Ÿè®¡æŠ¥å‘Šå™¨ - æ”¯æŒèšåˆç»Ÿè®¡æ ¼å¼çš„å›¾æ•°æ®"""

    def analyze_aggregated_graph_data(self, graph_list: List[Dict]) -> Dict[str, Any]:
        """åˆ†æèšåˆç»Ÿè®¡æ ¼å¼çš„å›¾æ•°æ®"""
        if not graph_list:
            return {"unique_nodes": 0, "unique_edges": 0, "node_type_counts": {}, "edge_type_counts": {}}

        logger.info(f"ğŸ” å¼€å§‹åˆ†æèšåˆå›¾æ•°æ®ï¼Œå…±{len(graph_list)}ä¸ªç»Ÿè®¡å—")

        # ç»Ÿè®¡å®ä½“ç§ç±»æ•°å’Œå‡ºç°æ¬¡æ•°
        node_type_counts = {}       # æ¯ç§èŠ‚ç‚¹ç±»å‹æœ‰å¤šå°‘ä¸ªä¸åŒå®ä½“
        edge_type_counts = {}       # æ¯ç§å…³ç³»ç±»å‹æœ‰å¤šå°‘ä¸ªä¸åŒå…³ç³»
        node_occurrences = {}       # èŠ‚ç‚¹ç±»å‹æ€»å‡ºç°æ¬¡æ•°
        edge_occurrences = {}       # å…³ç³»ç±»å‹æ€»å‡ºç°æ¬¡æ•°
        unique_nodes = {}           # è®°å½•æ¯ç§ç±»å‹çš„å”¯ä¸€å®ä½“
        unique_edges = {}           # è®°å½•æ¯ç§ç±»å‹çš„å”¯ä¸€å…³ç³»

        for i, data_block in enumerate(graph_list):
            # å¤„ç†nodesç»Ÿè®¡
            if 'nodes' in data_block:
                nodes_data = data_block['nodes']
                logger.info(f"   å—[{i}] - èŠ‚ç‚¹ç»Ÿè®¡é¡¹: {len(nodes_data)}")

                for node_key, count in nodes_data.items():
                    try:
                        import json
                        node_info = json.loads(node_key)
                        node_type = node_info.get('entity_type', 'Unknown')
                        node_label = node_info.get('label', 'Unknown')

                        # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
                        node_occurrences[node_type] = node_occurrences.get(
                            node_type, 0) + count

                        # ç»Ÿè®¡å®ä½“ç§ç±»æ•°
                        if node_type not in unique_nodes:
                            node_type_counts[node_type] = 0
                            unique_nodes[node_type] = set()

                        # ç”¨labelä½œä¸ºå”¯ä¸€æ ‡è¯†
                        if node_label not in unique_nodes[node_type]:
                            unique_nodes[node_type].add(node_label)
                            node_type_counts[node_type] += 1

                    except json.JSONDecodeError:
                        node_occurrences['Unknown'] = node_occurrences.get(
                            'Unknown', 0) + count
                        if 'Unknown' not in unique_nodes:
                            node_type_counts['Unknown'] = 0
                            unique_nodes['Unknown'] = set()

            # å¤„ç†edgesç»Ÿè®¡
            if 'edges' in data_block:
                edges_data = data_block['edges']
                logger.info(f"   å—[{i}] - è¾¹ç»Ÿè®¡é¡¹: {len(edges_data)}")

                for edge_key, count in edges_data.items():
                    try:
                        import json
                        edge_info = json.loads(edge_key)
                        edge_type = edge_info.get('label', 'Unknown')

                        # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
                        edge_occurrences[edge_type] = edge_occurrences.get(
                            edge_type, 0) + count

                        # ç»Ÿè®¡å…³ç³»ç§ç±»æ•°
                        if edge_type not in unique_edges:
                            edge_type_counts[edge_type] = 0
                            unique_edges[edge_type] = set()

                        # ç”¨å®Œæ•´edge_keyä½œä¸ºå”¯ä¸€æ ‡è¯†
                        if edge_key not in unique_edges[edge_type]:
                            unique_edges[edge_type].add(edge_key)
                            edge_type_counts[edge_type] += 1

                    except json.JSONDecodeError:
                        edge_occurrences['Unknown'] = edge_occurrences.get(
                            'Unknown', 0) + count
                        if 'Unknown' not in unique_edges:
                            edge_type_counts['Unknown'] = 0
                            unique_edges['Unknown'] = set()

        # è®¡ç®—æ€»æ•°
        total_unique_nodes = sum(node_type_counts.values())
        total_unique_edges = sum(edge_type_counts.values())
        total_node_occurrences = sum(node_occurrences.values())
        total_edge_occurrences = sum(edge_occurrences.values())

        logger.info(
            f"ğŸ” èšåˆåˆ†æå®Œæˆ: {total_unique_nodes}ä¸ªä¸åŒèŠ‚ç‚¹(å‡ºç°{total_node_occurrences}æ¬¡), {total_unique_edges}ä¸ªä¸åŒè¾¹(å‡ºç°{total_edge_occurrences}æ¬¡)")

        return {
            "unique_nodes": total_unique_nodes,
            "unique_edges": total_unique_edges,
            "total_node_occurrences": total_node_occurrences,
            "total_edge_occurrences": total_edge_occurrences,
            "node_type_counts": node_type_counts,
            "edge_type_counts": edge_type_counts,
            "node_occurrences": node_occurrences,
            "edge_occurrences": edge_occurrences
        }

    def print_graph_summary(self, graph_list: List[Dict], chunk_count: int, data_source: str = ""):
        """æ‰“å°å›¾æ•°æ®æ‘˜è¦"""
        logger.info(f"ğŸ” å¼€å§‹ç»Ÿè®¡å›¾æ•°æ®ï¼Œæ•°æ®æº: {data_source}")

        stats = self.analyze_aggregated_graph_data(graph_list)

        logger.info("=" * 50)
        logger.info(f"ğŸ“Š å›¾è°±ç»Ÿè®¡({data_source})")
        logger.info("=" * 50)
        logger.info(
            f"ğŸ·ï¸  èŠ‚ç‚¹æ€»æ•°: {stats['unique_nodes']} (å‡ºç°æ¬¡æ•°: {stats['total_node_occurrences']})")
        logger.info(
            f"ğŸ”— å…³ç³»æ€»æ•°: {stats['unique_edges']} (å‡ºç°æ¬¡æ•°: {stats['total_edge_occurrences']})")
        logger.info(f"ğŸ“„ å¤„ç†å—æ•°: {chunk_count}")

        # æ˜¾ç¤ºå®Œæ•´çš„èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
        logger.info("ğŸ“‹ å®Œæ•´èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡:")
        for node_type, unique_count in sorted(stats['node_type_counts'].items(), key=lambda x: x[1], reverse=True):
            occurrence_count = stats['node_occurrences'].get(node_type, 0)
            logger.info(
                f"   {node_type}: {unique_count}ç§ (å‡ºç°{occurrence_count}æ¬¡)")

        # æ˜¾ç¤ºå®Œæ•´çš„å…³ç³»ç±»å‹ç»Ÿè®¡
        logger.info("ğŸ”— å®Œæ•´å…³ç³»ç±»å‹ç»Ÿè®¡:")
        for edge_type, unique_count in sorted(stats['edge_type_counts'].items(), key=lambda x: x[1], reverse=True):
            occurrence_count = stats['edge_occurrences'].get(edge_type, 0)
            logger.info(
                f"   {edge_type}: {unique_count}ç§ (å‡ºç°{occurrence_count}æ¬¡)")

        # ä¸»è¦ç±»å‹æ˜¾ç¤ºï¼ˆå‰5åï¼‰
        top_node_types = sorted(
            stats['node_type_counts'].items(), key=lambda x: x[1], reverse=True)[:5]
        top_edge_types = sorted(
            stats['edge_type_counts'].items(), key=lambda x: x[1], reverse=True)[:5]

        logger.info(
            f"ğŸ¯ ä¸»è¦èŠ‚ç‚¹ç±»å‹: {', '.join([f'{k}({v})' for k, v in top_node_types])}")
        logger.info(
            f"ğŸ”„ ä¸»è¦å…³ç³»ç±»å‹: {', '.join([f'{k}({v})' for k, v in top_edge_types])}")
        logger.info("=" * 50)


class ContentAnalyzer:
    """å†…å®¹åˆ†æå™¨ - ç»Ÿä¸€å¤„ç†æ–‡æ¡£å†…å®¹åˆ†æå’Œä¿å­˜"""

    def analyze_and_optionally_save(
        self,
        docs: List,
        selected_docs: List,
        selection_info: Dict,
        output_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£å†…å®¹ï¼Œå¯é€‰æ‹©ä¿å­˜åˆ°æ–‡ä»¶

        Args:
            docs: æ‰€æœ‰æ–‡æ¡£å—
            selected_docs: é€‰ä¸­çš„æ–‡æ¡£å—
            selection_info: é€‰æ‹©ä¿¡æ¯
            output_dir: è¾“å‡ºç›®å½•ï¼ŒNoneè¡¨ç¤ºä¸ä¿å­˜æ–‡ä»¶
        """
        # åŸºç¡€ç»Ÿè®¡åˆ†æ
        content_analysis = self.analyze_content_distribution(docs)

        # æ·»åŠ é€‰æ‹©ç›¸å…³çš„ç»Ÿè®¡
        content_analysis.update({
            'selected_chunks_count': len(selected_docs),
            'selection_info': selection_info,
            'selection_ratio': len(selected_docs) / len(docs) if docs else 0
        })

        # ä¿å­˜è¯¦ç»†ä¿¡æ¯åˆ°æ–‡ä»¶
        if output_dir:
            self._save_chunks_analysis(
                docs, selected_docs, selection_info, output_dir)

        return content_analysis

    def _save_chunks_analysis(
        self,
        docs: List,
        selected_docs: List,
        selection_info: Dict,
        output_dir: str
    ):
        """ä¿å­˜chunksåˆ†æä¿¡æ¯åˆ°æ–‡ä»¶"""
        try:
            # ä¿å­˜é€‰ä¸­çš„chunks
            selected_chunks_path = os.path.join(
                output_dir, "03_selected_chunks.json")
            chunks_data = {
                'selected_chunks': [
                    {
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'length': doc.metadata.get('token_length', len(doc.page_content))
                    }
                    for doc in selected_docs
                ],
                'selection_info': selection_info,
                'total_chunks': len(docs),
                'selected_count': len(selected_docs)
            }

            file_manager = FileManager()
            file_manager.save_json(chunks_data, selected_chunks_path)
            logger.info(f"âœ… å·²ä¿å­˜é€‰ä¸­chunksä¿¡æ¯åˆ°: {selected_chunks_path}")

            # ğŸ”¥ æ–°å¢ï¼šä¿å­˜æ‰€æœ‰chunksåˆ°02.5_total_chunks.json
            total_chunks_path = os.path.join(
                output_dir, "02.5_total_chunks.json")
            total_chunks_data = [
                {
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'length': doc.metadata.get('token_length', len(doc.page_content))
                }
                for doc in docs
            ]

            file_manager.save_json(total_chunks_data, total_chunks_path)
            logger.info(f"âœ… å·²ä¿å­˜æ‰€æœ‰chunksä¿¡æ¯åˆ°: {total_chunks_path}")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜chunksåˆ†æä¿¡æ¯å¤±è´¥: {e}")

    def analyze_content_distribution(self, docs: List) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£å†…å®¹åˆ†å¸ƒï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        if not docs:
            return {
                'total_chunks': 0,
                'avg_length': 0,
                'length_distribution': {},
                'content_types': {}
            }

        # é•¿åº¦ç»Ÿè®¡
        lengths = [len(doc.page_content) for doc in docs]

        # é•¿åº¦åˆ†å¸ƒ
        length_ranges = {
            '0-500': 0,
            '500-1000': 0,
            '1000-2000': 0,
            '2000+': 0
        }

        for length in lengths:
            if length <= 500:
                length_ranges['0-500'] += 1
            elif length <= 1000:
                length_ranges['500-1000'] += 1
            elif length <= 2000:
                length_ranges['1000-2000'] += 1
            else:
                length_ranges['2000+'] += 1

        # å†…å®¹ç±»å‹ç®€å•åˆ†æ
        content_types = {
            'text_heavy': 0,
            'code_like': 0,
            'mixed': 0
        }

        for doc in docs:
            content = doc.page_content.lower()
            if any(keyword in content for keyword in ['def ', 'class ', 'import ', '{', '}', ';']):
                content_types['code_like'] += 1
            elif len(content.split()) > 50:
                content_types['text_heavy'] += 1
            else:
                content_types['mixed'] += 1

        return {
            'total_chunks': len(docs),
            'avg_length': sum(lengths) / len(lengths) if lengths else 0,
            'min_length': min(lengths) if lengths else 0,
            'max_length': max(lengths) if lengths else 0,
            'length_distribution': length_ranges,
            'content_types': content_types
        }


class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨"""

    def __init__(self, total_steps: int):
        self.total_steps = total_steps
        self.current_step = 0
        self.start_time = time.time()

    def update(self, description: str = ""):
        """æ›´æ–°è¿›åº¦"""
        self.current_step += 1
        progress = (self.current_step / self.total_steps) * 100

        elapsed_time = time.time() - self.start_time
        if self.current_step > 0:
            estimated_total_time = elapsed_time * self.total_steps / self.current_step
            remaining_time = estimated_total_time - elapsed_time
        else:
            remaining_time = 0

        logger.info(
            f"è¿›åº¦: {progress:.1f}% ({self.current_step}/{self.total_steps}) - "
            f"{description} - é¢„è®¡å‰©ä½™: {remaining_time:.1f}ç§’"
        )
