# coding:utf-8
"""
çŸ¥è¯†å›¾è°±æå–æ¨¡å— - è´Ÿè´£ä»æ–‡æ¡£å—ä¸­æå–çŸ¥è¯†å›¾è°±
"""
import re
import json
from typing import List, Dict, Any, Tuple, Optional
from langchain.docstore.document import Document
import logging
from utils import OpenAIAPIManager
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils import GPTResponseParser
import threading
logger = logging.getLogger(__name__)

# ---------- 1. é™æ€æ¨¡æ¿ ----------
_BASE_PROMPT_TMPL = """\
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘ç»œå®‰å…¨çŸ¥è¯†å›¾è°±æå–ä¸“å®¶ï¼Œæ ¹æ®userçš„contentä¿¡æ¯ï¼Œæå–å¯¹åº”çš„å®ä½“å’Œå…³ç³»ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„jsonæ ¼å¼è¿”å›

## å®ä½“ç±»å‹
{entity_desc}

## å…³ç³»ç±»å‹
{rel_desc}


## æå–è¦æ±‚
ä»å¨èƒæƒ…æŠ¥æ–‡æ¡£ä¸­æå–å®‰å…¨ç›¸å…³çš„å®ä½“å’Œå…³ç³»ï¼Œé‡ç‚¹å…³æ³¨ï¼š
- å¨èƒç»„ç»‡åŠå…¶å‘èµ·çš„æ”»å‡»äº‹ä»¶
- æ”»å‡»äº‹ä»¶çš„ç›®æ ‡ã€æˆ˜æœ¯ã€æŠ€æœ¯å’Œç¨‹åº
- ç¨‹åºä½¿ç”¨çš„å·¥å…·å’Œèµ„äº§
- æŠ¥å‘Šå¯¹æ”»å‡»äº‹ä»¶çš„è®°å½•

## ğŸš¨ é‡è¦ï¼šå®ä½“-å…³ç³»ä¸€è‡´æ€§è§„åˆ™
1. **å…³ç³»å¿…é¡»å¼•ç”¨å·²å­˜åœ¨çš„å®ä½“**ï¼šrelationshipsä¸­çš„sourceå’Œtargetå¿…é¡»æ˜¯entitiesä¸­æŸä¸ªå®ä½“çš„id
2. **é¿å…å­¤ç«‹å®ä½“**ï¼šæ¯ä¸ªentitiesä¸­çš„å®ä½“éƒ½åº”è¯¥è‡³å°‘å‚ä¸ä¸€ä¸ªå…³ç³»
3. **å®ä½“IDå”¯ä¸€æ€§**ï¼šæ¯ä¸ªå®ä½“çš„idå¿…é¡»å”¯ä¸€ï¼Œä¸èƒ½é‡å¤
4. **å…³ç³»å®Œæ•´æ€§**ï¼šå¦‚æœæå–äº†å…³ç³»ï¼Œå¿…é¡»åŒæ—¶æå–å…³ç³»ä¸¤ç«¯çš„å®ä½“
5. **æŠ¥å‘Šå®ä½“ç»Ÿä¸€æ€§**ï¼šè‹¥chunkå¼€å¤´ä¸º"# æŠ¥å‘Š\n"ï¼Œåˆ™æå–æˆreport

## è¾“å‡ºæ ¼å¼
ä¸¥æ ¼æŒ‰æ­¤JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»£ç å—æ ‡è®°ï¼š
{{"entities":[],"relationships":[]}}

## few_shotç¤ºä¾‹
{few_shot_examples}

## é¢å¤–è¦æ±‚
{guidance_text}

"""
_FEW_SHOT = {
    "english": '''
### ç¤ºä¾‹1ï¼šå¨èƒç»„ç»‡æ”»å‡»äº‹ä»¶
**è¾“å…¥æ–‡æœ¬ï¼š**
Behind the Great Wall: Void Arachne Targets Chinese-Speaking Users. Void Arachne group launched a campaign targeting Chinese users using SEO poisoning techniques.

**è¾“å‡ºï¼š**
{{"entities":[{{"labels":"Report","id":"report--great-wall","name":"Behind the Great Wall: Void Arachne Targets Chinese-Speaking Users","description":"å¨èƒæƒ…æŠ¥æŠ¥å‘Š"}},{{"labels":"ThreatOrganization","id":"threat-org--void-arachne","name":"Void Arachne","description":"å¨èƒç»„ç»‡"}},{{"labels":"AttackEvent","id":"attack-event--seo-campaign","name":"SEO Poisoning Campaign","description":"é’ˆå¯¹ä¸­æ–‡ç”¨æˆ·çš„SEOæŠ•æ¯’æ”»å‡»æ´»åŠ¨"}},{{"labels":"Target","id":"target--chinese-users","name":"Chinese-Speaking Users","description":"ä¸­æ–‡ç”¨æˆ·"}},{{"labels":"Technique","id":"technique--seo-poisoning","name":"SEO Poisoning","description":"æœç´¢å¼•æ“ä¼˜åŒ–æŠ•æ¯’æŠ€æœ¯"}}],
    "relationships":[{{"type":"BELONG","source":"report--great-wall","target":"attack-event--seo-campaign","confidence":0.95,"evidence":"æŠ¥å‘Šè®°å½•äº†SEOæŠ•æ¯’æ”»å‡»æ´»åŠ¨"}},{{"type":"LAUNCH","source":"threat-org--void-arachne","target":"attack-event--seo-campaign","confidence":0.95,"evidence":"Void Arachneç»„ç»‡å‘èµ·äº†æ”»å‡»æ´»åŠ¨"}},{{"type":"ATTACK","source":"attack-event--seo-campaign","target":"target--chinese-users","confidence":0.95,"evidence":"æ”»å‡»æ´»åŠ¨é’ˆå¯¹ä¸­æ–‡ç”¨æˆ·"}},{{"type":"ATTACK","source":"attack-event--seo-campaign","target":"technique--seo-poisoning","confidence":0.9,"evidence":"æ”»å‡»æ´»åŠ¨ä½¿ç”¨SEOæŠ•æ¯’æŠ€æœ¯"}}]}}

### ç¤ºä¾‹2ï¼šå·¥å…·å’Œç¨‹åºå…³ç³»
**è¾“å…¥æ–‡æœ¬ï¼š**
The malicious MSI file uses Dynamic Link Libraries during the installation process. The MSI installer deploys backdoor components to the system.

**è¾“å‡ºï¼š**
{{"entities":[{{"labels":"Tool","id":"tool--msi-file","name":"Malicious MSI File","description":"æ¶æ„MSIå®‰è£…æ–‡ä»¶"}},{{"labels":"Procedure","id":"procedure--dll-installation","name":"DLL Installation Process","description":"ä½¿ç”¨åŠ¨æ€é“¾æ¥åº“çš„å®‰è£…è¿‡ç¨‹"}},{{"labels":"Tool","id":"tool--dll","name":"Dynamic Link Libraries",
    "description":"åŠ¨æ€é“¾æ¥åº“"}}],"relationships":[{{"type":"USE","source":"procedure--dll-installation","target":"tool--msi-file","confidence":0.9,"evidence":"å®‰è£…è¿‡ç¨‹ä½¿ç”¨MSIæ–‡ä»¶"}},{{"type":"USE","source":"procedure--dll-installation","target":"tool--dll","confidence":0.95,"evidence":"å®‰è£…è¿‡ç¨‹ä½¿ç”¨åŠ¨æ€é“¾æ¥åº“"}}]}}

### ç¤ºä¾‹3ï¼šæˆ˜æœ¯æŠ€æœ¯å±‚çº§å…³ç³»
**è¾“å…¥æ–‡æœ¬ï¼š**
Initial Access (TA0001) includes Spearphishing Link (T1566.002) technique. The attacker implemented email-based social engineering procedures.

**è¾“å‡ºï¼š**
{{"entities":[{{"labels":"Tactic","id":"tactic--initial-access","name":"Initial Access","description":"TA0001: åˆå§‹è®¿é—®æˆ˜æœ¯"}},{{"labels":"Technique","id":"technique--spearphishing","name":"Spearphishing Link","description":"T1566.002: é’“é±¼é“¾æ¥æŠ€æœ¯"}},{{"labels":"Procedure","id":"procedure--email-social-eng","name":"Email Social Engineering",
    "description":"åŸºäºé‚®ä»¶çš„ç¤¾ä¼šå·¥ç¨‹å­¦ç¨‹åº"}}],"relationships":[{{"type":"HAS","source":"tactic--initial-access","target":"technique--spearphishing","confidence":0.95,"evidence":"åˆå§‹è®¿é—®æˆ˜æœ¯åŒ…å«é’“é±¼é“¾æ¥æŠ€æœ¯"}},{{"type":"LAUNCH","source":"technique--spearphishing","target":"procedure--email-social-eng","confidence":0.9,"evidence":"é’“é±¼æŠ€æœ¯å¯åŠ¨é‚®ä»¶ç¤¾å·¥ç¨‹åº"}}]}}

### ç¤ºä¾‹4ï¼šèµ„äº§åˆ©ç”¨å…³ç³»
**è¾“å…¥æ–‡æœ¬ï¼š**
Attackers used compromised web servers to host malicious payloads. The infrastructure served as distribution points for malware.

**è¾“å‡ºï¼š**
{{"entities":[{{"labels":"Asset","id":"asset--web-servers","name":"Compromised Web Servers","description":"è¢«æ”»é™·çš„WebæœåŠ¡å™¨"}},{{"labels":"Procedure","id":"procedure--payload-hosting","name":"Malicious Payload Hosting",
    "description":"æ¶æ„è½½è·æ‰˜ç®¡ç¨‹åº"}}],"relationships":[{{"type":"USE","source":"asset--web-servers","target":"procedure--payload-hosting","confidence":0.95,"evidence":"è¢«æ”»é™·çš„WebæœåŠ¡å™¨ç”¨äºæ‰˜ç®¡æ¶æ„è½½è·"}}]}}

### ç¤ºä¾‹5ï¼šç©ºç»“æœç¤ºä¾‹
**è¾“å…¥æ–‡æœ¬ï¼š**
The system was running normally without any suspicious activities detected during the monitoring period.

**è¾“å‡ºï¼š**
{{"entities":[],"relationships":[]}}
''',
    "chinese": """
### ç¤ºä¾‹1ï¼šä¸­æ–‡å¨èƒç»„ç»‡æ”»å‡»äº‹ä»¶
**è¾“å…¥æ–‡æœ¬ï¼š**
æµ·è²èŠ±ç»„ç»‡æ˜¯ç”±å¥‡å®‰ä¿¡å¨èƒæƒ…æŠ¥ä¸­å¿ƒæœ€æ—©æŠ«éœ²å¹¶å‘½åçš„ä¸€ä¸ªAPTç»„ç»‡ï¼Œè¯¥ç»„ç»‡é’ˆå¯¹ä¸­å›½æ”¿åºœã€ç§‘ç ”é™¢æ‰€ã€æµ·äº‹æœºæ„å±•å¼€äº†æœ‰ç»„ç»‡ã€æœ‰è®¡åˆ’ã€æœ‰é’ˆå¯¹æ€§çš„é•¿æ—¶é—´ä¸é—´æ–­æ”»å‡»ã€‚

**è¾“å‡ºï¼š**
{{"entities":[{{"labels":"Report","id":"report--qianxin-apt","name":"å¥‡å®‰ä¿¡å¨èƒæƒ…æŠ¥æŠ¥å‘Š","description":"å¨èƒæƒ…æŠ¥æŠ¥å‘Š"}},{{"labels":"ThreatOrganization","id":"threat-org--ocean-lotus","name":"æµ·è²èŠ±ç»„ç»‡","description":"APTå¨èƒç»„ç»‡"}},{{"labels":"AttackEvent","id":"attack-event--targeted-campaign","name":"é’ˆå¯¹æ€§æ”»å‡»æ´»åŠ¨","description":"æœ‰ç»„ç»‡æœ‰è®¡åˆ’çš„æ”»å‡»æ´»åŠ¨"}},{{"labels":"Target","id":"target--cn-gov","name":"ä¸­å›½æ”¿åºœæœºæ„","description":"æ”»å‡»ç›®æ ‡"}},{{"labels":"Target","id":"target--research-inst","name":"ç§‘ç ”é™¢æ‰€","description":"æ”»å‡»ç›®æ ‡"}},{{"labels":"Target","id":"target--maritime","name":"æµ·äº‹æœºæ„","description":"æ”»å‡»ç›®æ ‡"}}],"relationships":[{{"type":"BELONG","source":"report--qianxin-apt","target":"attack-event--targeted-campaign","confidence":0.95,"evidence":"æŠ¥å‘ŠæŠ«éœ²äº†é’ˆå¯¹æ€§æ”»å‡»æ´»åŠ¨"}},{{"type":"LAUNCH","source":"threat-org--ocean-lotus","target":"attack-event--targeted-campaign","confidence":0.95,"evidence":"æµ·è²èŠ±ç»„ç»‡å‘èµ·æ”»å‡»æ´»åŠ¨"}},{{"type":"ATTACK","source":"attack-event--targeted-campaign","target":"target--cn-gov","confidence":0.9,"evidence":"æ”»å‡»æ´»åŠ¨é’ˆå¯¹ä¸­å›½æ”¿åºœ"}},{{"type":"ATTACK","source":"attack-event--targeted-campaign","target":"target--research-inst","confidence":0.9,"evidence":"æ”»å‡»æ´»åŠ¨é’ˆå¯¹ç§‘ç ”é™¢æ‰€"}},{{"type":"ATTACK","source":"attack-event--targeted-campaign","target":"target--maritime","confidence":0.9,"evidence":"æ”»å‡»æ´»åŠ¨é’ˆå¯¹æµ·äº‹æœºæ„"}}]}}

### ç¤ºä¾‹2ï¼šä¸­æ–‡æ”»å‡»æŠ€æœ¯å’Œå·¥å…·
**è¾“å…¥æ–‡æœ¬ï¼š**
æ”»å‡»è€…ä½¿ç”¨é±¼å‰å¼é’“é±¼é‚®ä»¶ä½œä¸ºåˆå§‹è®¿é—®æ‰‹æ®µï¼Œé‚®ä»¶ä¸­åŒ…å«æ¶æ„é™„ä»¶ï¼Œåˆ©ç”¨0dayæ¼æ´æ‰§è¡Œæ¶æ„ä»£ç ã€‚

**è¾“å‡ºï¼š**
{{"entities":[{{"labels":"Technique","id":"technique--spearphishing","name":"é±¼å‰å¼é’“é±¼é‚®ä»¶","description":"é’“é±¼æ”»å‡»æŠ€æœ¯"}},{{"labels":"Tool","id":"tool--malicious-attachment","name":"æ¶æ„é™„ä»¶","description":"æ”»å‡»å·¥å…·"}},{{"labels":"Tool","id":"tool--zero-day","name":"0dayæ¼æ´","description":"é›¶æ—¥æ¼æ´åˆ©ç”¨å·¥å…·"}},{{"labels":"Procedure","id":"procedure--code-execution","name":"æ¶æ„ä»£ç æ‰§è¡Œ","description":"ä»£ç æ‰§è¡Œç¨‹åº"}}],"relationships":[{{"type":"USE","source":"technique--spearphishing","target":"tool--malicious-attachment","confidence":0.95,"evidence":"é’“é±¼é‚®ä»¶ä½¿ç”¨æ¶æ„é™„ä»¶"}},{{"type":"USE","source":"procedure--code-execution","target":"tool--zero-day","confidence":0.9,"evidence":"ä»£ç æ‰§è¡Œåˆ©ç”¨0dayæ¼æ´"}},{{"type":"LAUNCH","source":"technique--spearphishing","target":"procedure--code-execution","confidence":0.85,"evidence":"é’“é±¼æŠ€æœ¯å¯åŠ¨ä»£ç æ‰§è¡Œ"}}]}}
"""
}

_GUIDANCE_TEMPLATES = {
    "table": "\n## ğŸ“Š è¡¨æ ¼å†…å®¹æå–\n- æ¯è¡Œä¸ºå®ä½“ï¼Œåˆ—ä¸ºå±æ€§\n- é‡ç‚¹æå–æŠ€æœ¯IDå’Œå·¥å…·åç§°",

    "en": "\n## ğŸŒ English Content\n- Keep entity names in English\n- Extract all T1XXX patterns\n- Maintain tool name capitalization",

    "zh": "\n## ğŸ‡¨ğŸ‡³ ä¸­æ–‡å†…å®¹\n- å®ä½“åç§°ä½¿ç”¨ä¸­æ–‡ï¼Œä¿ç•™æŠ€æœ¯ID\n- å·¥å…·åç§°ä¿æŒè‹±æ–‡åŸå",

    "mixed": "\n## ğŸ”„ æ··åˆè¯­è¨€\n- ä¿æŒåŸæ–‡è¯­è¨€ï¼Œä¸è¦ç¿»è¯‘\n- ä¼˜å…ˆæå–æŠ€æœ¯IDæ¨¡å¼",

    "long": "\n## ğŸ“„ é•¿æ–‡æœ¬ï¼šé‡ç‚¹æå–æŠ€æœ¯IDã€å·¥å…·ã€å¨èƒç»„ç»‡",
    "short": "\n## ğŸ“ çŸ­æ–‡æœ¬ï¼šç²¾ç¡®æå–ï¼Œé¿å…é—æ¼å…³é”®å®ä½“",

    "attck": "\n## âš”ï¸ ATT&CKç»“æ„ï¼šæŒ‰æˆ˜æœ¯-æŠ€æœ¯å±‚çº§æå–ï¼Œå»ºç«‹IMPLEMENTå…³ç³»"
}


SECURITY_KEYWORDS = {
    # ä¸­æ–‡å…³é”®è¯
    'chinese': [
        'APT', 'æ”»å‡»', 'å¨èƒ', 'æ¶æ„', 'æ¼æ´', 'ç—…æ¯’', 'æœ¨é©¬', 'åé—¨',
        'é’“é±¼', 'é—´è°', 'æ¸—é€', 'å…¥ä¾µ', 'ç»„ç»‡', 'æ´»åŠ¨', 'æŠ€æœ¯', 'æˆ˜æœ¯',
        'å·¥å…·', 'æµç¨‹', 'æ ·æœ¬', 'åˆ†æ', 'æ£€æµ‹', 'é˜²å¾¡', 'å“åº”', 'æƒ…æŠ¥',
        'é»‘å®¢', 'ç½‘ç»œ', 'å®‰å…¨', 'æ•°æ®', 'æ³„éœ²', 'å‹’ç´¢', 'è½¯ä»¶', 'æ¶æ„ä»£ç ',
        'åƒµå°¸ç½‘ç»œ', 'å‘½ä»¤æ§åˆ¶', 'æ¨ªå‘ç§»åŠ¨', 'æƒé™æå‡', 'æŒä¹…åŒ–', 'æ•°æ®æ”¶é›†',
        'æ•°æ®çªƒå–', 'ç ´å', 'å½±å“', 'ç›®æ ‡', 'å—å®³è€…', 'è½½è·', 'æŠ•é€’',
        'æ‰§è¡Œ', 'é€šä¿¡', 'éšè”½', 'ä¼ªè£…', 'ç»•è¿‡', 'è§„é¿', 'ç›‘æ§', 'æ—¥å¿—'
    ],
    # è‹±æ–‡å…³é”®è¯
    'english': [
        'attack', 'threat', 'malicious', 'malware', 'vulnerability', 'exploit',
        'backdoor', 'trojan', 'virus', 'phishing', 'spear', 'espionage',
        'infiltration', 'penetration', 'intrusion', 'breach', 'compromise',
        'payload', 'C2', 'command', 'control', 'RAT', 'botnet', 'dropper',
        'loader', 'downloader', 'implant', 'backdoor', 'rootkit', 'keylogger',
        'stealer', 'ransomware', 'wiper', 'cryptor', 'packer', 'obfuscator',
        'reconnaissance', 'initial', 'access', 'execution', 'persistence',
        'privilege', 'escalation', 'defense', 'evasion', 'credential',
        'discovery', 'lateral', 'movement', 'collection', 'exfiltration',
        'impact', 'command', 'control', 'communication',
        'actor', 'group', 'campaign', 'operation', 'mission', 'target',
        'victim', 'infrastructure', 'domain', 'IP', 'hash', 'indicator',
        'IOC', 'TTPs', 'tactics', 'techniques', 'procedures', 'tools',
        'sample', 'binary', 'executable', 'script', 'shellcode', 'injection',
        'hooking', 'hijacking', 'spoofing', 'masquerading', 'living',
        'fileless', 'memory', 'registry', 'process', 'service', 'task',
        'scheduled', 'startup', 'autostart', 'persistence', 'steganography',
        'network', 'protocol', 'traffic', 'packet', 'connection', 'session',
        'tunnel', 'proxy', 'gateway', 'firewall', 'IDS', 'IPS', 'SIEM',
        'endpoint', 'host', 'server', 'client', 'browser', 'email',
        'analysis', 'forensic', 'investigation', 'detection', 'hunting',
        'monitoring', 'logging', 'signature', 'rule', 'alert', 'event',
        'intelligence', 'attribution', 'clustering', 'correlation'
    ]
}


class KnowledgeGraphExtractor:
    """çŸ¥è¯†å›¾è°±æå–å™¨ - è¿™æ˜¯å”¯ä¸€çš„å®ç°"""

    def __init__(self, kg_config: Dict[str, Any] = None, api_manager: Optional[OpenAIAPIManager] = None):
        """
        åˆå§‹åŒ–æå–å™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        self.api_manager = api_manager

        # ğŸ”¥ ç›´æ¥ä½¿ç”¨dictæ ¼å¼çš„å®ä½“å’Œå…³ç³»å®šä¹‰
        self.entity_types = kg_config.get('entity_types', {})
        self.relationship_types = kg_config.get('relationship_types', {})

        # å…¶ä»–é…ç½®å‚æ•°
        self.batch_size = kg_config.get('batch_size', 5)
        self.max_workers = kg_config.get('max_workers', 3)
        self.enable_threading = kg_config.get('enable_threading', True)
        self.filter_isolated_nodes = kg_config.get(
            'filter_isolated_nodes', True)

        # çº¿ç¨‹é”
        self._lock = threading.Lock()

        logger.info(f"ğŸ”§ çŸ¥è¯†å›¾è°±æå–å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   å®ä½“ç±»å‹: {list(self.entity_types.keys())}")
        logger.info(f"   å…³ç³»ç±»å‹: {list(self.relationship_types.keys())}")
        logger.info(f"   æ‰¹å¤„ç†å¤§å°: {self.batch_size}")
        logger.info(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        logger.info(f"   å¤šçº¿ç¨‹å¤„ç†: {self.enable_threading}")

    def get_entity_description(self, entity_type: str) -> str:
        """è·å–å®ä½“ç±»å‹çš„æè¿°"""
        return self.entity_types.get(entity_type, f"æœªçŸ¥å®ä½“ç±»å‹: {entity_type}")

    def get_relationship_description(self, relationship_type: str) -> str:
        """è·å–å…³ç³»ç±»å‹çš„æè¿°"""
        return self.relationship_types.get(relationship_type, f"æœªçŸ¥å…³ç³»ç±»å‹: {relationship_type}")

    def get_valid_types_summary(self) -> Dict[str, Any]:
        """è·å–æœ‰æ•ˆç±»å‹çš„æ‘˜è¦ä¿¡æ¯"""
        return {
            "valid_entity_types": list(self.entity_types.keys()),
            "valid_relationship_types": list(self.relationship_types.keys()),
            "entity_type_count": len(self.entity_types),
            "relationship_type_count": len(self.relationship_types),
            "entity_descriptions": self.entity_types,
            "relationship_descriptions": self.relationship_types
        }

    def _build_system_prompt(self, content: str, content_type: str = 'text') -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        # ğŸ”¥ ç§»é™¤è´¨é‡æ£€æŸ¥ï¼Œä¸“æ³¨äºæ„å»ºæç¤ºè¯
        guidance = []

        # ç®€åŒ–çš„å†…å®¹ç±»å‹æ£€æµ‹å’ŒæŒ‡å¯¼
        if content_type == 'table':
            guidance.append(_GUIDANCE_TEMPLATES["table"])

        # ç®€åŒ–çš„è¯­è¨€æ£€æµ‹
        chinese_ratio = len(re.findall(
            r'[\u4e00-\u9fff]', content)) / len(content) if content else 0
        english_ratio = len(re.findall(
            r'[a-zA-Z]', content)) / len(content) if content else 0

        if chinese_ratio > 0.6:
            guidance.append(_GUIDANCE_TEMPLATES["zh"])
        elif english_ratio > 0.6:
            guidance.append(_GUIDANCE_TEMPLATES["en"])
        else:
            guidance.append(_GUIDANCE_TEMPLATES["mixed"])

        # ç®€åŒ–çš„é•¿åº¦æŒ‡å¯¼
        if len(content) > 3000:
            guidance.append(_GUIDANCE_TEMPLATES["long"])
        elif len(content) < 500:
            guidance.append(_GUIDANCE_TEMPLATES["short"])

        # ATT&CKç»“æ„æ£€æµ‹
        if re.search(r'###\s+[^#\n]+?\s*\n.*?\*\*[^â€“]+?â€“\s*T\.?\d+', content, re.DOTALL):
            guidance.append(_GUIDANCE_TEMPLATES["attck"])

        guidance_text = '\n'.join(guidance)

        # æ ¹æ®è¯­è¨€é€‰æ‹©ä¸åŒçš„few shotç¤ºä¾‹
        if chinese_ratio > 0.5:
            few_shot_examples = _FEW_SHOT['chinese']
        else:
            few_shot_examples = _FEW_SHOT['english']

        return _BASE_PROMPT_TMPL.format(
            entity_desc=self.entity_types,
            rel_desc=self.relationship_types,
            few_shot_examples=few_shot_examples,
            guidance_text=guidance_text
        )

    def _filter_invalid_types(self, kg_data: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ”¥ ä¿®æ”¹ï¼šä½¿ç”¨é…ç½®çš„dictæ ¼å¼ç±»å‹è¿›è¡Œè¿‡æ»¤"""
        filtered_entities = []
        filtered_relationships = []

        # è·å–æœ‰æ•ˆçš„å®ä½“å’Œå…³ç³»ç±»å‹
        valid_entity_types = set(self.entity_types.keys())
        valid_relationship_types = set(self.relationship_types.keys())

        # è¿‡æ»¤å®ä½“
        for entity in kg_data.get('entities', []):
            entity_type = entity.get('labels', '')  # ç›´æ¥è·å–å­—ç¬¦ä¸²
            if entity_type in valid_entity_types:
                filtered_entities.append(entity)
            else:
                logger.debug(
                    f"è¿‡æ»¤æ— æ•ˆå®ä½“ç±»å‹: {entity_type} (å®ä½“: {entity.get('name', 'Unknown')})")

        # è¿‡æ»¤å…³ç³»
        for relationship in kg_data.get('relationships', []):
            rel_type = relationship.get('type', '')
            if rel_type in valid_relationship_types:
                filtered_relationships.append(relationship)
            else:
                logger.debug(f"è¿‡æ»¤æ— æ•ˆå…³ç³»ç±»å‹: {rel_type}")

        # ç»Ÿè®¡è¿‡æ»¤ç»“æœ
        original_entity_count = len(kg_data.get('entities', []))
        original_relationship_count = len(kg_data.get('relationships', []))
        filtered_entity_count = len(filtered_entities)
        filtered_relationship_count = len(filtered_relationships)

        if original_entity_count > filtered_entity_count:
            logger.info(
                f"ğŸ“Š å®ä½“è¿‡æ»¤: {original_entity_count} -> {filtered_entity_count} (è¿‡æ»¤äº†{original_entity_count - filtered_entity_count}ä¸ª)")

        if original_relationship_count > filtered_relationship_count:
            logger.info(
                f"ğŸ“Š å…³ç³»è¿‡æ»¤: {original_relationship_count} -> {filtered_relationship_count} (è¿‡æ»¤äº†{original_relationship_count - filtered_relationship_count}ä¸ª)")

        # ğŸ”¥ æ–°å¢ï¼šè¿‡æ»¤å­¤ç«‹å®ä½“å’Œæ‚¬æŒ‚å…³ç³»
        final_entities, final_relationships = self._filter_isolated_nodes_and_edges(
            filtered_entities, filtered_relationships)

        return {
            'entities': final_entities,
            'relationships': final_relationships
        }

    def _filter_isolated_nodes_and_edges(self, entities: List[Dict], relationships: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """
        è¿‡æ»¤å­¤ç«‹å®ä½“å’Œæ‚¬æŒ‚å…³ç³»

        Args:
            entities: å®ä½“åˆ—è¡¨
            relationships: å…³ç³»åˆ—è¡¨

        Returns:
            (æœ‰è¿æ¥çš„å®ä½“åˆ—è¡¨, æœ‰æ•ˆçš„å…³ç³»åˆ—è¡¨)
        """
        if not self.filter_isolated_nodes:
            logger.info("ğŸ”§ å­¤ç«‹èŠ‚ç‚¹è¿‡æ»¤å·²ç¦ç”¨ï¼Œè·³è¿‡")
            return entities, relationships
        logger.info(
            f"ğŸ§¹ å¼€å§‹è¿‡æ»¤å­¤ç«‹èŠ‚ç‚¹å’Œæ‚¬æŒ‚è¾¹ï¼Œå®ä½“æ•°: {len(entities)}, å…³ç³»æ•°: {len(relationships)}")

        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å®ä½“ID
        all_entity_ids = set()
        entity_id_to_entity = {}

        for entity in entities:
            entity_id = entity.get('id')
            if entity_id:
                all_entity_ids.add(entity_id)
                entity_id_to_entity[entity_id] = entity

        # ç¬¬äºŒæ­¥ï¼šæ”¶é›†å…³ç³»ä¸­å¼•ç”¨çš„å®ä½“ID
        connected_entity_ids = set()
        valid_relationships = []

        for relationship in relationships:
            source_id = relationship.get('source')
            target_id = relationship.get('target')

            # æ£€æŸ¥å…³ç³»çš„ä¸¤ç«¯å®ä½“æ˜¯å¦éƒ½å­˜åœ¨
            if source_id in all_entity_ids and target_id in all_entity_ids:
                valid_relationships.append(relationship)
                connected_entity_ids.add(source_id)
                connected_entity_ids.add(target_id)
            else:
                logger.debug(f"è¿‡æ»¤æ‚¬æŒ‚å…³ç³»: {source_id} -> {target_id} (å®ä½“ä¸å­˜åœ¨)")

        # ç¬¬ä¸‰æ­¥ï¼šè¿‡æ»¤å‡ºæœ‰è¿æ¥çš„å®ä½“
        connected_entities = []
        isolated_entities = []

        for entity_id in all_entity_ids:
            if entity_id in connected_entity_ids:
                connected_entities.append(entity_id_to_entity[entity_id])
            else:
                isolated_entities.append(entity_id_to_entity[entity_id])
                entity_name = entity_id_to_entity[entity_id].get(
                    'name', 'Unknown')
                logger.debug(f"è¿‡æ»¤å­¤ç«‹å®ä½“: {entity_name} (ID: {entity_id})")

        # ç»Ÿè®¡è¿‡æ»¤ç»“æœ
        original_entity_count = len(entities)
        original_relationship_count = len(relationships)
        final_entity_count = len(connected_entities)
        final_relationship_count = len(valid_relationships)

        isolated_count = len(isolated_entities)
        dangling_rel_count = original_relationship_count - final_relationship_count

        if isolated_count > 0:
            logger.info(f"ğŸ—‘ï¸  è¿‡æ»¤å­¤ç«‹å®ä½“: {isolated_count}ä¸ª")
            if isolated_count <= 5:  # å°‘é‡æ—¶æ˜¾ç¤ºå…·ä½“åç§°
                isolated_names = [e.get('name', 'Unknown')
                                  for e in isolated_entities]
                logger.info(f"   å­¤ç«‹å®ä½“åˆ—è¡¨: {isolated_names}")

        if dangling_rel_count > 0:
            logger.info(f"ğŸ—‘ï¸  è¿‡æ»¤æ‚¬æŒ‚å…³ç³»: {dangling_rel_count}ä¸ª")

        logger.info(
            f"âœ… å­¤ç«‹èŠ‚ç‚¹/è¾¹è¿‡æ»¤å®Œæˆ: å®ä½“ {original_entity_count}â†’{final_entity_count}, å…³ç³» {original_relationship_count}â†’{final_relationship_count}")

        return connected_entities, valid_relationships

    def extract_from_chunks(
        self,
        docs: List[Document],
    ) -> List[Dict[str, Any]]:
        """æå–çŸ¥è¯†å›¾è°± - æ”¯æŒå¤šçº¿ç¨‹å¤„ç†"""
        # ... chunké€‰æ‹©é€»è¾‘ä¿æŒä¸å˜ ...

        if self.enable_threading and len(docs) > 1:
            return self._extract_with_threading(docs)
        else:
            return self._extract_sequential(docs)

    def _extract_with_threading(self, docs: List[Document]) -> List[Dict[str, Any]]:
        """å¤šçº¿ç¨‹æå–çŸ¥è¯†å›¾è°±"""
        logger.info(
            f"ğŸ”¥ å¯ç”¨å¤šçº¿ç¨‹å¤„ç† - çº¿ç¨‹æ•°: {self.max_workers}, chunkæ•°: {len(docs)}")

        results = [None] * len(docs)  # é¢„åˆ†é…ç»“æœåˆ—è¡¨ï¼Œä¿æŒé¡ºåº
        successful_extractions = 0
        failed_extractions = 0

        # åˆ›å»ºçº¿ç¨‹æ± 
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡ï¼Œä¿æŒchunkç´¢å¼•
            future_to_index = {
                executor.submit(self._thread_safe_extract_single_chunk, doc, i): i
                for i, doc in enumerate(docs)
            }

            # æ”¶é›†ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_index):
                chunk_index = future_to_index[future]
                completed_count += 1

                try:
                    result = future.result()

                    # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ç´¢å¼•èµ‹å€¼è€Œä¸æ˜¯append
                    results[chunk_index] = result

                    # çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡æ›´æ–°
                    with self._lock:
                        if result and (len(result.get('entities', [])) > 0 or len(result.get('relationships', [])) > 0):
                            successful_extractions += 1
                            logger.debug(
                                f"âœ… Chunk {chunk_index+1} æå–æˆåŠŸ ({completed_count}/{len(docs)})")
                        else:
                            failed_extractions += 1
                            logger.debug(
                                f"âš ï¸ Chunk {chunk_index+1} æå–ä¸ºç©º ({completed_count}/{len(docs)})")

                except Exception as e:
                    logger.error(f"âŒ Chunk {chunk_index+1} å¤„ç†å¼‚å¸¸: {e}")
                    results[chunk_index] = {
                        "entities": [], "relationships": []}
                    with self._lock:
                        failed_extractions += 1

                # è¿›åº¦æç¤º
                if completed_count % 5 == 0 or completed_count == len(docs):
                    logger.info(
                        f"ğŸ”„ å¤šçº¿ç¨‹è¿›åº¦: {completed_count}/{len(docs)} ({completed_count/len(docs)*100:.1f}%)")

        # ğŸ”¥ ç¡®ä¿æ²¡æœ‰Noneå€¼
        results = [r if r is not None else {
            "entities": [], "relationships": []} for r in results]

        # æŒ‰chunkç´¢å¼•æ’åº
        results.sort(key=lambda x: x.get('chunk_index', 0))

        success_rate = (successful_extractions /
                        len(docs) * 100) if docs else 0

        logger.info(f"ğŸ¯ å¤šçº¿ç¨‹çŸ¥è¯†å›¾è°±æå–å®Œæˆ:")
        logger.info(f"   - æ€»chunkæ•°: {len(docs)}")
        logger.info(f"   - æˆåŠŸæå–: {successful_extractions}")
        logger.info(f"   - å¤±è´¥/è·³è¿‡: {failed_extractions}")
        logger.info(f"   - æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"   - ä½¿ç”¨çº¿ç¨‹æ•°: {self.max_workers}")

        return results

    def _thread_safe_extract_single_chunk(self, chunk: Document, chunk_index: int) -> Dict[str, Any]:
        """çº¿ç¨‹å®‰å…¨çš„å•chunkæå–"""
        try:
            # æ·»åŠ çº¿ç¨‹IDåˆ°æ—¥å¿—
            thread_id = threading.current_thread().ident
            logger.debug(f"ğŸ§µ çº¿ç¨‹{thread_id} å¼€å§‹å¤„ç† Chunk {chunk_index+1}")
            result = self._extract_from_single_chunk(
                chunk, chunk_index)

            logger.debug(f"ğŸ§µ çº¿ç¨‹{thread_id} å®Œæˆ Chunk {chunk_index+1}")
            return result

        except Exception as e:
            logger.error(f"ğŸ§µ çº¿ç¨‹å¼‚å¸¸ Chunk {chunk_index+1}: {e}")
            return {"entities": [], "relationships": []}

    def _extract_sequential(self, docs: List[Document]) -> List[Dict[str, Any]]:
        """åŸæœ‰çš„ä¸²è¡Œå¤„ç†æ–¹æ³•"""
        logger.info(f"ğŸ“ ä½¿ç”¨ä¸²è¡Œå¤„ç† - chunkæ•°: {len(docs)}")

        results = []
        successful_extractions = 0
        failed_extractions = 0

        for i, doc in enumerate(docs):
            try:
                result = self._extract_from_single_chunk(doc, i)
                results.append(result)

                if result and (len(result.get('entities', [])) > 0 or len(result.get('relationships', [])) > 0):
                    successful_extractions += 1
                    logger.debug(f"âœ… Chunk {i+1} æå–æˆåŠŸ")
                else:
                    failed_extractions += 1
                    logger.debug(f"âš ï¸ Chunk {i+1} æå–ä¸ºç©º")

            except Exception as e:
                logger.error(f"âŒ Chunk {i+1} å¤„ç†å¼‚å¸¸: {e}")
                results.append({"entities": [], "relationships": []})
                failed_extractions += 1

        success_rate = (successful_extractions /
                        len(docs) * 100) if docs else 0

        logger.info(f"ğŸ“Š ä¸²è¡ŒçŸ¥è¯†å›¾è°±æå–å®Œæˆ:")
        logger.info(f"   - æ€»chunkæ•°: {len(docs)}")
        logger.info(f"   - æˆåŠŸæå–: {successful_extractions}")
        logger.info(f"   - å¤±è´¥/è·³è¿‡: {failed_extractions}")
        logger.info(f"   - æˆåŠŸç‡: {success_rate:.1f}%")

        return results

    def _extract_from_single_chunk(self, chunk: Document, chunk_index: int) -> Dict[str, Any]:
        """ä»å•ä¸ªchunkæå–çŸ¥è¯†å›¾è°±"""
        chunk_content = chunk.page_content
        chunk_type = chunk.metadata.get('content_type', 'unknown')
        chunk_id = chunk.metadata.get('chunk_id', f'chunk_{chunk_index}')
        logger.info(
            f"ğŸ” å¼€å§‹å¤„ç† Chunk {chunk_id} \n å†…å®¹æ‘˜è¦: {chunk_content[:200]}...")

        try:
            if not chunk_content or not chunk_content.strip():
                logger.warning(f"âš ï¸ Chunk {chunk_id} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return {"entities": [], "relationships": []}

            # æ„å»ºæç¤ºè¯
            prompt = self._build_system_prompt(chunk_content, chunk_type)

            # è°ƒç”¨API
            messages = [
                {"role": "system",
                    "content": prompt},
                {"role": "user", "content": chunk_content}
            ]

            # ç›´æ¥ä½¿ç”¨api_managerçš„å±æ€§
            response = self.api_manager.call_api(
                messages=messages,
                model=self.api_manager.model,  # ç›´æ¥ä½¿ç”¨api_managerçš„å±æ€§
                temperature=self.api_manager.temperature,  # å›ºå®šæ¸©åº¦å‚æ•°
                max_tokens=self.api_manager.max_tokens,
                timeout=self.api_manager.timeout,
                top_p=self.api_manager.top_p,
                frequency_penalty=self.api_manager.frequency_penalty,
                presence_penalty=self.api_manager.presence_penalty
            )

            # ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
            if not isinstance(response, str):
                logger.warning(
                    f"âš ï¸ Chunk {chunk_id} APIè¿”å›éå­—ç¬¦ä¸²ç±»å‹: {type(response)}")
                response = str(response)

            # æ¸…ç†å’Œç©ºæ£€æŸ¥
            response = response.strip()
            if not response:
                logger.warning(f"âš ï¸ Chunk {chunk_id} GPTå“åº”ä¸ºç©º")
                extracted_data = {"entities": [], "relationships": []}
            else:
                logger.debug(f"ğŸ¤– Chunk {chunk_id} å“åº”é•¿åº¦: {len(response)}")

                # ç›´æ¥è°ƒç”¨è§£ææ–¹æ³•
                extracted_data = GPTResponseParser.parse_knowledge_graph_result(
                    response=response,
                    api_manager=self.api_manager
                )

            logger.debug(
                f"ğŸ“Š Chunk {chunk_id} è§£æç»“æœ: å®ä½“{len(extracted_data.get('entities', []))}, å…³ç³»{len(extracted_data.get('relationships', []))}")

            # è¿‡æ»¤æ— æ•ˆç±»å‹ å­¤ç«‹èŠ‚ç‚¹å’Œæ‚¬æŒ‚å…³ç³»
            filtered_data = self._filter_invalid_types(extracted_data)

            # ğŸ”¥ æ–°å¢ï¼šä¸ºæ¯ä¸ªå®ä½“å’Œå…³ç³»æ·»åŠ chunkä¿¡æ¯
            chunk_info = {
                'chunk_id': chunk_id,
                'chunk_type': chunk_type,
                'chunk_content': chunk_content,
                'chunk_length': len(chunk_content),
                'source_metadata': chunk.metadata if hasattr(chunk, 'metadata') else {}
            }

            # ä¸ºå®ä½“æ·»åŠ chunkä¿¡æ¯
            for entity in filtered_data.get('entities', []):
                if 'chunks_info' not in entity:
                    entity['chunks_info'] = []
                entity['chunks_info'].append(chunk_info)

            # ä¸ºå…³ç³»æ·»åŠ chunkä¿¡æ¯
            for relationship in filtered_data.get('relationships', []):
                if 'chunks_info' not in relationship:
                    relationship['chunks_info'] = []
                relationship['chunks_info'].append(chunk_info)

            # è¯¦ç»†çš„ç»“æœå¯¹æ¯”
            original_entities = len(extracted_data.get('entities', []))
            original_relationships = len(
                extracted_data.get('relationships', []))
            filtered_entities = len(filtered_data.get('entities', []))
            filtered_relationships = len(
                filtered_data.get('relationships', []))

            logger.debug(f"ğŸ¯ Chunk {chunk_id} è¿‡æ»¤ç»“æœ:")
            logger.debug(f"   å®ä½“: {original_entities} -> {filtered_entities}")
            logger.debug(
                f"   å…³ç³»: {original_relationships} -> {filtered_relationships}")

            # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œè®°å½•åŸå› 
            if filtered_entities == 0 and filtered_relationships == 0:
                if original_entities > 0 or original_relationships > 0:
                    logger.warning(f"âš ï¸ Chunk {chunk_id} æ‰€æœ‰å†…å®¹è¢«è¿‡æ»¤æ‰äº†ï¼")
                    logger.warning(f"   åŸå§‹GPTå“åº”å‰500å­—ç¬¦: {response[:500]}...")
                else:
                    logger.warning(f"âš ï¸ Chunk {chunk_id} GPTæœªæå–åˆ°ä»»ä½•å†…å®¹")
                    logger.warning(f"   å†…å®¹å‰500å­—ç¬¦: {chunk_content[:500]}...")

                    # ğŸ”¥ æ·»åŠ ï¼šåˆ†æä¸ºä»€ä¹ˆæ²¡æœ‰æå–åˆ°å†…å®¹
                    self._analyze_extraction_failure(
                        chunk_content, response)

            logger.info(
                f"âœ… Chunk {chunk_id} å¤„ç†å®Œæˆï¼Œå·²ä»chunkä¸­æŠ½å– {filtered_entities} ä¸ªå®ä½“å’Œ {filtered_relationships} ä¸ªå…³ç³»")
            return filtered_data

        except Exception as e:
            logger.error(f"âŒ Chunk {chunk_id} å¤„ç†å¤±è´¥: {e}")
            logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            return {"entities": [], "relationships": []}

    def _analyze_extraction_failure(self, chunk_content: str, gpt_response: str) -> str:
        """ç®€åŒ–ç‰ˆå¤±è´¥åŸå› åˆ†æ"""
        try:
            # å¿«é€Ÿæ£€æŸ¥å†…å®¹è´¨é‡
            if len(chunk_content.strip()) < 100:
                return "å†…å®¹è¿‡çŸ­"

            # æ£€æŸ¥å…³é”®è¯å¯†åº¦
            content_lower = chunk_content.lower()
            keyword_count = sum(1 for keyword in SECURITY_KEYWORDS['chinese'] + SECURITY_KEYWORDS['english']
                                if keyword.lower() in content_lower)

            if keyword_count < 2:
                return "ç¼ºä¹å®‰å…¨å…³é”®è¯"

            # æ£€æŸ¥æ˜¯å¦ä¸ºå¯¼èˆªå†…å®¹
            nav_indicators = ['é¦–é¡µ', 'ç™»å½•', 'æ³¨å†Œ', 'home', 'about', 'contact']
            if sum(1 for nav in nav_indicators if nav in content_lower) >= 2:
                return "ç–‘ä¼¼å¯¼èˆªå†…å®¹"

            # æ£€æŸ¥GPTå“åº”
            if not gpt_response or len(gpt_response.strip()) < 10:
                return "GPTæ— å“åº”"

            if not ('"entities"' in gpt_response and '"relationships"' in gpt_response):
                return "å“åº”æ ¼å¼é”™è¯¯"

            # å¦‚æœéƒ½æ­£å¸¸ä½†ä»å¤±è´¥
            return "è§£æå¤±è´¥"

        except Exception:
            return "åˆ†æå¼‚å¸¸"
