import json
import logging
import os
from typing import Dict, Any, List, Tuple, Optional, Set
from utils import OpenAIAPIManager, GPTResponseParser

logger = logging.getLogger(__name__)
# ---------- å®ä½“å¯¹é½æç¤ºè¯æ¨¡æ¿ ----------
_ENTITY_ALIGNMENT_PROMPT_TMPL = """åˆå¹¶ä»¥ä¸‹{entity_type}å®ä½“ä¸­çš„ç›¸åŒæ¦‚å¿µã€‚

å®ä½“ä¿¡æ¯ï¼š
{entity_details}

ç¤ºä¾‹ï¼š
è¾“å…¥: [
  {{0: "name": "APT1", "description": "ä¸­å›½é«˜çº§æŒç»­å¨èƒç»„ç»‡ï¼Œåˆç§°æ³¨é‡Šå°ç»„"}},
  {{1: "name": "Advanced Persistent Threat 1", "description": "ä¸­å›½ç½‘ç»œé—´è°ç»„ç»‡ï¼Œä¸“æ³¨äºçªƒå–çŸ¥è¯†äº§æƒ"}},
  {{2: "name": "Comment Crew", "description": "APT1çš„åˆ«åï¼Œå› åœ¨æ¶æ„è½¯ä»¶ä¸­ç•™ä¸‹æ³¨é‡Šè€Œå¾—å"}},
  {{3: "name": "SSH", "description": "å®‰å…¨å¤–å£³åè®®ï¼Œç”¨äºåŠ å¯†ç½‘ç»œè¿æ¥"}},
  {{4: "name": "Secure Shell", "description": "SSHåè®®çš„å…¨ç§°ï¼Œæä¾›å®‰å…¨çš„è¿œç¨‹è®¿é—®"}},
  {{5: "name": "ssh", "description": "SSHåè®®çš„å°å†™å½¢å¼"}}
]
è¾“å‡º: {{"merge_groups": [[0,1,2], [3,4,5]]}}

åˆå¹¶è§„åˆ™ï¼š
- ç¼©å†™ä¸å…¨ç§°ï¼šSSH â†” Secure Shell
- åŒä¹‰è¯/åˆ«åï¼šAPT1 â†” Comment Crew
- å¤§å°å†™å˜ä½“ï¼šSSH â†” ssh
- æè¿°ç›¸ä¼¼çš„åŒä¸€æ¦‚å¿µåˆå¹¶
- åŸºäºnameå’Œdescriptionç»¼åˆåˆ¤æ–­ï¼Œç›¸åŒæ¦‚å¿µä¸åŒè¡¨è¿°åˆå¹¶ï¼Œå®Œå…¨ä¸åŒæ¦‚å¿µåˆ†å¼€

âš ï¸ é‡è¦çº¦æŸï¼š
- æ¯ä¸ªç´¢å¼•åªèƒ½å‡ºç°åœ¨ä¸€ä¸ªç»„ä¸­ï¼Œä¸å…è®¸é‡å¤
- åªåˆå¹¶çœŸæ­£ç›¸ä¼¼çš„æ¦‚å¿µï¼Œä¸ç¡®å®šæ—¶å®å¯ä¸åˆå¹¶
- è¾“å‡ºå¿…é¡»æ˜¯å®Œæ•´æœ‰æ•ˆçš„JSONæ ¼å¼

æ³¨æ„ï¼šæ‰€æœ‰ç´¢å¼•å¿…é¡» â‰¤ {entity_count}

ç›´æ¥è¿”å›JSONæ ¼å¼çš„stringï¼Œä¸èƒ½åŒ…å«ä»£ç å—æç¤ºï¼š
{{"merge_groups": [[ç´¢å¼•ç»„1], [ç´¢å¼•ç»„2], ...]}}"""


class EntityAligner:
    """å®ä½“å¯¹é½å™¨ - ä¸“é—¨è´Ÿè´£å®ä½“å¯¹é½"""

    def __init__(self, config: Dict[str, Any], api_manager: Optional[OpenAIAPIManager] = None):
        self.config = config
        self.api_manager = api_manager

        # å¯¹é½å‚æ•°
        alignment_config = config.get(
            'graph_processor', {}).get('entity_alignment', {})
        self.similarity_threshold = alignment_config.get(
            'similarity_threshold', 0.75)
        self.enable_contains_match = alignment_config.get(
            'enable_contains_match', True)
        self.enable_acronym_match = alignment_config.get(
            'enable_acronym_match', True)

    def align_entities(self, entities: List[Dict]) -> Tuple[List[Dict], Dict[str, str], Dict[str, str]]:
        """
        å¯¹é½å®ä½“ï¼Œè¿”å›ä¸‰å…ƒç»„ï¼š(å¯¹é½åå®ä½“åˆ—è¡¨, åç§°æ˜ å°„, IDæ˜ å°„)

        Returns:
            Tuple[å¯¹é½åå®ä½“åˆ—è¡¨, åŸå§‹åç§°->å¯¹é½ååç§°çš„æ˜ å°„, åŸå§‹ID->å¯¹é½åIDçš„æ˜ å°„]
        """
        logger.info(f"ğŸ”„ å¼€å§‹å®ä½“å¯¹é½ï¼ŒåŸå§‹å®ä½“æ•°: {len(entities)}")

        # ä¼˜å…ˆå°è¯•å¤§æ¨¡å‹å¯¹é½
        if self.api_manager:
            try:
                return self._gpt_align(entities)
            except Exception as e:
                logger.warning(f"âš ï¸ å¤§æ¨¡å‹å¯¹é½å¤±è´¥: {e}ï¼Œé™çº§åˆ°è§„åˆ™å¯¹é½")

        # å¤‡é€‰æ–¹æ¡ˆï¼šè§„åˆ™å¯¹é½
        return self._rule_align(entities)

    def _gpt_align(self, entities: List[Dict]) -> Tuple[List[Dict], Dict[str, str], Dict[str, str]]:
        """å¤§æ¨¡å‹å®ä½“å¯¹é½ï¼Œè¿”å›ä¸‰å…ƒç»„"""
        # æŒ‰ç±»å‹åˆ†ç»„
        entities_by_type = {}
        for entity in entities:
            entity_type = entity.get('type', 'Unknown')
            if entity_type not in entities_by_type:
                entities_by_type[entity_type] = []
            entities_by_type[entity_type].append(entity)

        aligned_entities = []
        name_mapping = {}
        id_mapping = {}  # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿æ€»æ˜¯æœ‰IDæ˜ å°„

        for entity_type, type_entities in entities_by_type.items():
            if len(type_entities) <= 1:
                aligned_entities.extend(type_entities)
                for entity in type_entities:
                    name = entity.get('name', '')
                    entity_id = entity.get('id', '')
                    if name:
                        name_mapping[name] = name
                    if entity_id:
                        id_mapping[entity_id] = entity_id
                        self._add_id_variants(entity_id, entity_id, id_mapping)
                continue

            # ç¡®ä¿è¿”å›ä¸‰ä¸ªå€¼
            aligned_type_entities, type_name_mapping, type_id_mapping = self._gpt_align_by_type(
                entity_type, type_entities)
            aligned_entities.extend(aligned_type_entities)
            name_mapping.update(type_name_mapping)
            id_mapping.update(type_id_mapping)  # æ›´æ–°IDæ˜ å°„

        # ğŸ”¥ ä¿®å¤ï¼šè¿”å›ä¸‰ä¸ªå€¼
        return aligned_entities, name_mapping, id_mapping

    def _gpt_align_by_type(self, entity_type: str, entities: List[Dict]) -> Tuple[List[Dict], Dict[str, str], Dict[str, str]]:
        """å¯¹ç‰¹å®šç±»å‹çš„å®ä½“è¿›è¡Œå¤§æ¨¡å‹å¯¹é½ï¼Œè¿”å›ä¸‰å…ƒç»„"""
        if len(entities) <= 1:
            entity = entities[0] if entities else {}
            name = entity.get('name', '')
            entity_id = entity.get('id', '')
            name_mapping = {name: name} if name else {}
            id_mapping = {entity_id: entity_id} if entity_id else {}
            if entity_id:
                self._add_id_variants(entity_id, entity_id, id_mapping)
            return entities, name_mapping, id_mapping

        # ğŸ”¥ æ„å»ºå®ä½“ä¿¡æ¯æ—¶è®°å½•ç´¢å¼•æ˜ å°„
        entity_details = []
        entity_count = len(entities)  # è®°å½•å®é™…å®ä½“æ•°é‡
        for i, entity in enumerate(entities):
            name = entity.get('name', '')
            description = entity.get('description', '')
            entity_id = entity.get('id', f'entity_{i}')

            # ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šæˆªæ–­è¿‡é•¿çš„æè¿°ä»¥å‡å°‘promptå¤§å°
            if len(description) > 100:  # é™åˆ¶æè¿°é•¿åº¦
                description = description[:97] + "..."
            if len(name) > 50:  # é™åˆ¶åç§°é•¿åº¦
                name = name[:47] + "..."

            entity_info = {
                "name": name,
                "description": description or "æ— æè¿°",
                "id": entity_id
            }

            # æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
            entity_detail = f"{{{i}: \"name\": \"{entity_info['name']}\", \"description\": \"{entity_info['description']}\"}}"
            entity_details.append(entity_detail)

        # ğŸ”¥ ä½¿ç”¨æ”¹è¿›çš„æ¨¡æ¿
        entity_info_str = ",\n  ".join(entity_details)

        # ğŸ”¥ ç³»ç»Ÿæ¶ˆæ¯ä¼˜åŒ– - æ›´åŠ å…·ä½“
        system_content = f"""ä½ æ˜¯ä¸“ä¸šçš„å®ä½“å¯¹é½ä¸“å®¶ï¼Œèƒ½å¤Ÿæ ¹æ®å®ä½“çš„åç§°å’Œæè¿°å‡†ç¡®è¯†åˆ«åŒä¹‰å®ä½“ã€‚é‡ç‚¹å…³æ³¨ï¼š
1) åç§°çš„ç¼©å†™å…³ç³»
2) æè¿°çš„è¯­ä¹‰ç›¸ä¼¼æ€§  
3) åŒä¸€æ¦‚å¿µçš„ä¸åŒè¡¨è¿°æ–¹å¼

åˆå¹¶è§„åˆ™ï¼š
- ç¼©å†™ä¸å…¨ç§°ï¼šSSH â†” Secure Shell
- åŒä¹‰è¯/åˆ«åï¼šAPT1 â†” Comment Crew
- å¤§å°å†™å˜ä½“ï¼šSSH â†” ssh
- æè¿°ç›¸ä¼¼çš„åŒä¸€æ¦‚å¿µåˆå¹¶
- åŸºäºnameå’Œdescriptionç»¼åˆåˆ¤æ–­ï¼Œç›¸åŒæ¦‚å¿µä¸åŒè¡¨è¿°åˆå¹¶ï¼Œå®Œå…¨ä¸åŒæ¦‚å¿µåˆ†å¼€

âš ï¸ é‡è¦çº¦æŸï¼š
- æ¯ä¸ªç´¢å¼•åªèƒ½å‡ºç°åœ¨ä¸€ä¸ªç»„ä¸­ï¼Œä¸å…è®¸é‡å¤
- åªåˆå¹¶çœŸæ­£ç›¸ä¼¼çš„æ¦‚å¿µï¼Œä¸ç¡®å®šæ—¶å®å¯ä¸åˆå¹¶
- è¾“å‡ºå¿…é¡»æ˜¯å®Œæ•´æœ‰æ•ˆçš„JSONæ ¼å¼

æ³¨æ„ï¼šæ‰€æœ‰ç´¢å¼•å¿…é¡» â‰¤ {entity_count-1}

å®ä½“ç±»å‹ï¼š{entity_type}"""

        messages = [
            {
                "role": "system",
                "content": system_content
            },
            {"role": "user",
             "content": f"""è¯·åˆå¹¶ä»¥ä¸‹å®ä½“ä¸­çš„ç›¸åŒæ¦‚å¿µï¼š

å®ä½“ä¿¡æ¯ï¼š
{entity_info_str}

ç›´æ¥è¿”å›JSONæ ¼å¼çš„stringï¼Œä¸èƒ½åŒ…å«ä»£ç å—æç¤ºï¼š
{{"merge_groups": [[ç´¢å¼•ç»„1], [ç´¢å¼•ç»„2], ...]}}"""}
        ]

        logger.info(f"ğŸš€ è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œå®ä½“ç±»å‹: {entity_type}ï¼Œå®ä½“æ•°é‡: {len(entities)}")

        # APIè°ƒç”¨ä¿æŒä¸å˜
        response = self.api_manager.call_api(
            messages=messages,
            model=self.api_manager.model,
            temperature=self.api_manager.temperature,
            max_tokens=self.api_manager.max_tokens,
            timeout=self.api_manager.timeout,
            top_p=self.api_manager.top_p,
            frequency_penalty=self.api_manager.frequency_penalty,
            presence_penalty=self.api_manager.presence_penalty
        )

        # è§£æå¯¹é½ç»“æœ
        return self._parse_align_response(entities, response)

    def _parse_align_response(self, entities: List[Dict], response: str) -> Tuple[List[Dict], Dict[str, str], Dict[str, str]]:
        """è§£æå¤§æ¨¡å‹çš„å¯¹é½å“åº”ï¼Œè¿”å›ä¸‰å…ƒç»„ï¼š(å¯¹é½å®ä½“, åç§°æ˜ å°„, IDæ˜ å°„)"""
        try:
            merge_groups = GPTResponseParser.parse_merge_groups(
                response, self.api_manager)

            if not merge_groups:
                entity_names = [e.get('name', 'Unknown') for e in entities]
                entity_type = entities[0].get('type', 'Unknown')
                logger.warning(f"âš ï¸ æœªè§£æåˆ°æœ‰æ•ˆçš„åˆå¹¶ç»„ï¼Œä½¿ç”¨åŸå§‹å®ä½“")
                logger.warning(
                    f"ğŸ“‹ åŸå§‹å®ä½“ç±»å‹: {entity_type}, å®ä½“åˆ—è¡¨ ({len(entity_names)}ä¸ª): {entity_names}")

                # ğŸ”¥ ä¿®å¤ï¼šåˆ›å»ºå®Œæ•´çš„ä¸‰å…ƒç»„æ˜ å°„å…³ç³»
                name_mapping = {}
                id_mapping = {}
                for entity in entities:
                    name = entity.get('name', '')
                    entity_id = entity.get('id', '')
                    if name:
                        name_mapping[name] = name
                    if entity_id:
                        id_mapping[entity_id] = entity_id
                        self._add_id_variants(entity_id, entity_id, id_mapping)

                return entities, name_mapping, id_mapping

            # æ‰§è¡Œåˆå¹¶
            aligned_entities = []
            name_mapping = {}
            id_mapping = {}  # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿æ€»æ˜¯åˆ›å»ºIDæ˜ å°„
            processed_indices = set()

            for group in merge_groups:
                if any(idx in processed_indices for idx in group):
                    continue

                group_entities = []
                for idx in group:
                    if 0 <= idx < len(entities):
                        group_entities.append(entities[idx])
                        processed_indices.add(idx)

                if group_entities:
                    merged_entity = self._merge_entities(group_entities)
                    aligned_entities.append(merged_entity)

                    merged_name = merged_entity.get('name', '')
                    merged_id = merged_entity.get('id', '')

                    for entity in group_entities:
                        original_name = entity.get('name', '')
                        original_id = entity.get('id', '')

                        if original_name:
                            name_mapping[original_name] = merged_name

                        if original_id and merged_id:
                            id_mapping[original_id] = merged_id
                            self._add_id_variants(
                                original_id, merged_id, id_mapping)

            # å¤„ç†æœªåˆ†ç»„çš„å®ä½“
            for idx, entity in enumerate(entities):
                if idx not in processed_indices:
                    aligned_entities.append(entity)
                    name = entity.get('name', '')
                    entity_id = entity.get('id', '')

                    if name:
                        name_mapping[name] = name
                    if entity_id:
                        id_mapping[entity_id] = entity_id
                        self._add_id_variants(entity_id, entity_id, id_mapping)

            self._validate_id_mapping(entities, id_mapping)

            return aligned_entities, name_mapping, id_mapping

        except Exception as e:
            logger.error(f"âŒ è§£æGPTå¯¹é½å“åº”å¤±è´¥: {e}")
            return self._rule_align(entities)

    def _add_id_variants(self, original_id: str, mapped_id: str, id_mapping: Dict[str, str]):
        """ä¸ºIDæ·»åŠ å„ç§å˜ä½“æ˜ å°„"""
        if not original_id or not mapped_id:
            return

        # æ·»åŠ å¤§å°å†™å˜ä½“
        variants = [
            original_id.lower(),
            original_id.upper(),
            original_id.capitalize()
        ]

        # å¤„ç†æ ¼å¼å˜ä½“ (å¦‚: Techniques--T1059 <-> techniques--t1059)
        if '--' in original_id:
            parts = original_id.split('--', 1)
            if len(parts) == 2:
                prefix, suffix = parts
                variants.extend([
                    f"{prefix.lower()}--{suffix.lower()}",
                    f"{prefix.upper()}--{suffix.upper()}",
                    f"{prefix.capitalize()}--{suffix}",
                    f"{prefix.capitalize()}--{suffix.lower()}",
                    f"{prefix.capitalize()}--{suffix.upper()}"
                ])

        # æ·»åŠ æ‰€æœ‰å˜ä½“åˆ°æ˜ å°„ä¸­
        for variant in variants:
            if variant not in id_mapping:  # é¿å…è¦†ç›–å·²æœ‰æ˜ å°„
                id_mapping[variant] = mapped_id

    def _validate_id_mapping(self, entities: List[Dict], id_mapping: Dict[str, str]):
        """éªŒè¯IDæ˜ å°„å®Œæ•´æ€§"""
        # æ”¶é›†æ‰€æœ‰åŸå§‹ID
        original_ids = {e.get('id') for e in entities if e.get('id')}

        # æ£€æŸ¥å“ªäº›IDæ²¡æœ‰æ˜ å°„
        unmapped_ids = []
        for entity_id in original_ids:
            if entity_id not in id_mapping:
                unmapped_ids.append(entity_id)

        if unmapped_ids:
            logger.error(f"âŒ å‘ç°{len(unmapped_ids)}ä¸ªæœªæ˜ å°„çš„å®ä½“ID:")
            for uid in unmapped_ids[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.error(f"  - {uid}")
            if len(unmapped_ids) > 5:
                logger.error(f"  ... è¿˜æœ‰{len(unmapped_ids) - 5}ä¸ª")

        logger.info(f"ğŸ“Š IDæ˜ å°„ç»Ÿè®¡: åŸå§‹å®ä½“{len(entities)}ä¸ª, æ˜ å°„é¡¹{len(id_mapping)}ä¸ª")

    def _rule_align(self, entities: List[Dict]) -> Tuple[List[Dict], Dict[str, str], Dict[str, str]]:
        """åŸºäºè§„åˆ™çš„å®ä½“å¯¹é½ï¼Œè¿”å›ä¸‰å…ƒç»„"""
        logger.info("ğŸ“ ä½¿ç”¨è§„åˆ™å¯¹é½")

        aligned_entities = []
        name_mapping = {}
        id_mapping = {}
        processed_names = set()

        for entity in entities:
            name = entity.get('name', '')
            entity_id = entity.get('id', '')

            if name in processed_names:
                continue

            # æ‰¾åˆ°ç›¸ä¼¼çš„å®ä½“
            similar_entities = self._find_similar_entities(entity, entities)

            # åˆ›å»ºåˆå¹¶åçš„å®ä½“
            merged_entity = self._merge_entities(similar_entities)
            aligned_entities.append(merged_entity)

            # å»ºç«‹æ˜ å°„å…³ç³»
            aligned_name = merged_entity.get('name', '')
            aligned_id = merged_entity.get('id', '')

            for similar_entity in similar_entities:
                original_name = similar_entity.get('name', '')
                original_id = similar_entity.get('id', '')

                if original_name:
                    name_mapping[original_name] = aligned_name
                    processed_names.add(original_name)

                if original_id and aligned_id:
                    id_mapping[original_id] = aligned_id
                    # æ·»åŠ å˜ä½“æ˜ å°„
                    self._add_id_variants(original_id, aligned_id, id_mapping)

        logger.info(f"âœ… è§„åˆ™å¯¹é½å®Œæˆ: {len(entities)} -> {len(aligned_entities)}")
        return aligned_entities, name_mapping, id_mapping

    def _find_similar_entities(self, target_entity: Dict, all_entities: List[Dict]) -> List[Dict]:
        """æ‰¾åˆ°ä¸ç›®æ ‡å®ä½“ç›¸ä¼¼çš„æ‰€æœ‰å®ä½“"""
        target_name = target_entity.get('name', '').lower()
        target_type = target_entity.get('type', '')

        similar_entities = [target_entity]

        for entity in all_entities:
            if entity == target_entity:
                continue

            name = entity.get('name', '').lower()
            entity_type = entity.get('type', '')

            # åŒç±»å‹ä¸”ç›¸ä¼¼
            if entity_type == target_type and self._are_names_similar(target_name, name):
                similar_entities.append(entity)

        return similar_entities

    def _are_names_similar(self, name1: str, name2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªåç§°æ˜¯å¦ç›¸ä¼¼"""
        if name1 == name2:
            return True

        if self.enable_contains_match:
            if name1 in name2 or name2 in name1:
                return True

        if self.enable_acronym_match:
            # æ£€æŸ¥ç¼©å†™åŒ¹é…
            if self._is_acronym_match(name1, name2):
                return True

        # ç®€å•çš„å­—ç¬¦ç›¸ä¼¼åº¦
        common_chars = len(set(name1) & set(name2))
        total_chars = len(set(name1) | set(name2))
        similarity = common_chars / total_chars if total_chars > 0 else 0.0

        return similarity >= self.similarity_threshold

    def _is_acronym_match(self, name1: str, name2: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç¼©å†™åŒ¹é…"""
        words1 = name1.split()
        words2 = name2.split()

        # ä¸€ä¸ªæ˜¯å•è¯ï¼Œä¸€ä¸ªæ˜¯å¤šä¸ªå•è¯çš„é¦–å­—æ¯
        if len(words1) == 1 and len(words2) > 1:
            acronym = ''.join(w[0].lower() for w in words2 if w)
            return words1[0].lower() == acronym

        if len(words2) == 1 and len(words1) > 1:
            acronym = ''.join(w[0].lower() for w in words1 if w)
            return words2[0].lower() == acronym

        return False

    def _merge_entities(self, entities: List[Dict]) -> Dict:
        """åˆå¹¶å¤šä¸ªå®ä½“ä¸ºä¸€ä¸ªå®ä½“ï¼Œä¿ç•™chunkä¿¡æ¯"""
        if not entities:
            return {}

        if len(entities) == 1:
            entity = entities[0].copy()
            # ç¡®ä¿å®ä½“æœ‰æ­£ç¡®çš„IDæ ¼å¼
            if 'id' in entity:
                entity_id = entity['id']
                # ä¿®å¤IDæ ¼å¼ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å‰ç¼€
                if '--' not in entity_id or entity_id.startswith('unknown--'):
                    entity_type = entity.get('type', '').lower()
                    entity_name = entity.get(
                        'name', '').lower().replace(' ', '')
                    entity['id'] = f"{entity_type}--{entity_name}"
            return entity

        # é€‰æ‹©æœ€ä½³åç§°ï¼ˆæœ€é•¿çš„éç©ºåç§°ï¼‰
        best_name = max((e.get('name', '') for e in entities),
                        key=len) or entities[0].get('name', '')

        # é€‰æ‹©æœ€ä½³ç±»å‹
        best_type = entities[0].get('type', '')

        # åˆå¹¶æè¿°
        descriptions = [e.get('description', '')
                        for e in entities if e.get('description')]
        merged_description = '; '.join(
            set(descriptions)) if descriptions else ''

        # æ”¶é›†æ‰€æœ‰åŸå§‹åç§°
        original_names = []
        for entity in entities:
            name = entity.get('name', '')
            if name and name not in original_names:
                original_names.append(name)

        # æ”¶é›†æ‰€æœ‰chunkä¿¡æ¯
        all_chunks_info = []
        seen_chunk_indices = set()

        for entity in entities:
            chunks_info = entity.get('chunks_info', [])
            for chunk_info in chunks_info:
                chunk_index = chunk_info.get('chunk_index')
                if chunk_index is not None and chunk_index not in seen_chunk_indices:
                    all_chunks_info.append(chunk_info)
                    seen_chunk_indices.add(chunk_index)

        # ç”Ÿæˆæ­£ç¡®çš„å®ä½“ID
        entity_type_lower = best_type.lower()
        entity_name_normalized = best_name.lower().replace(' ', '').replace('-', '')
        merged_id = f"{entity_type_lower}--{entity_name_normalized}"

        merged_entity = {
            'name': best_name,
            'type': best_type,
            'id': merged_id,
            'description': merged_description,
            'labels': best_type,
            'properties': {
                'id': merged_id,
                'name': best_name,
                'entity_type': entity_type_lower,
                'merge_count': len(entities),
                'description': merged_description,
                'chunks_info': all_chunks_info
            },
            'chunks_info': all_chunks_info
        }

        # å¦‚æœæœ‰å¤šä¸ªåŸå§‹åç§°ï¼Œè®°å½•å®ƒä»¬
        if len(original_names) > 1:
            merged_entity['properties']['original_names'] = original_names

        return merged_entity


class EnhancedGraphDataProcessor:
    """ç®€åŒ–çš„å›¾æ•°æ®å¤„ç†å™¨ - ç»Ÿä¸€åè°ƒå„ç»„ä»¶"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @property
    def entity_aligner(self):
        """æ‡’åŠ è½½çš„å®ä½“å¯¹é½å™¨"""
        if not hasattr(self, '_entity_aligner'):
            graph_config = self.config.get('graph_processor', {})
            if graph_config.get('enable_entity_alignment', False):
                try:
                    from utils import OpenAIAPIManager
                    openai_config = self.config.get('openai', {})
                    api_manager = OpenAIAPIManager(openai_config)
                    self._entity_aligner = EntityAligner(
                        self.config, api_manager)
                    logger.info("âœ… å®ä½“å¯¹é½å™¨å·²å¯ç”¨")
                except Exception as e:
                    logger.warning(f"âš ï¸ å®ä½“å¯¹é½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                    self._entity_aligner = None
            else:
                self._entity_aligner = None
                logger.info("ğŸ“‹ å®ä½“å¯¹é½å™¨å·²ç¦ç”¨")
        return self._entity_aligner

    def extract_raw_graph_data(self, kg_results: List[Dict]) -> Dict[str, Any]:
        """æå–åŸå§‹å›¾æ•°æ®ï¼ˆå¯¹é½å‰ï¼‰"""
        logger.info("ğŸ“Š æå–åŸå§‹å›¾æ•°æ®")

        # åˆå¹¶æ‰€æœ‰å®ä½“å’Œå…³ç³»
        all_entities = []
        all_relationships = []

        for result in kg_results:
            all_entities.extend(result.get('entities', []))
            all_relationships.extend(result.get('relationships', []))

        return {
            'entities': all_entities,
            'relationships': all_relationships,
            'statistics': {
                'total_entities': len(all_entities),
                'total_relationships': len(all_relationships),
                'chunks_processed': len(kg_results)
            }
        }

    def extract_pure_graph_data(self, kg_results: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """
        ä»çŸ¥è¯†å›¾è°±ç»“æœä¸­æå–çº¯å‡€çš„å›¾æ•°æ®ï¼ˆå¸¦å®ä½“å¯¹é½ï¼‰

        Returns:
            Tuple[å®Œæ•´è®°å½•æ•°æ®, ç®€åŒ–å›¾æ•°æ®]
            å®Œæ•´è®°å½•æ•°æ®: åŒ…å«nodeså’Œedgesç»Ÿè®¡çš„åˆ—è¡¨ï¼Œnodesä¸­åŒ…å«all_chunks_info
            ç®€åŒ–å›¾æ•°æ®: ç”¨äºç”Ÿæˆå›¾çš„pkey-skeyæ ¼å¼ï¼Œä¸åŒ…å«æ— å…³å±æ€§
        """
        logger.info("ğŸ”„ å¼€å§‹æå–å®ä½“å¯¹é½å›¾æ•°æ®")

        # 1. æ”¶é›†æ‰€æœ‰åŸå§‹å®ä½“å’Œå…³ç³»
        all_entities = []
        all_relationships = []

        for result in kg_results:
            all_entities.extend(result.get("entities", []))
            all_relationships.extend(result.get("relationships", []))

        logger.info(
            f"æ”¶é›†åˆ°åŸå§‹æ•°æ®: {len(all_entities)}ä¸ªå®ä½“, {len(all_relationships)}ä¸ªå…³ç³»")

        entity_aligner = self.entity_aligner  # è§¦å‘æ‡’åŠ è½½
        # å®ä½“å¯¹é½
        if entity_aligner:
            try:
                # ğŸ”¥ æ¥æ”¶ä¸‰ä¸ªè¿”å›å€¼
                aligned_entities, name_mapping, id_mapping = self.entity_aligner.align_entities(
                    all_entities)
                logger.info(
                    f"âœ… å¤§æ¨¡å‹å¯¹é½å®Œæˆ: {len(all_entities)}ä¸ªåŸå§‹èŠ‚ç‚¹ -> {len(aligned_entities)}ä¸ªå¯¹é½èŠ‚ç‚¹")
                logger.info(f"åˆ›å»ºäº†{len(id_mapping)}ä¸ªIDæ˜ å°„")
            except Exception as e:
                logger.error(f"âŒ å®ä½“å¯¹é½å¤±è´¥: {e}")
                aligned_entities = all_entities
                name_mapping = {}
                # ğŸ”¥ åˆ›å»ºåŸºç¡€æ˜ å°„é¿å…å…³ç³»ä¸¢å¤±
                id_mapping = {}
                for entity in all_entities:
                    entity_id = entity.get('id', '')
                    if entity_id:
                        id_mapping[entity_id] = entity_id
                        # æ·»åŠ å˜ä½“æ˜ å°„
                        self._add_id_variants_fallback(entity_id, id_mapping)
        else:
            aligned_entities = all_entities
            name_mapping = {}
            id_mapping = {}
            for entity in all_entities:
                entity_id = entity.get('id', '')
                if entity_id:
                    id_mapping[entity_id] = entity_id
                    self._add_id_variants_fallback(entity_id, id_mapping)

        # å¤„ç†å…³ç³»ï¼ˆä½¿ç”¨å®Œæ•´çš„IDæ˜ å°„ï¼‰
        processed_relationships = self._process_relationships_with_mapping(
            all_relationships, id_mapping)

        # 5-8. ç”Ÿæˆç»“æœæ•°æ®ï¼ˆä½¿ç”¨æ–°çš„æŠ½è±¡å‡½æ•°ï¼‰
        full_result = self._build_full_graph_data(
            aligned_entities, processed_relationships)
        simple_result = self._build_simple_graph_data(
            aligned_entities, processed_relationships)

        logger.info(
            f"âœ… ç‚¹å¯¹é½å®Œæˆ: {len(all_entities)}ä¸ªåŸå§‹ç‚¹ -> {len(aligned_entities)}ä¸ªå¯¹é½ç‚¹")
        logger.info(
            f"âœ… è¾¹ç®€åŒ–å®Œæˆ: {len(processed_relationships)}ä¸ªå®Œæ•´è¾¹ -> {len(simple_result[0]['edges'])}ä¸ªç®€åŒ–è¾¹")

        return full_result, simple_result

    def _add_id_variants_fallback(self, entity_id: str, id_mapping: Dict[str, str]):
        """åå¤‡æ–¹æ¡ˆï¼šä¸ºå•ä¸ªå®ä½“IDæ·»åŠ å˜ä½“æ˜ å°„"""
        if not entity_id:
            return

        variants = [
            entity_id.lower(),
            entity_id.upper(),
            entity_id.capitalize()
        ]

        if '--' in entity_id:
            parts = entity_id.split('--', 1)
            if len(parts) == 2:
                prefix, suffix = parts
                variants.extend([
                    f"{prefix.lower()}--{suffix.lower()}",
                    f"{prefix.upper()}--{suffix.upper()}",
                    f"{prefix.capitalize()}--{suffix.lower()}",
                    f"{prefix.capitalize()}--{suffix.upper()}"
                ])

        for variant in variants:
            if variant not in id_mapping:
                id_mapping[variant] = entity_id

    def _process_relationships_with_mapping(self, relationships: List[Dict], id_mapping: Dict[str, str]) -> List[Dict]:
        """ä½¿ç”¨IDæ˜ å°„å¤„ç†å…³ç³»"""
        processed_relationships = []
        skipped_count = 0

        for rel in relationships:
            source_id = rel.get('source', '')
            target_id = rel.get('target', '')

            # æŸ¥æ‰¾æ˜ å°„
            mapped_source = id_mapping.get(source_id)
            mapped_target = id_mapping.get(target_id)

            if not mapped_source or not mapped_target:
                logger.warning(
                    f"è·³è¿‡å…³ç³»ï¼šæ— æ³•è§£æå®ä½“å¼•ç”¨ {source_id} -> {mapped_source} æˆ– {target_id} -> {mapped_target}")
                skipped_count += 1
                continue

            # åˆ›å»ºå¤„ç†åçš„å…³ç³»
            processed_rel = rel.copy()
            processed_rel['source'] = mapped_source
            processed_rel['target'] = mapped_target
            processed_relationships.append(processed_rel)

        if skipped_count > 0:
            logger.warning(f"âš ï¸ è·³è¿‡äº†{skipped_count}ä¸ªæ— æ³•è§£æçš„å…³ç³»")
        else:
            logger.info(f"âœ… æ‰€æœ‰{len(relationships)}ä¸ªå…³ç³»éƒ½æˆåŠŸè§£æ")

        return processed_relationships

    def _build_full_graph_data(self, aligned_entities: List[Dict], processed_relationships: List[Dict]) -> List[Dict]:
        """æ„å»ºå®Œæ•´çš„å›¾æ•°æ®ï¼ˆåŒ…å«æ‰€æœ‰è¯¦ç»†ä¿¡æ¯ï¼‰"""
        full_entities = self._build_full_entities(aligned_entities)
        full_relationships = self._build_full_relationships(
            processed_relationships)

        return [{
            "entities": full_entities,
            "relationships": full_relationships
        }]

    def _build_full_entities(self, aligned_entities: List[Dict]) -> List[Dict]:
        """æ„å»ºå®Œæ•´çš„å®ä½“æ•°æ®"""
        full_entities = []
        for entity in aligned_entities:
            entity_id = entity.get('id', '')
            entity_type = entity.get('type', '')
            name = entity.get('name', '')
            description = entity.get('description', '')
            properties = entity.get('properties', {})

            # ç¡®ä¿åŒ…å«chunks_info
            chunks_info = entity.get('chunks_info', [])
            if chunks_info:
                properties['chunks_info'] = chunks_info

            # æŒ‰ç…§æ ‡å‡†æ ¼å¼æ„é€ å®ä½“
            full_entity = {
                "name": name,
                "type": entity_type,
                "id": entity_id,
                "description": description,
                "labels": entity_type,
                "properties": properties,
                "chunks_info": chunks_info
            }
            full_entities.append(full_entity)

        return full_entities

    def _build_full_relationships(self, processed_relationships: List[Dict]) -> List[Dict]:
        """æ„å»ºå®Œæ•´çš„å…³ç³»æ•°æ®"""
        full_relationships = []
        for rel in processed_relationships:
            source_id = rel.get('source', '')
            target_id = rel.get('target', '')
            rel_type = rel.get('type', '')
            description = rel.get('description', '')
            confidence = rel.get('confidence', 0.7)
            chunks_info = rel.get('chunks_info', [])

            # æŒ‰ç…§æ ‡å‡†æ ¼å¼æ„é€ å…³ç³»
            full_relationship = {
                "source": source_id,
                "target": target_id,
                "relationship": rel_type,
                "description": description,
                "confidence": confidence,
                "properties": {
                    "source": source_id,
                    "target": target_id,
                    "relationship": rel_type,
                    "description": description,
                    "chunks_info": chunks_info
                },
                "chunks_info": chunks_info
            }
            full_relationships.append(full_relationship)

        return full_relationships

    def _build_simple_graph_data(self, aligned_entities: List[Dict], processed_relationships: List[Dict]) -> List[Dict]:
        """æ„å»ºç®€åŒ–çš„å›¾æ•°æ®ï¼ˆä»…åŒ…å«å¿…è¦å±æ€§ï¼‰"""
        simple_nodes = self._build_simple_nodes(aligned_entities)
        simple_edges = self._build_simple_edges(processed_relationships)

        return [{
            "nodes": simple_nodes,
            "edges": simple_edges
        }]

    def _build_simple_nodes(self, aligned_entities: List[Dict]) -> Dict[str, int]:
        """æ„å»ºç®€åŒ–çš„èŠ‚ç‚¹æ•°æ®"""
        simple_nodes = {}
        for entity in aligned_entities:
            entity_id = entity.get('id', '')
            entity_type = entity.get('type', '').lower()
            name = entity.get('name', '')

            # ç®€åŒ–èŠ‚ç‚¹æ ¼å¼ï¼Œä»…åŒ…å«å¿…è¦å±æ€§
            node_key = json.dumps({
                "pkey": entity_id,
                "label": name,
                "entity_type": entity_type
            }, ensure_ascii=False, sort_keys=True)

            merge_count = entity.get('properties', {}).get('merge_count', 1)
            simple_nodes[node_key] = merge_count

        return simple_nodes

    def _build_simple_edges(self, processed_relationships: List[Dict]) -> Dict[str, int]:
        """æ„å»ºç®€åŒ–çš„è¾¹æ•°æ®"""
        simple_edges = {}
        for rel in processed_relationships:
            source_id = rel.get('source', '')
            target_id = rel.get('target', '')
            rel_type = rel.get('type', '')

            # ç®€åŒ–è¾¹æ ¼å¼ï¼Œä»…åŒ…å«å¿…è¦å±æ€§
            edge_key = json.dumps({
                "pkey": source_id,
                "skey": target_id,
                "label": rel_type
            }, ensure_ascii=False, sort_keys=True)

            # ç´¯åŠ ç›¸åŒè¾¹çš„è®¡æ•°
            simple_edges[edge_key] = simple_edges.get(edge_key, 0) + 1

        return simple_edges
